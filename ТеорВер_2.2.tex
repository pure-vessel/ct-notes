\documentclass{article}
\input{Headers/header}
\input{Headers/formal}

\fancyhead[L]{Теория вероятности}

\let\eps\varepsilon

\newcommand{\A}{{\mathfrak A}}
\newcommand{\B}{{\mathfrak B}}

\DeclareMathOperator{\Expected}{\mathsf{E}}
\DeclareMathOperator{\Covariance}{cov}

\begin{document}
    \tableofcontents
    \section{Основы теории вероятности.}
    \paragraph{Вероятностное пространство.}
    \begin{definition}
        Пусть $\Omega$~--- множество, тогда $\A\in 2^\Omega$ называется \textbf{алгеброй}, если
        \begin{enumerate}
            \item $\Omega\in A$.
            \item $\forall A\in\mathfrak A~\overline A\in\A$. Здесь и далее $\overline A=\Omega\setminus A$.
            \item $\forall A,B\in\A~A\cup B\in\A$.
        \end{enumerate}
        При этом $\Omega$ называется \textbf{множеством элементов событий}, $\A$~--- \textbf{набор событий}, $A\in\A$~--- \textbf{событие}, $A\cup B=A+B$~--- \textbf{сумма событий}, $\overline A$~--- \textbf{противоположное событие}, $A\cap B=AB$~--- \textbf{произведение событий}.
    \end{definition}
    \begin{definition}
        Алгебра является \textbf{сигма-алгеброй}, если она замкнута относительно объединения счётного количества своих элементов.
    \end{definition}
    \begin{definition}
        Пусть $\A$~--- сигма-алгебра на $\Omega$. Пусть $P\colon \A\to[0;+\infty)$ и
        \begin{enumerate}
            \item $P(\Omega)=1$.
            \item Если $\{A_i\}_{i=1}^\infty\subset\A$ и $\forall A_iA_j=\varnothing$ то
            $$
            P\left(\bigcup\limits_{i=1}^\infty A_i\right)=\sum\limits_{i=1}^\infty P(A_i)
            $$
        \end{enumerate}
        Тогда $(\Omega;\A;P)$ называется \textbf{вероятностным пространством}.
    \end{definition}
    \begin{definition}
        Пара событий называется \textbf{несовместной}, если их пересечение пусто. Набор событий \textbf{несовместен}, если они попарно несовместны.
    \end{definition}
    \begin{definition}
        Пусть $A\subset2^\Omega$~--- алгебра. Тогда минимальная по включению сигма-алгебра $\sigma(A)\supset A$ называется \textbf{минимальной сигма-алгеброй}.
    \end{definition}
    \begin{claim}
        Таковая существует.
    \end{claim}
    \begin{proof}
        Хотя бы одна такая существует ($2^\Omega$), причём если пересечь сколько угодно сигма-алгебр, то получится искомая сигма-алгебра.
    \end{proof}
    \begin{definition}
        Пусть $\A$~--- алгебра на $\Omega$, $P\colon\A\to[0;+\infty)$ и
        \begin{itemize}
            \item $P(\Omega)=1$.
            \item Если $\{A_i\}_{i=1}^\infty\subset\A$ и $\bigsqcup\limits_{i=1}^\infty A_i\in\A$, то
            $$
            P\left(\bigsqcup\limits_{i=1}^\infty A_i\right)=\sum\limits_{i=1}^\infty P(A_i)
            $$
        \end{itemize}
        Тогда $(\Omega;\A;P)$~--- \textbf{вероятностное пространство в широком смысле}.
    \end{definition}
    \begin{theorem}[О продолжении меры]
        Пусть $(\Omega;\A;P)$~--- вероятностное пространство в широком смысле. Тогда существует единственная функция вероятности $Q\colon\sigma(\A)\to[0:+\infty)$, такое что $Q\Big|_{\A}\equiv P$.\\
        Без доказательства.
    \end{theorem}
    \begin{remark}
        Эта теорема позволяет нам сказать, например, что мы хотим задать вероятность на отрезках.
    \end{remark}
    \begin{definition}
        \textbf{Борелевская сигма-алгебра}~--- минимальная $\sigma$-алгебра, которая содержит все открытые множества.
    \end{definition}
    \begin{example}
        Дискретное вероятностное пространство: $\Omega=\{\omega_i\}_{i=1}^N$, $A=2^\Omega$, $P(\{\omega_i\})=p_i$, $\sum p_i=1$. Тогда $P(A)$~--- сумма вероятностей элементов $A$.
    \end{example}
    \begin{example}
        Геометрическая вероятность: $\Omega\subset\mathbb R^n$, измеримо по Лебегу, $\mu A<+\infty$, $\A$ состоит из измеримых по Лебегу множеств, $P(A)=\frac{\mu A}{\mu\Omega}$. Обычно при этом $\mathbb R^n$ не более чем трёхмерно.
    \end{example}
    \paragraph{Свойства вероятности.}
    \begin{property}
        $$
        \forall A,B\in\A~A\subset B\Rightarrow P(A)\leqslant P(B)
        $$
    \end{property}
    \begin{proof}
        Понятно, что $B\setminus A\in\A$. Тогда
        $$
        B=A\sqcup(B\setminus A)\Rightarrow P(B)=P(A)+P(B\setminus A)\geqslant P(A)
        $$
    \end{proof}
    \begin{corollary}
        $$\forall A\in\A~P(A)\leqslant1$$
    \end{corollary}
    \begin{property}
        $$
        P(A)=1-P(\overline A)
        $$
    \end{property}
    \begin{property}
        $$
        P(A+B)=P(A)+P(B)-P(AB)
        $$
    \end{property}
    \begin{proof}
        $$
        B=(B\setminus AB)\sqcup AB\Rightarrow P(B)=P(B\setminus AB)+P(AB)
        $$
        Тогда
        $$
        P(A+B)=P(A)+P(B\setminus AB)=P(A)+P(B)-P(AB)
        $$
    \end{proof}
    \begin{claim}[Формула включений-исключений]
        $$
        P(A_1+\cdots+A_n)=\sum\limits_{i=1}^nP(A_i)-\sum\limits_{\substack{i,j=1\\i<j}}^nP(A_iA_j)+\sum\limits_{\substack{i,j,k=1\\i<j<k}}^nP(A_iA_jA_k)-\cdots+(-1)^nP(A_1\cdots A_n)
        $$
    \end{claim}
    \begin{proof}
        Мне лень это писать, докажите сами по индукции.
    \end{proof}
    \begin{claim}
        $$
        P\left(\bigcup\limits_iA_i\right)\leqslant\sum\limits_i P(A_i)
        $$
    \end{claim}
    \begin{proof}
        Пусть $B_1=A_1$, $B_2=A_2\overline{A_1}$, $B_3=A_3\overline{A_1\cup A_2}$ и так далее. Тогда
        $$
        \bigcup\limits_iA_i=\bigsqcup\limits_iB_i
        $$
        При этом $B_i\subset A_i$, а значит
        $$
        \sum\limits_i P(A_i)\geqslant\sum\limits_i P(B_i)
        $$
    \end{proof}
    \begin{theorem}
        Пусть $(\Omega;\A;P)$~--- вероятностное пространство. Тогда следующие утверждения равносильны:
        \begin{enumerate}
            \item $P$ счётно-аддитивна.
            \item $P$ конечно-аддитивна и $\forall \{B_i\}_{i=1}^\infty:B_{i+1}\subset B_i$, $B=\bigcap\limits_{i=1}^\infty B_i$ $\lim\limits_{n\to\infty}P(B_i)=P(B)$ (непрерывность сверху).
            \item $P$ конечно-аддитивна и $\forall \{C_i\}_{i=1}^\infty:C_{i+1}\supset B_i$, $C=\bigcup\limits_{i=1}^\infty C_i$ $\lim\limits_{n\to\infty}P(C_i)=P(C)$ (непрерывность снизу).
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        Равносильность двух непрерывностей тривиально из формул де Моргана.\\
        Докажем, что из 1 следует 2. Конечная аддитивность есть, докажем непрерывность сверху. Пусть $A_1=B_1\overline{B_2}$, $A_2=B_2\overline{B_3}$ и так далее. Очевидно, $A_i$ несовместны. Также очевидно, что $A_i$ несовместны с $B$. Также заметим, что
        $$
        B_n=B\sqcup\bigsqcup\limits_{i=n+1}^\infty A_i
        $$
        Отсюда $P(B_n)=P(B)+\sum\limits_{i=n+1}^\infty P(A_i)$, а справа остаток (очевидно, сходящегося) ряда, который стремится к нулю при $n\to\infty$.\\
        Теперь из 2 докажем 1. Рассмотрим $\{A_i\}_{i=1}^\infty$ несовместные. Очевидно,
        $$
        \sum\limits_{i=1}^\infty P(A_i)=\lim\limits_{n\to\infty}\sum\limits_{i=1}^nP(A_i)
        $$
        А ещё мы знаем, что
        $$
        \bigsqcup\limits_{i=1}^\infty A_i=\bigsqcup\limits_{i=1}^nA_i\sqcup\bigsqcup\limits_{i=n+1}^\infty A_i
        $$
        То есть
        $$
        \lim\limits_{n\to\infty}P\left(\bigsqcup\limits_{i=1}^nA_i\right)=
        \lim\limits_{n\to\infty}\left(P\left(\bigsqcup\limits_{i=1}^\infty A_i\right)-P\left(\bigsqcup\limits_{i=n+1}^\infty A_i\right)\right)=
        P\left(\bigsqcup\limits_{i=1}^\infty A_i\right)-\lim\limits_{n\to\infty}P\left(\bigsqcup\limits_{i=n+1}^\infty A_i\right)
        $$
        Второе слагаемое~--- ноль но непрерывности меры, а отсюда счётная аддитивность.
    \end{proof}
    \paragraph{Условная вероятность.}
    \begin{remark}
        Пусть $|\Omega|=n$, $|A|=k$, $|B|=m$, $|AB|=l$. Если мы знаем, что $B$ произошло, как узнать вероятность того, что произошло $A$? Ну, это
        $$
        \frac lm=\frac{l/n}{m/n}=\frac{P(AB)}{P(B)}
        $$
    \end{remark}
    \begin{definition}
        Пусть $(\Omega;\A;P)$~--- вероятностное пространство, $B\in\A$, $P(B)>0$. Тогда \textbf{условной вероятностью} $A$ при условии $B$ называется
        $$
        P(A|B)=\frac{P(AB)}{P(B)}
        $$
        Также обозначается $P_B(A)$.
    \end{definition}
    \begin{property}
        Несложно проверить, что условная вероятность является вероятностью.
    \end{property}
    \begin{claim}[Произведение вероятностей]
        Несложно по определению проверить
        $$
        P(A_1\cdots A_n)=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)\cdots P(A_n|A_1A_2\cdots A_{n-1})
        $$
    \end{claim}
    \begin{theorem}[Формула полной вероятности]
        Пусть $A\in\A$, $B_i\in\A$ несовместны, $A\subset\bigsqcup\limits_{i=1}^nB_i$ (обычно объединение равно $\Omega$), и $\forall i\in[0:n]~P(B_i)>0$. Тогда
        $$
        P(A)=\sum\limits_{i=1}^nP(A|B_i)P(B_i)
        $$
    \end{theorem}
    \begin{proof}
        $$
        P(A)=P\left(A\cap\bigsqcup\limits_{i=1}^nB_i\right)=P\left(\bigsqcup\limits_{i=1}^nA\cap B_i\right)
        $$
        Всё.
    \end{proof}
    \begin{theorem}[Формула Байеса]
        Пусть $A,B\in\A$, $P(A)>0$, $P(B)>0$. Тогда
        $$
        P(A|B)=\frac{P(B|A)P(A)}{P(B)}
        $$
    \end{theorem}
    \begin{proof}
        Очевидно из определения.
    \end{proof}
    \begin{definition}
        События $A,B\in\A$ называются \textbf{независимыми}, если
        $$
        P(AB)=P(A)P(B)
        $$
    \end{definition}
    \begin{definition}
        Говорят, что $A_1,A_2,\ldots,A_n\in\A$ \textbf{независимы в совокупности}, если
        $P(A_1A_2\cdots A_n)=P(A_1)P(A_2)\cdots P(A_n)$
    \end{definition}
    \begin{property}
        Несложно проверить, что независимость событий $A,B$ равносильна $P(A|B)=P(A)$.
    \end{property}
    \begin{property}
        Независимые в совокупности события попарно независимы. Обратное неверно.
    \end{property}
    \begin{definition}
        Пусть у нас есть два вероятностных пространства: $(\Omega_1,\A_1;P_1)$ и $(\Omega_2,\A_2;P_2)$. Рассмотрим вот такое вероятностное пространство: $(\Omega,\A;P)$, где $\Omega=\Omega_1\times\Omega_2$, $\A$~--- минимальная $\sigma$-алгебра, включающая в себя $\A_1\times\A_2$,
        $$
        P((A_1;A_2))=P_1(A_1)P_2(A_2)
        $$
        Тогда $(\Omega_1,\A_1;P_1)$ и $(\Omega_2,\A_2;P_2)$~--- независимые испытания.
    \end{definition}
    \paragraph{Схема Бернулли.}
    \begin{example}
        Схема Бернулли: $\Omega_1=\{0;1\}$, $\A_1=2^{\Omega_1}$, $P_1(1)=p$, $P_1(0)=1-p=q$. Хочется рассмотреть эту штуку в степени $n$ (то есть $n$ одинаковых независимых испытаний). Тогда что у нас получается для $\omega\in\Omega=\Omega_1^n$?
        $$
        P(\omega)=\sum\limits_{i=1}^nP_i(\omega_i)=p^{\sum\omega_i}q^{n-\sum\omega_i}
        $$
        Посчитаем тут такую вероятность: пусть $S_n$~--- количество успехов в $n$ испытаниях? Посчитаем вероятность того, что $S_n=k$? Очевидно, оно равно $\Cnk nkp^kq^{n-k}$.
    \end{example}
    \begin{claim}
        Пусть $k^*$~--- наиболее вероятное число успехов в Бернуллиевских испытаниях. Тогда
        $$
        k^*=\begin{cases}
            p(n-1)\text{ или }p(n-1)+1 & p(n-1)\in\mathbb N\\
            \lceil p(n-1)-1\rceil & p(n-1)\notin\mathbb N
        \end{cases}
        $$
    \end{claim}
    \begin{proof}
        Давайте рассмотрим вот такое частное:
        $$
        \frac{P(S_n=k+1)}{P(S_n=k)}
        $$
        Чему оно равно?
        $$
        \frac{P(S_n=k+1)}{P(S_n=k)}=\frac{\Cnk n{k+1}p^{k+1}q^{n-k-1}}{\Cnk nkp^kq^{n-k}}=\frac pq\cdot\frac{n-k}{k+1}
        $$
        Нам хочется оценить, больше это чем 1 или меньше (это позволит нам найти $K^*$). Ну,
        $$
        \frac pq\cdot\frac{n-k}{k+1}>1\Leftrightarrow p(n-k)>q(k+1)\Leftrightarrow pn-pk>k=pk+1-p\Leftrightarrow pn>k+1-p\Leftrightarrow pn+p-1>k
        $$
        То есть возрастание достигается при $k<p(n-1)-1$, а иначе убывание. Тогда где экстремум? Рассмотрим $k=p(n-1)-1$. Если это целое число, то там $P(S_n=k+1)=P(S_n=k)$, и это самое $k$ даёт значение больше остальных. То есть $k^*=p(n-1)-1$ или $k^*=p(n-1)$.\\
        А что если оно не целое? То надо куда-то округлить. А именно вверх, потому что тогда оно больше, чем следующее, а предыдущее меньше его.
    \end{proof}
    \begin{example}
        Пусть $n=10000$, $p=\frac1{10000}$. Давайте посчитаем $P(S_n>3)$. Ну, это
        $$
        1-P(S_n\leqslant 3)=1-q^{10000}-10000pq^{10000-1}-\Cnk{10000}2p^2q^{10000-2}-\Cnk{10000}3p^3q^{10000-3}
        $$
        Фиг мы такое посчитаем.
    \end{example}
    \begin{example}
        Или если взять $p=q=0.5$, то при $n=5\cdot10^3$ мы не сможем нормально посчитать $P(S_n=2349)$.
    \end{example}
    \begin{remark}
        Ну и как такое считать?
    \end{remark}
    \begin{theorem}[Теорема Пуассона]
        Пусть у нас есть несколько схем Бернулли. В первой одно испытание и вероятность успеха $p_1$, во второй~--- 2 и вероятность успеха $p_2$, в $n$-ной $n$ испытаний и вероятность $p_n$. Пусть $np_n\underset{n\to\infty}\longrightarrow\lambda>0$. Тогда
        $$
        P(S_n=k)\underset{n\to\infty}\longrightarrow e^{-\lambda}\frac{\lambda^k}{k!}
        $$
    \end{theorem}
    \begin{proof}
        Известно,
        $$
        P(S_n=k)=\frac1{k!}n(n-1)\cdots(n-k+1)p_n^k(1-p_n)^{n-k}
        $$
        Известно, что
        $$
        np_n=\lambda+o(1)\Rightarrow p_n=\frac\lambda n+o\left(\frac1n\right)
        $$
        Тогда
        $$
        P(S_n=k)=\frac1{k!}\cancelto1{n(n-1)\cdots(n-k+1)\frac1{n^k}}\cancelto{\lambda^k}{(\lambda+o(1))^k}\frac{\cancelto{e^{-\lambda}}{\left(1-\frac\lambda n+o\left(\frac1n\right)\right)^n}}{\cancelto1{\left(1-\frac\lambda n+o\left(\frac1n\right)\right)^k}}\underset{n\to\infty}\longrightarrow e^{-\lambda}\frac{\lambda^k}{k!}
        $$
    \end{proof}
    \begin{lemma}
        Пусть $p\in(0;1)$, $H(x)=x\ln\frac xp+(1-x)\ln\frac{1-x}{1-p}$. Пусть $p^*=\frac kn$. Пусть $k\rightarrow+\infty$, $n-k\rightarrow+\infty$. Тогда
        $$
        P(S_n=k)\sim\frac1{\sqrt{2\pi np^*(1-p^*)}}\exp(-nH(p^*))
        $$
    \end{lemma}
    \begin{proof}
        Мы знаем формулу Стирлинга
        $$
        n!\sim\sqrt{2\pi n}n^ne^{-n}
        $$
        Тогда
        \[\begin{split}
            P(S_n=k)&=\frac{\sqrt{2\pi n}n^ne^{-n}}{\sqrt{2\pi\underbrace{k}_{np^*}}k^ke^{-k}\sqrt{2\pi\underbrace{(n-k)}_{n(1-p^*)}}(n-k)^{n-k}e^{-n+k}}p^k(1-p)^{n-k}=\\
            &=\frac{n^np^k(1-p)^{n-k}}{\sqrt{2\pi np^*(1-p^*)}k^k(n-k)^{n-k}}=\frac1{\sqrt{2\pi np^*(1-p^*)}}\exp\underbrace{\ln\frac{n^np^k(1-p)^{n-k}}{k^k(n-k)^{n-k}}}_L
        \end{split}\]
        При этом
        \[\begin{split}
            L&=\ln\frac{n^np^k(1-p)^{n-k}}{k^k(n-k)^{n-k}}=\ln\frac{n^np^k(1-p)^n(n-k)^k}{k^k(n-k)^n(1-p)^k}=\\
            &=\ln\left(\underbrace{\frac{n^n}{(n-k)^n}}_{(1-p^*)^{-n}}(1-p)^n\right)+\ln\frac{p^k(n-k)^k}{(np^*)^k(1-p)^k}=\\
            &=n\ln\frac{1-p}{1-p^*}+k\ln\frac{p}{p^*}+k\ln\frac{(n-k)}{n(1-p)}=n\ln\frac{1-p}{1-p^*}+k\ln\frac{p}{p^*}+k\ln\frac{1-p^*}{1-p}=\\
            &=-(n-k)\ln\frac{1-p^*}{1-p}-k\ln\frac{p^*}{p}=-n\underbrace{\left(p^*\ln\frac{p^*}{p}+(1-p^*)\ln\frac{1-p^*}{1-p}\right)}_{H(p^*)}
        \end{split}\]
        Это ли не то, что нам надо?
    \end{proof}
    \begin{lemma}
        $$
        H(x)=\frac{(x-p)^2}{2p(1-p)}+O((x-p)^3)
        $$
    \end{lemma}
    \begin{proof}
        $$
        H'(x)=\ln\frac xp+x\cdot\frac px\cdot\frac1p-\ln\frac{1-x}{1-p}-1=\ln\frac xp-\ln\frac{1-x}{1-p}
        $$
        $$
        H''(x)=\frac1x+\frac1{1-x}
        $$
        Тогда $H'(p)=0$, $H''(p)=\frac1{p(1-p)}$. По Тейлору получаем искомое.
    \end{proof}
    \begin{theorem}[Локальная теорема Муавра~--- Лапласа]
        Пусть $p\in(0;1)$, $H(x)=x\ln\frac xp+(1-x)\ln\frac{1-x}{1-p}$. Пусть $p^*=\frac kn$. Пусть $k\rightarrow+\infty$, $n-k\rightarrow+\infty$.\\
        Пусть $k=np=p(n^{2/3})$. Тогда
        $$
        P(S_n=k)\sim\frac1{\sqrt{2\pi np(1-p)}}\exp\left(-\frac{(k-np)^2}{2np(1-p)}\right)
        $$
    \end{theorem}
    \begin{proof}
        Известно
        $$
        P(S_n=k)\sim\frac1{\sqrt{2\pi np(1-p)}}\exp(-nH(p^*))
        $$
        Отсюда
        $$
        P(S_n=k)\sim\frac1{\sqrt{2\pi np(1-p)}}\exp(-n\frac{(p^*-p)^2}{2p(1-p)}+n\cdot O((p^*-p)^3))
        $$
        Заметим, что $\frac kn-p=o(n^{-1/3})$. Тогда
        $$
        P(S_n=k)\sim\frac1{\sqrt{2\pi np(1-p)}}\exp\left(-n\frac{(p-k/n)^2}{2p(1-p)}+O(n(k/n-p)^3)\right)\sim\frac1{\sqrt{2\pi np(1-p)}}\exp\left(-n\frac{(np-k)^2}{2p(1-p)n^2}+o(1)\right)
        $$
        Что и требовалось доказать.
    \end{proof}
    \begin{theorem}[Интегральная теорема Муавра~--- Лапласа]
        Пусть
        $$
        \Phi(x)=\frac1{\sqrt{2\pi}}\int\limits_{-\infty}^xe^{-\frac{t^2}2}~\mathrm dt
        $$
        Далее мы будем называть эту функцию функцией стандартного нормального распределения. Тогда
        $$
        \sup\limits_{-\infty<x_1<x_2<+\infty}\left|P\left(x_1\leqslant\frac{S_n-np}{\sqrt{npq}}\leqslant x_2\right)(\Phi(x_2)-\Phi(x_1))\right|\underset{n\to\infty}\longrightarrow0
        $$
        Иными словами
        $$
        P\left(x_1\leqslant\frac{S_n-np}{\sqrt{npq}}\leqslant x_2\right)\approx\frac1{\sqrt{2\pi}}\int\limits_{x_1}^{x_2}e^{-\frac{t^2}2}~\mathrm dt
        $$
        Пока без доказательства.
    \end{theorem}
    \begin{remark}
        Оценка теоремы Пуассона.\\
        Обычно в задачах $np_n$ не стремится, а просто равно $\lambda>0$. Тогда
        $$
        \sum\limits_{k=0}^\infty \left|P(S_n=k)-e^{-\lambda}\frac{\lambda^k}{k!}\leqslant\frac\lambda n\right|\leqslant\frac{2\lambda}n\min\{2;\lambda\}
        $$
        Оценка локальной теоремы Лапласа. Если $|p^*-p|\leqslant \frac12 mn\min\{p;q\}$, то
        $$
        P(S_n=k)=\frac1{\sqrt{2\pi np(1-p)}}\exp\left(-\frac{(k-np)^2}{2np(1-p)}\right)(1+\eps(k;n))
        $$
        Где
        $$
        \eps(k;n)=\exp\left(\theta\frac{|k-np|^3}{3n^2p^2q^2}+\frac1{npq}\left(\frac16+|k-np|\right)\right)\qquad |\theta|<1
        $$
        Оценка интегральной теоремы Лапласа.
        $$
        \sum\limits_x\left|P\left(\frac{S_n-np}{\sqrt{npq}}\leqslant x\right)-\Phi(x)\right|\leqslant\frac{p^2+q^2}{\sqrt{npq}}
        $$
    \end{remark}
    \begin{example}
        Пусть у нас есть два узла связи на 2000 пользователей в каждом. И у нас есть канал связи, который пропускает $N$. Хочется минимизировать $N$, но так, чтобы вероятность перегрузки была меньше $\frac1{100}$. Будем предполагать, что люди пользуются данным каналом связи в течение двух минут из одного часа, то есть каждый пользователь может пользоваться каналом в данный момент с вероятностью $p=\frac1{30}$.\\
        Ну так и что мы хотим по сути? Мы хотим $P(S_{2000}>N)<\frac1{100}$, что равносильно $P(S_{2000}\leqslant N)\geqslant\frac{99}{100}$.\\
        Используем Пуассона: $np\approx 6.67$.
        $$
        \sum\limits_{k=0}^Ne^{-\lambda}\frac{\lambda^k}{k!}
        $$
        Это мы хрен посчитаем, но, короче, получится $N=87$.\\
        А если применить интегральную теорему Муавра~--- Лапласа, то получим мы
        $$
        N=\left\lceil q_{\frac{99}{100}}\sqrt{npq}+np\right\rceil=86
        $$
        Где $q_{\frac{99}{100}}$~--- такое число, что $\Phi(q_{\frac{99}{100}})=\frac{99}{100}$.
    \end{example}
    \begin{definition}
        Если $\alpha\in(0;1)$ и $\Phi(q_\alpha)=\alpha$, то $q_\alpha$ называется \textbf{квантилем порядка $\alpha$}.
    \end{definition}
    \section{Случайные величины.}
    \subsection{Одномерные случайные величины.}
    \paragraph{Распределение случайных величин, функция распределения случайных величин.}
    \begin{definition}
        \textbf{Борелевская сигма-алгебра}~--- это минимальная сигма-алгебра, содержащая все открытые множества.
    \end{definition}
    \begin{definition}
        Пусть $\Omega;\A$~--- множество с сигма-алгеброй. Тогда такое $\xi\colon\Omega\to\mathbb R$, что $\forall B\in\B~\xi^{-1}(B)\in\A$, называется \textbf{случайной величиной}.
    \end{definition}
    \begin{definition}
        Пусть $(\Omega;\A;P)$~--- вероятностное пространство, $\xi$~--- случайная величина. Тогда распределение $\xi$~--- функция
        $$
        P_\xi\colon\substack{\B\to\mathbb R\\B\mapsto P(\{\omega\mid \xi(\omega)\in B\})}
        $$
    \end{definition}
    \begin{remark}
        $P(\{\omega\mid \xi(\omega)\in B\})$ обозначается $P(\xi\in B)$.
    \end{remark}
    \begin{property}
        $P_\xi$~--- вероятность на $(\mathbb R;\B)$.
    \end{property}
    \begin{definition}
        Пусть $\xi$~--- случайная величина. Тогда
        $$
        F_\xi(t)=P(\xi\leqslant t)
        $$
        называется \textbf{функцией распределения} $\xi$
    \end{definition}
    \begin{property}
        Очевидно, функция распределения нестрого возрастает.
    \end{property}
    \begin{property}
        Не менее очевидно, $F_\xi(+\infty)=1$, $F_\xi(-\infty)=0$;
    \end{property}
    \begin{property}
        Функция распределения непрерывна справа.
    \end{property}
    \begin{proof}
        Возьмём $F(t+\eps_n)-F(t)$. Она равна $P(t<\xi\leqslant t+\eps_n)$. При $\eps_n\to0$, получим, что аргумент $P$ стремится к $\varnothing$, а значит $P(t<\xi\leqslant t+\eps_n)$ стремится к нулю.
    \end{proof}
    \begin{lemma}
        Пусть $P\colon\B\to\mathbb R$~--- некоторая функция. Тогда
        \[\begin{split}
            &\forall\{B_n\}_{n=1}^\infty\subset\B:B_{n+1}\subset B_n~P(B_n)\underset{n\to\infty}\rightarrow P\left(\bigcap\limits_{n=1}^\infty B_n\right)\Leftrightarrow\\
            \Leftrightarrow&\forall\{C_n\}_{n=1}^\infty\subset\B:C_{n+1}\subset C_n,\bigcap\limits_{n=1}^\infty C_n=\varnothing~P(C_n)\underset{n\to\infty}\rightarrow0
        \end{split}\]
    \end{lemma}
    \begin{proof}
        Следствие слева направо очевидно. Наоборот. Пусть
        $$
        \bigcap\limits_{n=1}^\infty B_n=B
        $$
        Возьмём $C_n=B_n\overline{B}$. Тогда, очевидно, $C_n$ подходят под условие справа, а значит $P(C_n)\longrightarrow0$.\\
        Также несложно заметить, что $P(B_n)=P(C_n)+P(B)$, а отсюда получим $P(B_n)\longrightarrow P(B)$.
    \end{proof}
    \begin{theorem}
        Пусть $F$~--- монотонно возрастающая непрерывная слева функция, равная нулю в $-\infty$ и единице в $+\infty$. Тогда существует вероятностное пространство и случайная величина в нём, что $F$~--- её функция распределения.
    \end{theorem}
    \begin{proof}
        Пусть $\Omega=\mathbb R$, $\A$~--- алгебра, состоящая из множеств вида $\bigsqcup\limits_{k=1}^n(a_k;b_k]$ или $(-\infty;b)$ или $(a;\infty)$ или $\mathbb R$.\\
        $$
        P\left(\bigsqcup\limits_{k=1}^n(a_k;b_k]\right)=\sum\limits_{k=1}^nF(b_k)-F(a_k)
        $$
        $$
        P((-\infty;b))=F(b)\qquad P((a;+\infty))=1-F(a)\qquad P(\mathbb R)=1
        $$
        Получим вероятностное пространство в широком смысле (разве что непрерывность сверху надо проверить). Ну, проверим её, используя лемму. Пусть, не умаляя общности, $A_n=(a_{n,1};a_{n,2}]$, $A_{n+1}\subset A_n$ и пересечение всех пусто.\\
        Из непрерывности $F$ справа следует, что существует $B_n=(b_{n,1};b_{n,2}]$, где $\Cl B_n\subset A_n$ и $P(A_n)-P(B_n)\leqslant\eps 2^{-n}$. Тогда пересечение всех $B_n$ также пусто.\\
        Предположим, что существует $M$, такое что $\forall n~A_n\in[-M;M]$. $[-M;M]$~--- компакт, следовательно. Заметим, что
        $$
        [-M;M]=\bigcup\limits_{k=1}^\infty [-M;M]\setminus\Cl B_n
        $$
        Справа~--- открытое покрытие компакта, значит из него можно вытащить конечное подпокрытие, то есть пересечение какого-то конечного числа $B_n$ пусто. Пусть это пересечение от $1$ до $n_0$. Тогда
        $$
        P(A_{n_0})=P(A_{n_0}\setminus\bigcap\limits_{k=1}^{n_0}B_k)+P\left(\bigcap\limits_{k=1}^{n_0}B_k\right)
        $$
        Отсюда $P\left(\bigcap\limits_{k=1}^{n_0}B_k\right)=0$. А
        $$
        P(A_{n_0}\setminus\bigcap\limits_{k=1}^{n_0}B_k)=P(\bigcap\limits_{k=1}^{n_0}A_{n_0}\setminus B_k)\leqslant P(\bigcap\limits_{k=1}^{n_0}A_k\setminus B_k)\leqslant\sum\limits_{k=1}P(A_n)-P(B_k)\leqslant\eps\sum\limits{k=1}^{n_0} 2^{-n}<\eps
        $$
        Если же мы не находимся в промежутке $[-M;M]$, то можно указать такие $M_1$ и $M_2$, что $P((-\infty;M_1)\cup(M_2;+\infty))<\frac\eps2$. Тогда
        $$
        P(A_n)=P(A_n[M_1;M_2])+P(A_n\overline{[M_1;M_2]})
        $$
        Левую часть суммы мы разобрали, а правая мала т.к. $M_2$, что $P((-\infty;M_1)\cup(M_2;+\infty))<\frac\eps2$.\\
        Осталось предъявить случайную величину $\xi(\omega)=\omega$.
    \end{proof}
    \paragraph{Типы распределений.}
    \subparagraph{Дискретные случайные величины и распределения.}
    \begin{definition}
        Случайная величина $\xi$ называется \textbf{дискретной}, если существует такое не более чем счётное множество $E$, что $P_\xi(E)=1$.
    \end{definition}
    \begin{example}
        Вырожденное: $P(\xi=c)=1$. Обозначают $I(c)$ или $I_c$.
    \end{example}
    \begin{example}
        \label{Распределение Бернулли}
        \label{Bern}
        Распределение Бернулли: $P(\xi=0)=p$, $P(\xi=1)=q=1-p$. Обозначение: $\operatorname{Bern}(p)$.
    \end{example}
    \begin{example}
        \label{Биномиальное распределение}
        \label{Bin}
        Биномиальное распределение: $P(\xi=k)=\Cnk nk p^kq^{n-k}$. Обозначение: $\operatorname{Bin}(n;p)$,
    \end{example}
    \begin{example}
        \label{Отрицательное биномиальное распределение}
        \label{NB}
        \label{Геометрическое распределение}
        \label{Geom}
        Отрицательное биномиальное распределение: $\xi=(\min n:S_n=r)-r$, где $r\in\mathbb n$. То есть
        $$
        P(\xi=k)=\Cnk{k+r-1}{r-1}p^{r}q^{k}
        $$
        Обозначение: $\operatorname{NB}(r;p)$. Также это обобщается на произвольное $r$ при помощи гамма-функции.\\
        В случае $r=1$ распределение называется геометрическим. Геометрическое распределение~--- количество неудач до первого успеха. Обозначается $\operatorname{Geom}(p)$.
    \end{example}
    \begin{example}
        \label{Распределение Пуассона}
        \label{Pois}
        Распределение Пуассона:
        $$
        P(\xi=k)=e^{-\lambda}\frac{\lambda^k}{k!}\qquad k\in\mathbb Z_+
        $$
        Обозначение: $\operatorname{Pois}(\lambda)$.
    \end{example}
    \begin{definition}
        \textbf{Носителем случайной величины} $\xi$ называется минимально по включению замкнутое множество $E$, удовлетворяющее условию $P(\xi\in E)=1$.
    \end{definition}
    \subparagraph{Абсолютно непрерывные случайные величины и распределения.}
    \begin{definition}
        Величина $\xi$ (или её случайно распределение) называется \textbf{абсолютно непрерывной},  если существует $p\in L(\mathbb R\to\mathbb R)$ с неотрицательными значениями такая что $P(\xi\in B)=\int_BP(x)~\mathrm dx$. В таком случае $p$ называется \textbf{плотностью} $\xi$.
    \end{definition}
    \begin{property}
        В таком случае
        $$F(t)=\int\limits_{-\infty}^tp(x)~\mathrm dx$$
        То есть плотность почти всюду равна производной функции распределения.
    \end{property}
    \begin{property}
        $P(\xi=c)=0$.
    \end{property}
    \begin{property}
        Функция распределения абсолютно непрерывной случайной величины непрерывна на $\mathbb R$.
    \end{property}
    \begin{property}
        $$P(x_0\leqslant\xi\leqslant x_0+h)=P(x_0<\xi<x_0+h)=F(x_0+h)-F(x_0)=p(x_0)h+o(h)$$
    \end{property}
    \begin{property}
        Пусть $E=\supp p$. Тогда $E$ является носителем по нашему прошлому определению.
    \end{property}
    \begin{example}
        $$
        p(x)=\frac1{b-a}\chi_{[a;b]}
        $$
        Тогда
        $$
        F(x)=\begin{cases}
            0 & x<a\\
            \frac{x-a}{b-a} & x\in[a;b)\\
            1 & x\geqslant b
        \end{cases}
        $$
        Обозначение: $U[a;b]$.
    \end{example}
    \begin{claim}
        Пусть $\xi=U[a;b]$, $c>0$. Тогда $\eta=c\xi+d=U[ac+d;bc+d]$
    \end{claim}
    \begin{proof}
        $$
        P(\eta\leqslant t)=P(c\xi+d\leqslant t)=P\left(\xi\leqslant\frac{t-d}c\right)
        $$
        Несложно проверить, что это именно $U[ac+d;bc+d]$.
    \end{proof}
    \begin{example}
        \label{Нормальное распределение}
        \label{Гауссовское распределение}
        \label{N}
        Нормальное (гауссовское) распределение: $\operatorname{N}(\mu;\sigma^2)$:
        $$
        p(x)=\frac1{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
        $$
        \begin{figure}[H]
            \begin{tikzpicture}
                \begin{axis}[
                    width = 15cm, height = 5cm,
                    trig format plots = rad,
                    grid = both,
                    xmin = -2,
                    xmax = 2,
                    ymin = 0,
                    ymax = 1,
                    axis equal,
                    axis x line = middle,
                    axis y line = middle,
                    axis line style = {->, color=black},
                    xtick distance = 1,
                    ytick distance = 1,
                    minor tick num = 3,
                    xlabel={$x$},
                    ylabel={$p(x)$},
                    ]
                    \addplot[domain=-2:2,samples=100]{1/sqrt(pi/2)*e^(-x^2*2)};
                \end{axis}
            \end{tikzpicture}
        \end{figure}\noindent
        $\operatorname{N}(0;1)$~--- стандартное нормальное распределение. Ещё оно обозначается $\Phi(x)$.
    \end{example}
    \begin{claim}
        Пусть $\xi=\operatorname{N}(\mu;\sigma^2)$, $\eta=a\xi+b$. Тогда
        $\eta=\operatorname{N}(a\mu_b;a^2\sigma^2)$.
    \end{claim}
    \begin{proof}
        Пусть $a>0$. Обозначим $y=ax+b$. Тогда
        \[\begin{split}
            P(\eta\leqslant t)&=P(a\xi+b\leqslant t)=P(\xi\leqslant\frac{t-b}a)=F_\xi(\frac{t-b}a)=\frac1{2\pi\sigma^2}\int\limits_{-\infty}^{\frac{t-b}a}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)~\mathrm dx=\\
            &=\frac1{\sqrt{2\pi\sigma^2a^2}}\int\limits_{-\infty}^t\exp\left(-\frac{(\frac{y-b}a-\mu)^2}{2\sigma^2}\right)=\frac1{\sqrt{2\pi\sigma^2a^2}}\int\limits_{-\infty}^t\exp\left(-\frac{(y-b-a\mu)^2}{2a^2\sigma^2}\right)
        \end{split}\]
        Что и требовалось доказать. При $a<0$ аналогично.
    \end{proof}
    \begin{corollary}
        Если $\xi=\operatorname{N}(0;1)$, то $\sigma\xi+\mu=\operatorname{N}(\mu;\sigma^2)$.
    \end{corollary}
    \begin{example}
        \label{Распределение Коши}
        \label{Cauchy}
        Распределение Коши: $\operatorname{Cauchy}(x_0;\gamma)$.
        $$
        p(x)=\frac1{\pi\gamma}\cdot\frac1{1+\left(\frac{x-x_0}\gamma\right)^2}
        $$
        Тогда
        $$
        F(t)=\frac1{\pi\gamma}\int\limits_{-\infty}^t\frac{\mathrm dx}{1+\left(\frac{x-x_0}\gamma\right)^2}=\frac1\pi\atan\left(\frac{t-x_0}\gamma\right)+\frac12
        $$
    \end{example}
    \begin{example}
        \label{Экспоненциальное распределение}
        \label{Exp}
        Экспоненциальное распределение: $\operatorname{Exp}(\lambda)$.
        $$
        p(x)=\lambda e^{-\lambda x}\chi_{\mathbb R_+}
        $$
        Тогда
        $$
        F(t)=1-e^{-\lambda t}\chi_{\mathbb R_+}
        $$
    \end{example}
    \begin{example}
        \label{Gamma-распределение}
        $\Gamma$-распределение: $\Gamma(k,\lambda)$. Сначала для $k\in\mathbb N$, $\lambda>0$.
        $$
        p(x)=\frac{\lambda^kx^{k-1}}{(k-1)!}e^{-\lambda x}\qquad x\geqslant 0
        $$
        Для $k=\alpha>0$ заменим $(k-1)!$ на $\Gamma(\alpha)$.
    \end{example}
    \begin{claim}
        Пусть $\xi$~--- абсолютно непрерывная случайная величина с плотностью $p_\xi$. Пусть $g\in C^{(1)}(\mathbb R\to\mathbb R)$~--- строго монотонна. Пусть $\eta=g(\xi)$. Тогда
        $$
        p_\eta(y)=p_\xi(g^{-1}(y))\left|\frac{\mathrm d}{\mathrm dy}g^{-1}(y)\right|=p_\xi(g^{-1}(y))\left|\frac1{g'(g^{-1}(y))}\right|
        $$
    \end{claim}
    \begin{proof}
        Во-первых, у условиях теоремы $g^{-1}$ существует. Также второе равенство следует из первого в силу теоремы об обратном отображении.\\
        Не умаляя общности, $g$ строго возрастает. Тогда
        $$
        F_\eta(y)=P(g(\xi)\leqslant y)\overset{g\uparrow}=P(\xi\leqslant g^{-1}(y))=\int\limits_{-\infty}^{g^{-1}(y)}p_\xi(x)~\mathrm dx
        $$
        Продифференцировав это равенство по $y$ получим искомое равенство.
    \end{proof}
    \subparagraph{Сингулярные случайные величины и распределения.}
    \begin{definition}
        $x$ называется \textbf{точкой роста} монотонно возрастающей функции $f$, если $\forall\eps>0~f(x+\eps)-f(x-\eps)>0$.
    \end{definition}
    \begin{definition}
        Случайная величина $\xi$ (и её распределение) называются \textbf{сингулярными}, если $F_\xi\in C(\mathbb R)$ и мера множества точек $F_\xi$ роста равна нулю.
    \end{definition}
    \begin{example}
        Функция Кантора~--- функция, которая выглядит так: в нуле она равна нулю, в единице~--- единице, а во всех остальных точках строится так: отрезок $[0;1]$ делится на три части и в средней части равна среднему значению краёв (т.е. $\frac12$). И так далее.\\
        Оня является функцией распределения сингулярной случайной величины.
    \end{example}
    \begin{theorem}[Теорема Лебега]
        Пусть $F$~--- функция распределения. Тогда существуют единственные $F_{\mathrm{disc}}$, $F_{\mathrm{ac}}$ и $F_{\mathrm{sing}}$, которые в сумме дают $F$.\\
        Без доказательства.
    \end{theorem}
    \subsection{Многомерные случайные величины.}
    \paragraph{Распределение многомерных случайных величин.}
    \begin{definition}
        Вектор $\xi$ называется \textbf{случайным вектором} или \textbf{многомерной случайной величиной}, если $\xi_i$~--- случайная величина.
    \end{definition}
    \begin{definition}
        \textbf{Распределением случайного вектора} называется функция $P_\xi$, определённая на $\B^n$, заданная так:
        $$
        P_\xi(B_1;\ldots;B_n)=P(\{(\omega_1;\ldots;\omega_n)\mid\xi_1(\omega_1)\in B_1\land\cdots\land \xi_n(\omega_n)\in B_n\})
        $$
        Последнее обычно обозначается так: $P(\xi_1\in B_1,\xi_2\in B_2;\ldots;\xi_n\in B_n)$.
    \end{definition}
    \begin{definition}
        \textbf{Функцией распределения случайного вектора} $\xi$ называется функция
        $$
        F_\xi(t_1;\ldots;t_n)=P_\xi(\forall i\in[1:n]~\xi_i\leqslant t_i)
        $$
    \end{definition}
    \begin{claim}
        \[\begin{split}
            P(\forall i\in[1:n]~&a_i<\xi_i\leqslant b_i)=\\
            &F(b_1;b_2;\ldots;b_n)\\
            &-F(a_1;b_2;\ldots;b_n)-F(b_1;a_2;\ldots;b_n)-\cdots-F(b_1;b_2;\ldots;a_n)\\
            &+\vdots\\
            &\pm F(a_1;a_2;\ldots;a_n)
        \end{split}\]
        То есть в этой сумме участвует $F$ от $a_i$ и $b_i$ в произвольном сочетании, при этом минус стоит там, где нечётное количество $a_i$.
    \end{claim}
    \begin{proof}
        $$
            P(\forall i\in[1:n]~a_i<\xi_i\leqslant b_i)=P(\xi_1\leqslant b_1,\forall i\in[2:n]~a_i<\xi_i\leqslant b_i)-P(a_1\geqslant\xi_1,\forall i\in[2:n]~a_i<\xi_i\leqslant b_i)
        $$
        Проведя такую операцию несколько раз, получим искомое.
    \end{proof}
    \begin{remark}
        Если ввести обозначение $\Delta_{a_i;b_i}F=F(\cdot;\ldots;b_i;\cdot;\ldots;\cdots)-F(\cdot;\ldots;a_i;\cdot;\ldots;\cdots)$, то арифметическая сумма из утверждения выше записывается как
        $$
        \Delta_{a_1;b_1}\Delta_{a_2;b_2}\cdots\Delta_{a_n;b_n}F
        $$
    \end{remark}
    \begin{property}
        $$F(+\infty;\cdots;+\infty)=1$$
        $$F(-\infty;\cdots;-\infty)=0$$
        $F$ непрерывна справа.
    \end{property}
    \begin{theorem}
        Если функция распределения удовлетворяет трём свойствам выше, то она является функцией распределения некоторого случайного вектора.
    \end{theorem}
    \begin{proof}
        Аналогично одномерному случаю.
    \end{proof}
    \begin{definition}
        Случайный вектор (и его распределение) называется \textbf{дискретным}, если существует не более чем счётное множество $E$ такое что $P(\xi\in E)=1$.
    \end{definition}
    \begin{example}
        \label{Poly}
        Полиномиальное распределение. Пусть $p=(p_1;\ldots;p_m)$, где $\sum p_i=1$. Обозначается $\operatorname{Poly}(n;p)$.\\
        Физическая интерпретация такая: мы бросаем кубик с $m$ гранями $n$ раз. Пусть $S_{n,j}$~--- количество исходов типа $j$ в $n$ независимых испытаниях. Тогда искомая случайная величина обладает распределением
        $$
        P(S_{n,1}\in B_1;S_{n,2}\in B_2;\ldots;S_{n,m}\in B_m)
        $$
        Рассмотрим $P(S_{n,1}=k_1;S_{n,2}=k_2;\ldots;S_{n,m}=k_m)$, где $\sum k_m=n$. Чему равна такая вероятность? Ну,
        $$
        \frac{n!}{k_1!k_2!\cdots k_m!}p_1^{k_1}p_2^{k_2}\cdots p_m^{k_m}
        $$
    \end{example}
    \begin{remark}
        Несложно заметить, что штука справа~--- слагаемые в сумме $(p_1+p_2+\cdots+p_m)^n$.
    \end{remark}
    \begin{definition}
        Случайный вектор $\xi$ (и его распределение) называется \textbf{абсолютно непрерывным}, если существует $p\in L(\mathbb R^n\to\mathbb R)$ с неотрицательными значениями такая что $P(\xi\in B)=\int_BP~\mathrm d\mu_n$ (тут интеграл $n$-кратный). В таком случае $p$ называется \textbf{плотностью} $\xi$.
    \end{definition}
    \begin{example}
        Случайный вектор $\xi$ имеет стандартное многомерное нормальное распределение $\operatorname{N}(\mathbb0_n;E_n)$, если его плотность равна
        $$
        p(x_1;\ldots;x_n)=\frac1{(\sqrt{2\pi})^n}\exp\left(-\frac{\sum\limits_{i=1}^nx_i^2}2\right)
        $$
    \end{example}
    \begin{property}
        Несложно заметить, что это произведение плотностей одномерных стандартных нормальных распределений.
    \end{property}
    \begin{example}
        \label{N (многомерное)}
        Случайный вектор $\eta$ имеет многомерное нормальное распределение $\operatorname{N}(\mu;\Sigma)$ (где $\mu\in\mathbb R^n$, $\Sigma$~--- симметричная матрица $n\times n$ с неотрицательными собственными числами), если он равен $\sqrt\Sigma\xi+\mu$, где $\xi$~--- стандартный многомерный гауссовский вектор.
    \end{example}
    \begin{remark}
        В случае $\Sigma>0$ можно написать плотность:
        $$
        p(y)=\frac1{\sqrt{2\pi}^n\sqrt{\det\Sigma}}\exp\left(-\frac12(y-\mu)^T\Sigma^{-1}(y-\mu)\right)
        $$
    \end{remark}
    \paragraph{Независимые случайные величины.}
    \begin{definition}
        Случайные величины $\xi_1;\ldots;\xi_n$ называются \textbf{независимыми}, если для любых борелевских множеств $B_1;\ldots;B_n$
        $$
        P(\xi_1\in B_1\land\cdots\land\xi_n\in B_n)=P(\xi_1\in B_1)\cdot\cdots\cdot P(\xi_n\in B_n)
        $$
    \end{definition}
    \begin{definition}
        Случайные величины $\{\xi_i\}_{i=1}^\infty$ \textbf{независимы}, если
        $$
        \forall m~\xi_1;\ldots;\xi_m\text{ независимы}
        $$
    \end{definition}
    \begin{theorem}
        Случайные величины независимы тогда и только тогда, когда их совместная функция распределения равна произведению одномерных функций распределения.
    \end{theorem}
    \begin{proof}
        В одну сторону очевидно, в другую~--- доказательства не будет.
    \end{proof}
    \begin{theorem}[Критерий независимости дискретных случайных величин]
        Пусть $\xi_1;\ldots;\xi_n$~--- дискретны. Пусть $\xi_k$ имеет своим множеством значений $\{x_{i,k}\}_i$. Тогда эти величины независимы тогда и только тогда, когда
        $$
        \forall x_{\text{индексы}}~
        P(\xi_1=x_{i_1,1}\land\cdots\land\xi_n=x_{i_n,n})=P(\xi_1=x_{i_1,1})\cdot\cdots\cdot P(\xi_n=x_{i_n,n})
        $$
    \end{theorem}
    \begin{proof}
        Необходимость очевидна, теперь достаточность, которая тоже довольно проста. Условие $\xi_k\in B_k$ можно представить как $\xi_k=x_{j_1,k}\lor\cdots\lor \xi_k=x_{j_m,k}$. Отсюда используем сумму вероятностей и предпосылку импликации.
    \end{proof}
    \begin{theorem}[Критерий независимости абсолютно непрерывных случайных величин]
        Абсолютно непрерывные случайные величины независимы тогда и только тогда, когда их совместная плотность равна произведению одномерных плотностей.
    \end{theorem}
    \begin{proof}
        $$
        \int\limits_{-\infty}^{t_1}\cdots\int\limits_{-\infty}^{t_n}p(x_1;\ldots;x_n)~\mathrm dx_1\cdots\mathrm dx_n=F(t_1;\ldots;t_n)=F(t_1)\cdot\cdots\cdot F(t_n)=\prod\int\limits_{-\infty}^{t_k}p(x_k)~\mathrm dx_k
        $$
        Это если нам дана независимость, а узнать мы хотим указанное в теореме (дифференцируем указанное равенство). Следствие обратно сами докажете, используя аналогичные формулы.
    \end{proof}
    \begin{example}
        Пусть $\xi_1$ и $\xi_2$~--- независимые \hyperref[Распределение Бернулли]{Бернуллиевские случайные величины} с параметром $p$. Что будет, если их сложить? Очевидно, получится \hyperref[Биномиальное распределение]{биномиальное распределение} $\operatorname{Bin}(2;p)$.\\
        Тривиально по индукции докажете, что это для любого количества одинаковых независимых $\operatorname{Bern}(p)$ работает.
    \end{example}
    \begin{example}
        Пусть $\xi_1=\operatorname{Pois}(\lambda_1)$, $\xi_2=\operatorname{Pois}(\lambda_2)$ независимы. Чему равна сумма этих случайных \hyperref[Распределение Пуассона]{величин пуассона}? Получится $\operatorname{Pois}(\lambda_1+\lambda_2)$, докажете, опять же, сами, рассмотрев $P(\xi_1+\xi_2=k)$.
    \end{example}
    \begin{example}
        Пусть $\xi_1,\ldots,\xi_n\sim\hyperref[Geom]{\operatorname{Geom}}(p)$ и независимы. Тогда сумма имеет распределение $\hyperref[NB]{\operatorname{NB}}(n;p)$
    \end{example}
    \begin{example}
        Пусть $\xi_1,\xi_2\sim\hyperref[Exp]{\operatorname{Exp}}(\lambda)$ независимы. И мы опять считаем сумму $\xi_1+\xi_2$. Как несложно заметить, нам нужно посчитать
        $$
        p_{\xi_1,\xi_2}(t)=\int\limits_{-\infty}^\infty p_1(x)p_2(t-x)~\mathrm dx
        $$
        Если $t\leqslant0$, ответ $0$. Иначе искомая плотность равна $\lambda^2te^{-\lambda t}$. Это же \hyperref[Gamma-распределение]{Gamma-распределение} $\Gamma(2;\lambda)$.
    \end{example}
    \begin{example}
        Сложение \hyperref[Нормальное распределение]{нормальных распределений}. Пусть $\xi_1\sim\operatorname{N}(\mu_1;\sigma_1^2)$, $\xi_2\sim\operatorname{N}(\mu_2;\sigma_2^2)$ независимы, тогда $\xi_1+\xi_2\sim\operatorname{N}(\mu_1+\mu_2;\sigma_1^2+\sigma_2^2)$. Докажите это сами.
    \end{example}
    \section{Об интегралах.}
    \begin{remark}
        Пусть $(\Omega;\A;P)$~--- вероятностное пространство, $\xi$~--- случайный вектор с распределением $P_\xi$. Пусть $g\colon\mathbb R^n\to\mathbb R$ измерима. Тогда
        $$
        \int_\Omega g(\xi(\omega))~P(\mathrm d\omega)=\int_{\mathbb R^n}g(x)~P_\xi(\mathbb dx)
        $$
        Для дискретного случая это равно $\sum g(x_k)P(\xi=x_k)$, а для абсолютно непрерывного~--- $\int g(x)p(x)~\mathrm dx$.
    \end{remark}
    \begin{theorem}[Теорема Фубини]
        Пусть $\xi,\eta$~---независимые случайные величины, $g\colon\mathbb R^2\to\mathbb R$ измерима. Тогда
        $$
        \iint g(x,y)~P_{\xi,\eta}(\mathrm dx,\mathrm dy)=\iint g(x;y)~\mathrm dF_\xi(x)~\mathrm dF_\eta(y)=\iint g(x;y)~\mathrm dF_\eta(y)~\mathrm dF_\xi(x)
        $$
    \end{theorem}
    \section{Числовые характеристики случайных величин.}
    \subsection{Математическое ожидание.}
    \begin{definition}
        Пусть $\xi$~--- случайная величина, тогда её \textbf{математическим ожиданием} называется
        $$
        \Expected\xi=\int\limits_\mathbb Rx~\mathrm dF_\xi(x)
        $$
    \end{definition}
    \begin{property}
        Пусть $\xi_1;\ldots;\xi_n$~--- случайные величины, $g\colon\mathbb R^n\to\mathbb R$. Тогда
        $$
        Eg(\xi_1,\ldots,\xi_n)=\int\limits_{\mathbb R^n}g(\xi_1,\ldots,\xi_n)~P_{\xi_1;\ldots;\xi_n}(\mathrm dx_1;\ldots;\mathrm dx_n)
        $$
    \end{property}
    \begin{property}
        Математическое ожидание линейно.
    \end{property}
    \begin{property}
        Математическое ожидание произведения независимых величин равно произведению их математических ожиданий (обратное неверно).
    \end{property}
    \begin{theorem}[Неравенство Маркова]
        \label{Неравенство Маркова}
        Если $\xi\geqslant0$ и существует $\Expected\xi$, то
        $$
        \forall c>0~P(X\geqslant c)\leqslant\frac{\Expected\xi}c
        $$
    \end{theorem}
    \begin{proof}
        $$
        P(X\geqslant c)=\int_{x\geqslant c}\mathrm dF(x)\leqslant\int_{x\geqslant c}\frac xc\mathrm dF(x)\leqslant\frac1c\int\limits_{\mathbb R}x\mathrm dF(x)=\frac1cE\xi
        $$
    \end{proof}
    \begin{corollary}
        Ожидание индикатора $\xi\in B$ равно $P(\xi\in B)$.
    \end{corollary}
    \begin{property}
        Если $P(\xi\geqslant0)=1$, то $\Expected\xi\geqslant0$.
    \end{property}
    \begin{property}
        Если $\xi\geqslant0$ и $\Expected\xi=0$, то $P(\xi=0)=1$.
    \end{property}
    \begin{proof}
        $$
        1=P(\xi\geqslant0)=P(\xi\geqslant0)+P(\xi<c)\leqslant P(\xi<c)\leqslant 1
        $$
        Отсюда тут везде равенства, значит $P(\xi<c)=1$.
    \end{proof}
    \begin{property}
        Если $\xi$ дискретно с носителем $\mathbb N$, то $\Expected\xi=\sum\limits_{k=1}^\infty P(\xi\geqslant k)$.\\
        Очевидно.
    \end{property}
    \begin{property}
        Если $\xi$ непрерывно $\xi\geqslant0$, то $\Expected\xi=\int\limits_0^{+\infty} P(\xi\geqslant r)~\mathrm dr$
    \end{property}
    \subsection{Дисперсия.}
    \begin{definition}
        \textbf{Дисперсией случайной величины} $\xi$ называется $\operatorname{Var}\xi=\Expected(\xi-\Expected\xi)^2$. Также обозначается $\Variance\xi$.
    \end{definition}
    \begin{property}
        $\Variance\xi\geqslant0$.
    \end{property}
    \begin{property}
        $$
        \Variance\xi=\Expected\xi^2-(\Expected\xi)^2
        $$
    \end{property}
    \begin{property}
        $$
        \Variance(\xi+c)=\Variance\xi
        $$
    \end{property}
    \begin{property}
        $$
        \Variance a\xi=a^2\Variance\xi
        $$
    \end{property}
    \begin{property}
        $\Variance(\xi\pm\eta)=\Variance\xi+\Variance\eta\pm 2(\Expected\xi\eta-\Expected\xi\cdot \Expected\eta)$
    \end{property}
    \begin{corollary}
        Если $\xi,\eta$ независимы, то $\Variance(\xi\pm\eta)=\Variance\xi+\Variance\eta$.
    \end{corollary}
    \begin{theorem}[Неравенство Чебышёва]
        \label{Неравенство Чебышёва}
        Пусть $\xi$~--- случайная величина, существуют $\Expected\xi$ и $\Variance\xi$. Тогда
        $$
        \forall\varepsilon>0~P(|\xi-\Expected\xi|\geqslant\varepsilon)\leqslant\frac{\Variance\xi}{\eta^2}
        $$
    \end{theorem}
    \begin{proof}
        Примените неравенство Маркова для $P(|\xi-\Expected\xi|^2\geqslant\varepsilon^2)$.
    \end{proof}
    \begin{property}
        $$P(\xi-\Expected\xi\geqslant0)=1$$.
    \end{property}
    \begin{proof}
        См. свойство математического ожидания.
    \end{proof}
    \begin{property}
        $$\Variance\xi=\min\limits_{a\in\mathbb R}\Expected(\xi-a)^2$$
    \end{property}
    \begin{proof}
        Искомая формула под минимумом~--- парабола, вершину которой посчитаете сами и найдёте, что она именно в нужной точке.
    \end{proof}
    \subsection{Примеры.}
    \begin{example}
        $$\hyperref[Bern]{\operatorname{Bern}}(p)$$
        $$\Expected\xi=p$$
        $$\Variance\xi=pq$$
    \end{example}
    \begin{example}
        $$\hyperref[Bin]{\operatorname{Bin}}(n,p)$$
        Чтобы доказать свойства, проще всего вспомнить, что это сумма независимых Вернуллиевских величин.
        $$\Expected\xi=np$$
        $$\Variance\xi=npq$$
    \end{example}
    \begin{example}
        $$\hyperref[Pois]{\operatorname{Pois}}(\lambda)$$
        $$\Expected\xi=\sum\limits_{k=1}^\infty ke^{-\lambda}\frac{\lambda^k}{k!}=e^{-\lambda}\lambda\sum\limits_{k=1}^\infty\frac{\lambda^{k-1}}{(k-1)!}=\lambda$$
        Для подсчёта дисперсии заметим, что $\Expected(\xi(\xi-1))=\Variance\xi+(\Expected\xi)^2-\Expected\xi$. Тогда
        $$\Expected(\xi(\xi-1))=\sum\limits_{k=2}^\infty k(k-1)\frac{\lambda^ke^{-\lambda}}{k!}=\lambda^2e^{-\lambda}\sum\limits_{k=2}^\infty \frac{\lambda^{k-2}}{(k-2)!}=\lambda^2$$
        Отсюда
        $$\Variance\xi=\lambda$$
    \end{example}
    \begin{example}
        $$\hyperref[Geom]{\operatorname{Geom}}(p)$$
        Будем считать носителем $\mathbb Z_+$.
        $$\Expected\xi=\sum\limits_{k=1}^\infty P(x\geqslant k)=\sum\limits_{k=1}^\infty\sum\limits_{i=k}^\infty(1-p)^ip=\sum\limits_{k=1}^\infty p\frac{(1-p)^k}{1-(1-p)}=\frac{1-p}p$$
        $$
        \Expected(\xi(\xi-1))=\sum\limits_{k=2}^\infty k(k-1)(1-p)^{k}p
        =(1-p)^2p\sum\limits_{k=2}^\infty k(k-1)(1-p)^{k-2}
        =(1-p)^2p\sum\limits_{k=2}^\infty \frac{\partial^2(1-p)^k}{\partial p^2}
        =(1-p)^2p\frac{\partial^2}{\partial p^2}\sum\limits_{k=2}^\infty(1-p)^k\overset{\substack{\text{дальше}\\\text{сам}}}=\frac{2(1-p)^2}{p^2}
        $$
        $$\Variance\xi=\frac{1-p}{p^2}$$
    \end{example}
    \begin{example}
        Равномерное распределение на $[a;b]$.\\
        Заметим тот факт, что $\xi$~--- равномерное распределение на $[a;b]$ можно выразить через $\eta$~--- равномерное распределение на $[0;1]$:
        $$
        \xi=(b-a)\eta+a
        $$
        Отсюда
        $$\Expected\xi=\frac{a+b}2$$
        $$\Variance\xi=\frac{(a+b)^2}{12}$$
    \end{example}
    \begin{example}
       $$\hyperref[N]{\operatorname{N}}(\mu;\sigma^2)$$
       $$\Expected\xi=\mu$$
       $$\Variance\xi=\sigma^2$$
       Сами как-нибудь проинтегрируете, мне лень.
    \end{example}
    \begin{example}
        $$\hyperref[Exp]{\operatorname{Exp}}(\lambda)$$
        $$\Expected\xi=\int\limits_0^{+\infty}x\lambda e^{-\lambda x}~\mathrm dx=-xe^{-\lambda x}\Big|_0^{+\infty}+\frac1\lambda\int\limits_0^{+\infty}\lambda e^{-\lambda x}~\mathrm dx=\frac1\lambda$$
        $$\Expected\xi^2=\int\limits_0^{+\infty}x^2\lambda e^{-\lambda x}~\mathrm dx=-x^2e^{-\lambda x}\Big|_0^{+\infty}+\frac2\lambda\int\limits_0^{+\infty}\lambda xe^{-\lambda x}~\mathrm dx=\frac2{\lambda^2}$$
        $$\Variance\xi=\frac1{\lambda^2}$$
    \end{example}
    \begin{example}
        $$\hyperref[Gamma-распределение]{\operatorname{\Gamma}}(n,\lambda)=\operatorname{Exp}(\lambda)+\cdots+\operatorname{Exp}(\lambda)$$
        $$\Expected\xi=\frac n\lambda$$
        $$\Variance\xi=\frac n{\lambda^2}$$
        Доказывать для нецелого $n$ не будем.
    \end{example}
    \begin{example}
        $$\hyperref[Cauchy]{\operatorname{Cauchy}}(0;1)$$
        $$\Expected\xi=\frac1\pi\int\limits_{-\infty}^{+\infty}\frac{x~\mathrm dx}{1+x^2}$$
        Мат. ожидание не существует.
    \end{example}
    \subsection{Моменты и связанное с ними.}
    \begin{definition}
        \textbf{Моментом случайной величины} $\xi$ порядка $k$ называется $\Expected \xi^k$.
    \end{definition}
    \begin{definition}
        \textbf{Абсолютным моментом случайной величины} $\xi$ порядка $k$ называется $\Expected |\xi|^k$.
    \end{definition}
    \begin{definition}
        \textbf{Центральным моментом случайной величины} $\xi$ порядка $k$ называется $\Expected (\xi-\Expected\xi)^k$.
    \end{definition}
    \begin{definition}
        \textbf{Абсолютным центральным моментом случайной величины} $\xi$ порядка $k$ называется $\Expected|\xi-\Expected\xi|^k$.
    \end{definition}
    \begin{definition}
        \textbf{Коэффициентом ассиметрии случайной величины} $\xi$ называется $\frac{\Expected (\xi-\Expected\xi)^3}{\sigma^3}$.
    \end{definition}
    \begin{definition}
        \textbf{Коэффициентом эксцесса случайной величины} $\xi$ называется $\frac{\Expected (\xi-\Expected\xi)^4}{\sigma^4}-3$.
    \end{definition}
    \subsection{Мода.}
    \begin{definition}
        \textbf{Модой дискретной случайной величины} $\xi$ называется число $x_k^*=\operatorname{argmax}_{x_k}P(\xi=x_k)$.
    \end{definition}
    \begin{definition}
        \textbf{Модой непрерывной случайной величины} $\xi$ называется число $x^*=\operatorname*{argmax}_{x\in\mathbb R}p(x)$.
    \end{definition}
    \begin{definition}
        Случайная величина называется \textbf{$n$-модальной} (\textbf{унимодальной} для $n=1$, \textbf{бимодальной} для $n=2$...), если у неё $n$ \underline{\textit{локальных}} максимумов.
    \end{definition}
    \subsection{Квантиль, медиана.}
    \begin{definition}
        \textbf{Квантилью случайной величины} $\xi$ порядка $\alpha\in(0;1)$ называется такое число $q_\alpha$, что $P(\xi\geqslant q_\alpha)\geqslant1-\alpha$ и $P(\xi\leqslant q_\alpha)\leqslant\alpha$.\\
        Квантиль порядка $\frac12$ называется \textbf{медианой}.
    \end{definition}
    \begin{remark}
        В дискретном случае определяется неоднозначно. Есть несколько способов это исправить, обращайте внимание на то, какой используется в Вашей задаче.
    \end{remark}
    \begin{theorem}
        Пусть $m=\operatorname{med}\xi$. Тогда $m=\operatorname*{argmin}_{a\in\mathbb R}{\Expected|\xi-a|}$, если для любого $a$ искомое ожидание существует.
    \end{theorem}
    \begin{proof}
        Н.У.О. $m=0$, иначе сдвинем $\xi$. Сравним $\Expected|\xi-c|$ и $\Expected|\xi|$. Пусть $c>0$. Тогда
        $$
        |\xi-c|-|\xi|=\begin{cases}
            -c & \xi>c\\
            c-2\xi & 0<\xi\leqslant c,c-2\xi\geqslant -c\\
            c & x\leqslant0
        \end{cases}
        $$
        отсюда
        $$
        \Expected(|\xi-c|-|\xi|)=\Expected(|\xi-c|-|\xi|)\mathbb1(\xi\leqslant0)+\Expected(|\xi-c|-|\xi|)\mathbb1(\xi>0)\geqslant c\Expected\mathbb1(\xi\leqslant1)-c\Expected\mathbb1(\xi>c)=c(2P(\xi\leqslant0)-1)\geqslant0
        $$
        То есть $\Expected|\xi-c|$ больше либо равно $\Expected|\xi|$.
    \end{proof}
    \subsection{Ковариация. Коэффициентом корреляции.}
    \begin{definition}
        \textbf{Ковариацией случайных величин} $\xi$ и $\eta$ называется $\Covariance(\xi;\eta)=\Expected(\xi-\Expected\xi)\Expected(\eta-\Expected\eta)$
    \end{definition}
    \begin{property}
        $$
        \Covariance(\xi;\eta)=\Expected(\xi\eta)-\Expected\xi\Expected\eta
        $$
    \end{property}
    \begin{property}
        $$
        \Variance(\xi_1+\cdots+\xi_n)=\sum\Variance\xi_i+\sum\Covariance(\xi_i;\xi_j)
        $$
    \end{property}
    \begin{property}
        $$
        \Covariance(\xi;\eta)=\Covariance(\eta;\xi)
        $$
    \end{property}
    \begin{property}
        $$
        \Covariance(\xi;\xi)=\Variance(\xi)
        $$
    \end{property}
    \begin{property}
        Ковариация линейна по обоим аргументам.
    \end{property}
    \begin{property}
        $$
        \Covariance(\xi;c)=0
        $$
    \end{property}
    \begin{property}
        Ковариация независимых величин равна нулю. Обратное в общем случае неверно.
    \end{property}
    \begin{example}
        Пусть $\xi\sim\hyperref[N]{\operatorname{N}(0;1)}$, $\eta=\xi^2$. Они не независимы, при этом
        $$
        \Expected\xi=0
        $$
        $$
        \Expected\xi\eta=\frac1{\sqrt{2\pi}}\int\limits_{-\infty}^{+\infty}x^3e^{-x^2/2}~\mathrm dx=0
        $$
        Значит корреляция равна нулю.
    \end{example}
    \begin{definition}
        \textbf{Коэффициентом корреляции случайных величин} $\xi$ и $\eta$ называется $\rho(\xi;\eta)=\frac{\Covariance(\xi;\eta)}{\sqrt{\Variance\xi\Variance\eta}}$
    \end{definition}
    \begin{property}
        $$
        |\rho(\xi;\eta)|\leqslant1
        $$
    \end{property}
    \begin{proof}
        Чтобы это доказать, посмотрим на следующее свойство и рассмотрим $\Variance(\tilde\xi+\tilde\eta)$ и $\Variance(\tilde\xi-\tilde\eta)$.
    \end{proof}
    \begin{property}
        $$
        \rho(\xi;\eta)=\Covariance\left(\underbrace{\frac{\xi-\Expected\xi}{\sqrt{\Variance\xi}}}_{\tilde\xi};\underbrace{\frac{\eta-\Expected\eta}{\sqrt{\Variance\eta}}}_{\tilde\eta}\right)
        $$
    \end{property}
    \begin{property}
        Если $\eta=a\xi+b,a\neq0$, то
        $$
        \rho(\xi;\eta)=\sign a
        $$
    \end{property}
    \begin{property}
        Если $\rho(\xi;\eta)=1$, то $\tilde\xi-\tilde\eta=c$.
        Если $\rho(\xi;\eta)=-1$, то $\tilde\xi+\tilde\eta=c$.
    \end{property}
    \begin{definition}
        Две величины \textbf{некоррелированы}, если их ковариация (следовательно, коэффициент корреляции) равна нулю.
    \end{definition}
    \begin{claim}
        Независимость влечёт некоррелированность, обратное в общем случае неверно.
    \end{claim}
    \subsection{Характеристики векторных случайных величин.}
    \begin{definition}
        \textbf{Математическим ожиданием случайного вектора} называется вектор математических ожиданий его компонент.
    \end{definition}
    \begin{theorem}
        $\Expected A\xi=A\Expected\xi$
    \end{theorem}
    \begin{definition}
        \textbf{Дисперсией случайного вектора} $\xi$ называется
        $$
        \Expected(\xi-\Expected\xi)(\xi-\Expected\xi)^T
        $$
        Несложно заметить, что это матрица ковариаций компонент.
    \end{definition}
    \begin{property}
        $$
        \Variance\xi=(\Variance\xi)^T
        $$
    \end{property}
    \begin{property}
        $$
        \Variance\xi\geqslant0
        $$
        (Речь идёт про неотрицательную определённость.)
    \end{property}
    \begin{proof}
        Рассмотрим $T^T\Variance\xi T$
        $$
        T^T\Variance\xi T=T^T\Expected(\xi-\Expected\xi)(\xi-\Expected\xi)^TT=\Expected\underbracket{T^T(\xi-\Expected\xi)}\underbracket{(\xi-\Expected\xi)^TT}=\Expected(T^T(\xi-\Expected\xi))^2\geqslant0
        $$
    \end{proof}
    \begin{property}
        $$
        \Variance(\xi+c)=\Variance\xi
        $$
    \end{property}
    \begin{property}
        Если $\xi$ и $\eta$ независимы, то
        $$
        \Variance(\xi+\eta)=\Variance\xi+\Variance\eta
        $$
    \end{property}
    \begin{proof}
        Если $i=j$, то просто имеем дисперсии на диагоналях, и для одномерных дисперсий мы знаем такой результат.\\
        Иначе раскроем коварицаии по линейности  получим искомое.
    \end{proof}
    \begin{example}
        $$\hyperref[Poly]{\operatorname{Poly}}(n,p)$$
        Вспомним, что наша случайная величина является суммой независимых случайных величин с распределением $\operatorname{Poly}(1,p)$. Математические ожидание же $\xi_1\sim\operatorname{Poly}(1,p)$ равно $\left(\begin{matrix}
            p_1\\p_2\\\vdots\\p_m
        \end{matrix}\right)$.\\
        Дисперсия также считается как $n$ умножить на $\Variance\xi_1$. Осталось только это посчитать.
        $$
        \Variance\xi_1=\left(\begin{matrix}
            \Covariance(\xi_1^{(1)},\xi_1^{(1)}) & \Covariance(\xi_1^{(1)},\xi_1^{(2)}) & \cdots & \Covariance(\xi_1^{(1)},\xi_1^{(m)})\\
            \Covariance(\xi_1^{(2)},\xi_1^{(1)}) & \Covariance(\xi_1^{(2)},\xi_1^{(2)}) & \cdots & \Covariance(\xi_1^{(2)},\xi_1^{(m)})\\
            \vdots & \vdots & \ddots & \vdots\\
            \Covariance(\xi_1^{(m)},\xi_1^{(1)}) & \Covariance(\xi_1^{(m)},\xi_1^{(2)}) & \cdots & \Covariance(\xi_1^{(m)},\xi_1^{(m)})\\
        \end{matrix}\right)
        $$
        Если $i=j$, то $\Covariance(\xi_1^{(i)}l\xi_1^{(i)})=\Variance\xi_1^{(i)}=\Expected(\xi_1^{(i)})^2-(\Expected\xi_1^{(i)})^2=p_i(1-p_i)$.\\
        Если $i\neq j$, то $\Covariance(\xi_1^{(i)}l\xi_1^{(j)})=\Expected(\xi_1^{(i)}\xi_1^{(j)})-\Expected\xi_1^{(i)}\Expected\xi_1^{(j)}=-p_ip_j$.
        Посмотрим на $\Expected(\xi_1^{(i)}\xi_1^{(j)})$. Тут просто выражение под мат.ожиданием ноль, ведь в $\xi_1$ только одна компонента равна единице.
    \end{example}
    \begin{example}
        $$\hyperref[N (многомерное)]{\operatorname{N}}(\mu;\Sigma)$$
        Дл этого рассмотрим стандартный Гауссовский вектор, а потом рассмотрим нестандартный. Ну, у него компоненты~--- независимые $\operatorname{N}(0;1)$. У каждого мат. ожидание ноль, а дисперсия один. И в силу независимости ковариации равны нулю, а значит $\Expected\xi=\mathbb0$, $\Variance\xi$~--- единичная матрица.\\
        Теперь $\operatorname{N}(\mu;\Sigma)$. Мы знаем, что если $\eta=A\xi+\mu$ и $\xi\sim\operatorname{N}(\mathbb0;E)$, то $\eta\sim\operatorname{N}(\mu;AA^T)$, а отсюда $\Expected\eta=\mu$, $\Variance\eta=AA^T$.\\
        При этом на линейной алгебре доказывали, что для $\Sigma\geqslant0$ и $\Sigma=\Sigma^T$ у $\Sigma$ существует корень. Отсюда $\operatorname{\mu;\Sigma}$ подходит под предыдущий случай.
    \end{example}
    \begin{claim}
        $$\left(\begin{matrix}
            \eta_1\\\eta_2
        \end{matrix}\right)\sim\operatorname{N}(\left(\begin{matrix}
            \mu_1\\\mu_2
        \end{matrix}\right);\left(\begin{matrix}
            \Sigma_{11} & \Sigma_{12}\\\Sigma_{21} & \Sigma_{22}
        \end{matrix}\right))
        \Rightarrow\xi_1\sim\operatorname{N}(\mu_1;\Sigma_{11}),\xi_2\sim\operatorname{N}(\mu_2;\Sigma_{22})
        $$
    \end{claim}
    \begin{proof}
        Рассмотрим спектральное представление $\Sigma$:
        $$
        \left(\begin{matrix}
            \Sigma_{11} & \Sigma_{12}\\\Sigma_{21} & \Sigma_{22}
        \end{matrix}\right)=
        \left(\begin{matrix}
            U_{11} & U_{12}\\U_{21} & U_{22}
        \end{matrix}\right)
        \left(\begin{matrix}
            \Lambda_{1} & \mathbb0\\\mathbb0 & \Lambda_{2}
        \end{matrix}\right)
        \left(\begin{matrix}
            U_{11}^T & U_{21}^T\\U_{12}^T & U_{22}^T
        \end{matrix}\right)
        $$
        Корень из этой байды равен тому же самому, только с корнями $\sqrt{\Lambda_i}$. В итоге
        $$
        \left(\begin{matrix}
            \eta_{1}\\\eta_{2}
        \end{matrix}\right)=
        \left(\begin{matrix}
            U_{11} & U_{12}\\U_{21} & U_{22}
        \end{matrix}\right)
        \left(\begin{matrix}
            \sqrt{\Lambda_{1}} & \mathbb0\\\mathbb0 & \sqrt{\Lambda_{2}}
        \end{matrix}\right)
        \left(\begin{matrix}
            U_{11}^T & U_{21}^T\\U_{12}^T & U_{22}^T
        \end{matrix}\right)
        \left(\begin{matrix}
            \xi_{1}\\\xi_{2}
        \end{matrix}\right)
        $$
        Где $\xi_1$ и $\xi_2$~--- нормальные Гауссовские векторы.\\
        Для того, чтобы проверить то, что мы хотим, надо всего лишь посмотреть на эту формулу и узнать, что будет её первой компонентой, и что~--- второй. Получим именно то, что нужно.
    \end{proof}
    \begin{claim}
        Для нормальных случайных векторов некоррелированность влечёт независимость.
    \end{claim}
    \begin{proof}
        Пусть мы знаем некоррелированность. Тогда
        $$\left(\begin{matrix}
            \eta_1\\\eta_2
        \end{matrix}\right)\sim\operatorname{N}(\left(\begin{matrix}
            \mu_1\\\mu_2
        \end{matrix}\right);\left(\begin{matrix}
            \sigma_{\xi_1}^2 & \mathbb0\\\mathbb0 & \sigma_{\xi_2}^2
        \end{matrix}\right))
        $$
        Если $\sigma_{\xi_1}=0$ или $\sigma_{\xi_2}=0$, то всё очевидно. Иначе
        $$
        p(x_1;x_2)=\frac1{2\pi\sqrt{\sigma_{\xi_1}^2\sigma_{\xi_2}^2}}\underbrace{\exp\left(-\frac12\left(\begin{matrix}
            x_1-\mu_1\\x_2\mu_2
        \end{matrix}\right)^T
        \left(\begin{matrix}
            \sigma_{\xi_1}^{-2} & \mathbb0\\\mathbb0 & \sigma_{\xi_2}^{-2}
        \end{matrix}\right)
        \left(\begin{matrix}
            x_1-\mu_1\\x_2-\mu_2
        \end{matrix}\right)\right)}_{\exp\left(-\frac12\left(\frac{(x_1-\mu_1)^2}{\sigma_{\xi_1}^2}+\frac{(x_2-\mu_2)^2}{\sigma_{\xi_2}^2}\right)\right)}
        $$
    \end{proof}
    \section{Вероятностные неравенства.}
    \begin{remark}
        См. также \ref{Неравенство Маркова} \ref{Неравенство Чебышёва}
    \end{remark}
    \begin{theorem}[Слабый закон больших чисел]
        \label{Слабый закон больших чисел}
        Пусть $\xi_1;\ldots;\xi_n$~--- независимые одинаково распределённые величины. И пусть у них ожидание $\mu$ и дисперсия $\sigma^2$. Тогда
        $$
        \forall\varepsilon>0~P(|\overline\xi-\mu|\geqslant\varepsilon)\underset{n\to\infty}\longrightarrow0
        $$
        Где
        $$
        \overline\xi=\frac{\sum\limits_{i=1}^n\xi_i}n
        $$
    \end{theorem}
    \begin{proof}
        $$
        P(|\overline\xi-\mu|\geqslant\varepsilon)\leqslant\frac{\Variance\overline\xi}{\varepsilon^2}=\frac{\sigma^2}{n\varepsilon^2}
        $$
    \end{proof}
    \begin{definition}
        $$
        L_2=\{\xi\mid\Expected\xi^2<\infty\}
        $$
    \end{definition}
    \begin{claim}
        Пусть $\xi,\eta\in L_2$, Тогда
        $$
        |\Expected\xi\eta|\leqslant\Expected\xi^2\Expected\eta^2
        $$
    \end{claim}
    \begin{proof}
        Это просто неравенство Коши~--- Буняковского~--- Шварца для скалярного произведения $(\xi;\eta)\mapsto\Expected\xi\eta$.
    \end{proof}
    \begin{claim}
        Пусть $\xi,\eta\in L_2$, Тогда
        $$
        (\Covariance(\xi;\eta))^2\leqslant\Variance\xi\Variance\eta
        $$
    \end{claim}
    \begin{proof}
        Это просто неравенство Коши~--- Буняковского~--- Шварца для скалярного произведения $(\xi;\eta)\mapsto\Expected(\xi-\Expected\xi)(\eta-\Expected\eta)$.
    \end{proof}
    \begin{claim}
        Если $g$ выпукла вниз, то $g(\Expected\xi)\leqslant\Expected g(\xi)$.
    \end{claim}
    \begin{proof}
        неравенство Йенсена.
    \end{proof}
    \begin{claim}
        Если $p,q$~--- сопряжённые показатели, то
        $$
        \Expected|\xi\eta|\leqslant(\Expected|\xi|^p)^{1/p}(\Expected|\eta|^q)^{1/q}
        $$
    \end{claim}
    \begin{proof}
        Неравенство Гёльдера.
    \end{proof}
    \begin{claim}
        Если $p\geqslant1$, то
        $$
        (\Expected|\xi+\eta|^p)^{1/p}\leqslant(\Expected|\xi|^p)^{1/p}+(\Expected|\eta|^p)^{1/p}
        $$
    \end{claim}
    \begin{proof}
        Неравенство Минковского.
    \end{proof}
    \begin{claim}[неравенства Ляпунова]
        Пусть $p<q$. Тогда
        $$
        (\Expected|\xi|^p)^{1/p}\leqslant(\Expected|\xi|^q)^{1/q}
        $$
    \end{claim}
    \section{Условные распределения, мат. ожидания и дисперсии.}
    \begin{claim}
        Пусть $\xi_1;\ldots;\xi_n$~--- дискретный вектор. Тогда
        $$
        P(\xi_1=x_1,\ldots,\xi_k=x_k\mid \xi_{k+1}=x_{k+1},\ldots,\xi_n=x_n)=\frac{P(\xi_1=x_1,\ldots,\xi_n=x_n)}{P(\xi_{k+1}=x_{k+1},\ldots,\xi_n=x_n)}
        $$
        \\
        Пусть $\xi_1;\ldots;\xi_n$~--- абсолютно непрерывный вектор. Тогда
        $$
        p(x_1,\ldots,x_k\mid x_{k+1},\ldots,x_n)=\frac{p_{\xi_1;\ldots;\xi_n}(x_1,\ldots,x_n)}{p_{\xi_{k+1};\ldots;\xi_n}(x_{k+1},\ldots,x_n)}
        $$
    \end{claim}
    \begin{corollary}
        Если $(\xi;\eta)$ дискретны, то
        $$
        P(\xi=k)=\sum\limits_jP(\xi=k\mid\eta=y_j)P_\eta(\eta=y_i)
        $$
        Если $(\xi;\eta)$ непрерывны, то
        $$
        p(x)=\int p(x\mid y)p_\eta(y)~\mathrm dy
        $$
    \end{corollary}
    \begin{definition}
        Условное математическое ожидание $E(\xi\mid\eta)$ в дискретном случае равно
        $$
        \sum\limits_kx_kP(\xi=x_k\mid\eta)
        $$
        в непрерывном случае равно
        $$
        \int xp(x\mid y)~\mathrm dx
        $$
    \end{definition}
    \begin{property}
        $$
        \Expected(\xi\mid\eta)=\operatorname*{argmin}_{f(\eta)\in L_2}\Expected(\xi-f(\eta))^2
        $$
    \end{property}
    \begin{property}
        $$
        \Expected(\xi\mid\xi)=\xi
        $$
    \end{property}
    \begin{property}
        Если $\xi$ и $\eta$ независимы, то
        $$
        \Expected(\xi\mid\eta)=\Expected\xi
        $$
    \end{property}
    \begin{claim}[Формула полной вероятности для математического ожидания]
        \begin{property}
            $$
            \Expected(\xi)=\Expected\Expected(\xi\mid\eta)
            $$
        \end{property}
    \end{claim}
    \begin{definition}
        \textbf{Условная дисперсия} $\Variance(\xi\mid\eta)$ равна
        $$
        \Expected((\xi-\Expected(\xi\mid\eta))^2\mid\eta)=\Expected(\xi^2\mid\eta)-(\Expected(\xi\mid\eta))^2
        $$
    \end{definition}
    \begin{property}
        $$
        \Variance\eta=\Expected\Variance(\eta\mid\xi)+\Variance\Expected(\xi\mid\eta)
        $$
    \end{property}
    \begin{proof}
        $$
        \Expected\eta^2=\Expected\Expected(\eta^2\mid\xi)=\Expected\left(\Variance(\eta\mid\xi)+(\Expected(\eta\mid\xi))^2\right)
        $$
        $$
        \Expected\eta=\Expected\Expected(\eta\mid\xi)
        $$
        Тогда
        $$
        \Variance\eta=\Expected\eta^2-(\Expected\eta)^2=\Expected\Variance(\eta\mid\xi)+\underbrace{\Expected(\Expected(\eta\mid\xi))^2-(\Expected\Expected(\eta\mid\xi))^2}_{\Variance\Expected(\xi\mid\eta)}
        $$
    \end{proof}
    \begin{example}
        Пусть
        $$\left(\begin{matrix}
            \xi_1\\\xi_2
        \end{matrix}\right)\sim\hyperref[N (многомерное)]{\operatorname{N}}(\mu;\Sigma)$$
        Тогда
        $$
        \Expected(\xi_2\mid\xi_1)=\mu_2+\frac{\Covariance(\xi_1;\xi_2)}{\Variance\xi_1}(\xi_1-\mu_1)
        $$
        Для доказательства введём
        $$
        \eta=\xi_2-\frac{\Covariance(\xi_1;\xi_2)}{\Variance\xi_1}\xi_1
        $$
        Несложно заметить, что $\Covariance(\eta;\xi_1)=0$, а значит они независимы. Тогда
        $$
        \Expected(\xi_2\mid\xi_1)=\Expected(\eta\mid\xi_1)+\frac{\Covariance(\xi_1;\xi_2)}{\Variance\xi_1}\Expected(\xi_1\mid\xi_1)
        $$
    \end{example}
    \section{Сходимости.}
    \begin{definition}
        Пусть $\xi_n$, $\xi$~--- случайные величины на одном пространстве. Говорят, что $\xi_n$ \textbf{сходится к $\xi$ почти наверное}, если
        $$
        P({\omega\mid \lim\limits_{n\to\infty}\xi_n(\omega)=\xi(\omega)})=1
        $$
    \end{definition}
    \begin{definition}
        Пусть $\xi_n$, $\xi$~--- случайные величины на одном пространстве. Говорят, что $\xi_n$ \textbf{сходится к $\xi$ по вероятности}, если
        $$
        \forall\varepsilon>0~P(|\xi_n-\xi|\geqslant\varepsilon)\underset{n\to\infty}\longrightarrow0
        $$
    \end{definition}
    \begin{definition}
        Пусть $\xi_n$, $\xi$~--- случайные величины на одном пространстве. Говорят, что $\xi_n$ \textbf{сходится к $\xi$ в среднем порядка $p$}, если
        $$
        \Expected|\xi_n-\xi|^p\underset{n\to\infty}\longrightarrow0
        $$
    \end{definition}
    \begin{definition}
        Пусть $\xi_n$, $\xi$~--- случайные величины. Говорят, что $\xi_n$ \textbf{сходится к $\xi$ по распределению}, если
        $$
        \forall f\text{ непрерывна и ограничена}~\Expected f(\xi_n)\rightarrow \Expected f(\xi)
        $$
    \end{definition}
    \begin{theorem}
        Сходимость почти наверное влечёт сходимость по вероятности.\\
        Сходимость в среднем порядка $p$ влечёт сходимость по вероятности.\\
        Сходимость по вероятности влечёт сходимость по распределению.\\
    \end{theorem}
    \begin{proof}
        \begin{itemize}
            \item[$p\to d$]
            $$
            |\Expected(f(\xi_n)-f(\xi))|\leqslant\Expected|f(\xi_n)-f(\xi)|\mathbb1(|\xi_n-\xi|\geqslant\delta_\varepsilon)+\Expected|f(\xi_n)-f(\xi)|\mathbb1(|\xi_n-\xi|<\delta_\varepsilon)\leqslant2MP(|\xi_n-\xi|\geqslant\delta_\varepsilon)+\varepsilon
            $$
            Здесь $\delta_\varepsilon$~--- из определения непрерывности $f$, $M$~--- то, чем ограничена $f$.
            \item[$L_p\to p$]
            $$
            P(|\xi_n-\xi|\geqslant\varepsilon)\leqslant
            P(|\xi_n-\xi|^p\geqslant\varepsilon^p)\leqslant
            \frac{\Expected|\xi_n-\xi|^p}{\varepsilon^p}\longrightarrow0
            $$
            \item[$a.s.\to p$]
            Пусть $O=\{\omega\mid\xi_n(\omega)\nrightarrow\xi(\omega)\}$. Известно $P(O)=0$.\\
            Пусть $\varepsilon>0$, $A_1=\bigcup\limits_{m\geqslant n}\{\omega\mid|\xi_m(\omega)-\xi(\omega)|>\varepsilon\}$.\\
            Очевидно, $A_{n+1}\subset A_n$, следовательно $P(A_n)\rightarrow P(A)$, где $A=\bigcap\limits_n A_n$.\\
            Пусть $\omega\in O\compl$. Тогда
            $$\exists N(\varepsilon,\omega)~\forall n>N~|\xi_n(\omega)-\xi(\omega)|<\varepsilon$$
            Это значит $\omega\notin A_N$, следовательно $\omega\in A\compl$. Отсюда $P(A)=0$.\\
            Осталось заметить, что
            $$P(|\xi_n-\xi|\geqslant\varepsilon)\leqslant P(A_n)$$
        \end{itemize}
    \end{proof}
    \begin{remark}
        Ни одна из обратных импликаций в общем случае неверна. Ни одна из импликаций между $a.s.$ и $L_p$ не влечёт другую.
    \end{remark}
    \begin{example}
        $$d\nrightarrow p$$
        Пусть $P(\omega_1)=P(\omega_2)=\frac12$. Пусть
        $$
        \xi(\omega_i)=(-1)^i\qquad\xi_n(\omega_i)=(-1)^{i+n}
        $$
        Тогда
        $$
        \Expected f(\xi)=\frac{f(\xi(\omega_1))+f(\xi(\omega_2))}2=\frac{f(1)+f(-1)}2=\Expected f(\xi_n)
        $$
        Однако
        $$
        P(\xi_n(\omega_i)-\xi(\omega_i))=\pm2\nrightarrow0
        $$
    \end{example}
    \begin{example}
        $$a.s.\nrightarrow L_p$$
        Возьмём равномерное распределение на $[0;1]$. Пусть $\xi\equiv0$,
        $$\xi_n(\omega)=\begin{cases}
            e^n & \omega\in[0;1/n]\\0 & \mathrm{otherwise}
        \end{cases}$$
        Очевидно, сходимость почти наверное есть. Но
        $$
        \Expected|\xi_n-\xi|^p=\frac{e^{np}}n\nrightarrow0
        $$
    \end{example}
    \begin{example}
        $$
        p\nrightarrow a.s.
        $$
        Пусть
        $$\xi_{2^n}(\omega)=\begin{cases}
            \frac1{2^n} & \omega\in[0;\frac1{2^n}]\\0&\mathrm{otherwise}
        \end{cases}$$
        $$
        \forall m~\exists n:m\in(2^n;2^{n+1})~\xi_m(\omega)=\begin{cases}
            \frac1{2^n} & \omega\in[\frac{m-2^n}{2^n};\frac{m-2^n+1}{2^n}]\\0&\mathrm{otherwise}
        \end{cases}
        $$
        Тогда
        $$
        P(|\xi_m(\omega)-\xi(\omega)|>\varepsilon)\rightarrow0
        $$
    \end{example}
    \begin{example}
        $$
        L_p\nrightarrow a.s.
        $$
        Возьмём предыдущий пример и заменим $2^n$ на $n!$.
    \end{example}
    \begin{theorem}
        Пусть $F_n,F$~--- функции распределения. Тогда сходимость по распределению равносильна $$
        \forall x\in C(F)~F_n(x)\to F(x)
        $$
    \end{theorem}
    \begin{proof}
        \begin{itemize}
            \item[$\Rightarrow$]
            Пусть $x_0\in C(F)$. Пусть
            $$
            f_\varepsilon(t)=\begin{cases}
                1 & t\leqslant x_0\\
                0 & t\geqslant x_0+\varepsilon\\
                \text{линейно} & \mathrm{otherwise}
            \end{cases}
            $$
            $$F_n(x_0)=\int\limits_{-\infty}^{x_0}~\mathrm dF_n(x)=\int\limits_{-\infty}^{x_0}f_\varepsilon(t)~\mathrm dF_n(x)\leqslant =\int\limits_{-\infty}^{+\infty}f_\varepsilon(t)~\mathrm dF_n(x)\rightarrow \int\limits_{-\infty}^{x_0+\varepsilon}f_\varepsilon(t)~\mathrm dF(x)\leqslant F(x+\varepsilon)$$
            \item[$\Leftarrow$]
            Пока не будем доказывать.
        \end{itemize}
    \end{proof}
    \begin{remark}
        Условие в теореме равносильно.
        $$\forall x,y~F_n(x)-F_n(y)\rightarrow F(x)-F(y)$$
    \end{remark}
    \begin{remark}
        Если $f\in C(\mathbb R)$, то
        $$\sup\limits_{x\in\mathbb R}|F_n(x)-F(x)|\rightarrow0$$
    \end{remark}
    \begin{remark}
        Если $F_n,F$ дискретны и имеют одинаковый носитель, то для сходимости по распределению достаточно доказать, что вероятность $\xi_n=x_k$ сходится к $\xi=x_k$.
    \end{remark}
    \begin{theorem}[Свойства сходимостей]
        \begin{enumerate}
            \item Пусть $\xi_n\overset{a.s.}\rightarrow\xi$, $|\xi_n|\overset{a.s.}\leqslant\eta$, $\Expected\eta<+\infty$. Тогда $\Expected\xi_n\rightarrow\Expected\xi$.
            \item Пусть $\xi_n\overset p\rightarrow\xi$, $f\in C(\mathbb R)$. Тогда $f(\xi_n)\overset p\rightarrow f(\xi)$.
            \item Пусть $\xi_n-\eta_n\overset p\rightarrow0$, $\xi_n\overset p\rightarrow\xi$, тогда $\eta_n\overset p\rightarrow\xi$.
            \item Пусть $\xi_n-\eta_n\overset p\rightarrow0$, $\eta_n\overset d\rightarrow\eta$, тогда $\xi_n\overset p\rightarrow\eta$.
            \item Если $\xi_n\overset d\rightarrow\xi$, $\eta_n\overset p\rightarrow0$, то $\xi_n\eta_n\overset p\rightarrow\xi$.
            \item $\xi_n\overset d\rightarrow C$ тогда и только тогда, когда $\xi_n\overset p\rightarrow C$.
            \item Если $\xi_n\overset d\rightarrow\xi$, $\eta_n\overset p\rightarrow C$, то $\xi_n+\eta_n\overset d\rightarrow\xi+C$, $\xi_n\eta_n\overset d\rightarrow\xi C$, если $C\neq0$, то ещё $\frac{\xi_n}{\eta_n}\overset d\rightarrow\frac{\xi}C$.
            \item Арифметические операции сохраняют сходимость по вероятности.
        \end{enumerate}
    \end{theorem}
    \begin{theorem}
        Пусть $\scriptF$~--- множество функций распределений. Пусть $\scriptG$~--- множество неубывающих функций, непрерывных справа, $g(+\infty)\leqslant1$, $g(-\infty)\geqslant 0$. Тогда для любой последовательности $\{g_n\}\subset\scriptG$. Тогда существует подпоследовательность $g_{n_k}$, сходящаяся к $g\in\scriptG$.
    \end{theorem}
    \begin{remark}
        Для $\scriptF$ это в общем случае неверно:
        $$
        f_n(t)=\begin{cases}
            0 & t<-n\\
            \frac12 & t\in[-n;n)\\
            1 & t\geqslant n
        \end{cases}
        $$
        Тогда эта штука сходится к $\frac12\notin\scriptF$.
    \end{remark}
    \begin{definition}
        Последовательность случайных величин называется \textbf{плотной}, если
        $$
        \forall\varepsilon>0~\exists N~\inf\limits_n P(-N\leqslant\xi_n\leqslant N)>1-\varepsilon
        $$
    \end{definition}
    \begin{definition}
        \textbf{Класс $\scriptL$ определяет распределение}, если $\scriptL$~--- подмножество непрерывных ограниченных функций и
        $$\forall f\in L~\int f~\mathrm dG=\int f~\mathrm dF\Rightarrow F=G$$
    \end{definition}
    \begin{theorem}
        Пусть $\{F_n\}\subset\scriptF$, $\scriptL$ определяет распределение. В таком случае существование $\lim F_n=F\in \scriptF$ равносильно конъюнкции условий $\{F_n\}$ плотно и $\forall f\in\scriptL~\exists \lim\int f~\mathrm dF_n$
    \end{theorem}
    \begin{proof}
        Следствие направо тривиально. Докажем налево. Известно, что у $F_n$ есть сходящаяся в $\scriptG$ подпоследовательность $F_{n_k}$. Пусть она сходится к $F_1\in\scriptG$.\\
        Пусть $\varepsilon>0$. Для него существует такое $N$, что
        $$
        1\geqslant F_n(N)>F_n(N)-F_n(-N)>1-\varepsilon
        $$
        Пусть $x_0$~--- точка непрерывности $F_1$ и $x_0\geqslant N$. Тогда $1\geqslant F_n(x_0)>1-\varepsilon$. Если мы рассмотрим $x_1<-N$, то получим $F_n(x_1)<\varepsilon$. Отсюда $F_1$ на бесконечности равно 1, а на минус бесконечности~--- нулю. То есть $F_1\in\scriptF$.\\
         $\lim\int fdF_{n_k}=\int fdF_1=\int fdF_2\Rightarrow F_1=F_2$~--- любая подпоследовательность сходится к одному распределению, следовательно $\exists\lim F_n F$
    \end{proof}
    \begin{corollary}
        Пусть $\scriptL$ определяет распределение, $\forall f\in\scriptL~\int f~\mathrm dF_n\rightarrow\int f~\mathrm dF$, $F\in\scriptG$.\\
        Тогда из хотя бы одного из условий
        \begin{itemize}
            \item $\{F_n\}$ плотная.
            \item $F\in\scriptF$.
            \item $f\equiv1$.
        \end{itemize}
        Следует $F_n\rightarrow F\in\scriptF$.
    \end{corollary}
 \end{document}