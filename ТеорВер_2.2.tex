\documentclass{article}
\input{Headers/header}
\input{Headers/formal}

\fancyhead[L]{Теория вероятности}
\fancyfoot[C]{Иванов Тимофей, Чулков Алексей}

\let\eps\varepsilon

\newcommand{\A}{{\mathfrak A}}
\newcommand{\B}{{\mathfrak B}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{cov}

\begin{document}
    \tableofcontents
    \section{Основы теории вероятности.}
    \paragraph{Вероятностное пространство.}
    \begin{definition}
        Пусть $\Omega$~--- множество, тогда $\A\in 2^\Omega$ называется \textbf{алгеброй}, если
        \begin{enumerate}
            \item $\Omega\in A$.
            \item $\forall A\in\mathfrak A~\overline A\in\A$. Здесь и далее $\overline A=\Omega\setminus A$.
            \item $\forall A,B\in\A~A\cup B\in\A$.
        \end{enumerate}
        При этом $\Omega$ называется \textbf{множеством элементов событий}, $\A$~--- \textbf{набор событий}, $A\in\A$~--- \textbf{событие}, $A\cup B=A+B$~--- \textbf{сумма событий}, $\overline A$~--- \textbf{противоположное событие}, $A\cap B=AB$~--- \textbf{произведение событий}.
    \end{definition}
    \begin{definition}
        Алгебра является \textbf{сигма-алгеброй}, если она замкнута относительно объединения счётного количества своих элементов.
    \end{definition}
    \begin{definition}
        Пусть $\A$~--- сигма-алгебра на $\Omega$. Пусть $P\colon \A\to\mathbb [0;+\infty)$ и
        \begin{enumerate}
            \item $P(\Omega)=1$.
            \item Если $\{A_i\}_{i=1}^\infty\subset\A$ и $\forall_{i, j| i \ne j} A_iA_j=\varnothing$ то
            $$
            P\left(\bigcup\limits_{i=1}^\infty A_i\right)=\sum\limits_{i=1}^\infty P(A_i)
            $$
        \end{enumerate}
        Тогда $(\Omega;\A;P)$ называется \textbf{вероятностным пространством}.
    \end{definition}
    \begin{definition}
        Пара событий называется \textbf{несовместной}, если их пересечение пусто. Набор событий \textbf{несовместен}, если они попарно несовместны.
    \end{definition}
    \begin{definition}
        Пусть $A\subset2^\Omega$~--- алгебра. Тогда минимальная по включению сигма-алгебра $\sigma(A)\supset A$ называется \textbf{минимальной сигма-алгеброй}.
    \end{definition}
    \begin{claim}
        Таковая существует.
    \end{claim}
    \begin{proof}
        Хотя бы одна такая существует ($2^\Omega$), причём если пересечь сколько угодно сигма-алгебр, то получится искомая сигма-алгебра.
    \end{proof}
    \begin{definition}
        Пусть $\A$~--- алгебра на $\Omega$, $P\colon\A\to[0;+\infty)$ и
        \begin{itemize}
            \item $P(\Omega)=1$.
            \item Если $\{A_i\}_{i=1}^\infty\subset\A$ и $\bigsqcup\limits_{i=1}^\infty A_i\in\A$, то
            $$
            P\left(\bigsqcup\limits_{i=1}^\infty A_i\right)=\sum\limits_{i=1}^\infty P(A_i)
            $$
        \end{itemize}
        Тогда $(\Omega;\A;P)$~--- \textbf{вероятностное пространство в широком смысле}.
    \end{definition}
    \begin{theorem}[О продолжении меры]
        Пусть $(\Omega;\A;P)$~--- вероятностное пространство в широком смысле. Тогда существует единственная функция вероятности $Q\colon\sigma(\A)\to[0:+\infty)$, такое что $Q\Big|_{\A}\equiv P$.\\
        Без доказательства.
    \end{theorem}
    \begin{remark}
        Эта теорема позволяет нам сказать, например, что мы хотим задать вероятность на отрезках.
    \end{remark}
    \begin{definition}
        \textbf{Борелевская сигма-алгебра}~--- минимальная $\sigma$-алгебра, которая содержит все открытые множества.
    \end{definition}
    \begin{example}
        Дискретное вероятностное пространство: $\Omega=\{\omega_i\}_{i=1}^N$, $A=2^\Omega$, $P(\{\omega_i\})=p_i$, $\sum p_i=1$. Тогда $P(A)$~--- сумма вероятностей элементов $A$.
    \end{example}
    \begin{example}
        Геометрическая вероятность: $\Omega\subset\mathbb R^n$, измеримо по Лебегу, $\mu A<+\infty$, $\A$ состоит из измеримых по Лебегу множеств, $P(A)=\frac{\mu A}{\mu\Omega}$. Обычно при этом $\mathbb R^n$ не более чем трёхмерно.
    \end{example}
    \paragraph{Свойства вероятности.}
    \begin{property}
        $$
        \forall A,B\in\A~A\subset B\Rightarrow P(A)\leqslant P(B)
        $$
    \end{property}
    \begin{proof}
        Понятно, что $B\setminus A\in\A$. Тогда
        $$
        B=A\sqcup(B\setminus A)\Rightarrow P(B)=P(A)+P(B\setminus A)\geqslant P(A)
        $$
    \end{proof}
    \begin{corollary}
        $$\forall A\in\A~P(A)\leqslant1$$
    \end{corollary}
    \begin{property}
        $$
        P(A)=1-P(\overline A)
        $$
    \end{property}
    \begin{property}
        $$
        P(A+B)=P(A)+P(B)-P(AB)
        $$
    \end{property}
    \begin{proof}
        $$
        B=(B\setminus AB)\sqcup AB\Rightarrow P(B)=P(B\setminus AB)+P(AB)
        $$
        Тогда
        $$
        P(A+B)=P(A)+P(B\setminus AB)=P(A)+P(B)-P(AB)
        $$
    \end{proof}
    \begin{claim}[Формула включений-исключений]
        $$
        P(A_1+\cdots+A_n)=\sum\limits_{i=1}^nP(A_i)-\sum\limits_{\substack{i,j=1\\i<j}}^nP(A_iA_j)+\sum\limits_{\substack{i,j,k=1\\i<j<k}}^nP(A_iA_jA_k)-\cdots+(-1)^nP(A_1\cdots A_n)
        $$
    \end{claim}
    \begin{proof}
        Мне лень это писать, докажите сами по индукции.
    \end{proof}
    \begin{claim}
        $$
        P\left(\bigcup\limits_iA_i\right)\leqslant\sum\limits_i P(A_i)
        $$
    \end{claim}
    \begin{proof}
        Пусть $B_1=A_1$, $B_2=A_2\overline{A_1}$, $B_3=A_3\overline{A_1\cup A_2}$ и так далее. Тогда
        $$
        \bigcup\limits_iA_i=\bigsqcup\limits_iB_i
        $$
        При этом $B_i\subset A_i$, а значит
        $$
        \sum\limits_i P(A_i)\geqslant\sum\limits_i P(B_i)
        $$
    \end{proof}
    \begin{theorem}
        Пусть $(\Omega;\A;P)$~--- вероятностное пространство. Тогда следующие утверждения равносильны:
        \begin{enumerate}
            \item $P$ счётно-аддитивна.
            \item $P$ конечно-аддитивна и $\forall \{B_i\}_{i=1}^\infty:B_{i+1}\subset B_i$, $B=\bigcap\limits_{i=1}^\infty B_i$ $\lim\limits_{n\to\infty}P(B_i)=P(B)$ (непрерывность сверху).
            \item $P$ конечно-аддитивна и $\forall \{C_i\}_{i=1}^\infty:C_{i+1}\supset B_i$, $C=\bigcup\limits_{i=1}^\infty C_i$ $\lim\limits_{n\to\infty}P(C_i)=P(C)$ (непрерывность сверху).
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        Равносильность двух непрерывностей тривиально из формул де Моргана.\\
        Докажем, что из 1 следует 2. Конечная аддитивность есть, докажем непрерывность сверху. Пусть $A_1=B_1\overline{B_2}$, $A_2=B_2\overline{B_3}$ и так далее. Очевидно, $A_i$ несовместны. Также очевидно, что $A_i$ несовместны с $B$. Также заметим, что
        $$
        B_n=B\sqcup\bigsqcup\limits_{i=n+1}^\infty A_i
        $$
        Отсюда $P(B_n)=P(B)+\sum\limits_{i=n+1}^\infty P(A_i)$, а справа остаток (очевидно, сходящегося) ряда, который стремится к нулю при $n\to\infty$.\\
        Теперь из 2 докажем 1. Рассмотрим $\{A_i\}_{i=1}^\infty$ несовместные. Очевидно,
        $$
        \sum\limits_{i=1}^\infty P(A_i)=\lim\limits_{n\to\infty}\sum\limits_{i=1}^nP(A_i)
        $$
        А ещё мы знаем, что
        $$
        \bigsqcup\limits_{i=1}^\infty A_i=\bigsqcup\limits_{i=1}^nA_i\sqcup\bigsqcup\limits_{i=n+1}^\infty A_i
        $$
        То есть
        $$
        \lim\limits_{n\to\infty}P\left(\bigsqcup\limits_{i=1}^nA_i\right)=
        \lim\limits_{n\to\infty}\left(P\left(\bigsqcup\limits_{i=1}^\infty A_i\right)-P\left(\bigsqcup\limits_{i=n+1}^\infty A_i\right)\right)=
        P\left(\bigsqcup\limits_{i=1}^\infty A_i\right)-\lim\limits_{n\to\infty}P\left(\bigsqcup\limits_{i=n+1}^\infty A_i\right)
        $$
        Второе слагаемое~--- ноль но непрерывности меры, а отсюда счётная аддитивность.
    \end{proof}
    \paragraph{Условная вероятность.}
    \begin{remark}
        Пусть $|\Omega|=n$, $|A|=k$, $|B|=m$, $|AB|=l$. Если мы знаем, что $B$ произошло, как узнать вероятность того, что произошло $A$? Ну, это
        $$
        \frac lm=\frac{l/n}{m/n}=\frac{P(AB)}{P(B)}
        $$
    \end{remark}
    \begin{definition}
        Пусть $(\Omega;\A;P)$~--- вероятностное пространство, $B\in\A$, $P(B)>0$. Тогда \textbf{условной вероятностью} $A$ при условии $B$ называется
        $$
        P(A|B)=\frac{P(AB)}{P(B)}
        $$
        Также обозначается $P_B(A)$.
    \end{definition}
    \begin{property}
        Несложно проверить, что условная вероятность является вероятностью.
    \end{property}
    \begin{claim}[Произведение вероятностей]
        Несложно по определению проверить
        $$
        P(A_1\cdots A_n)=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)\cdots P(A_n|A_1A_2\cdots A_{n-1})
        $$
    \end{claim}
    \begin{theorem}[Формула полной вероятности]
        Пусть $A\in\A$, $B_i\in\A$ несовместны, $A\subset\bigsqcup\limits_{i=1}^nB_i$ (обычно объединение равно $\Omega$), и $\forall i\in[0:n]~P(B_i)>0$. Тогда
        $$
        P(A)=\sum\limits_{i=1}^nP(A|B_i)P(B_i)
        $$
    \end{theorem}
    \begin{proof}
        $$
        P(A)=P\left(A\cap\bigsqcup\limits_{i=1}^nB_i\right)=P\left(\bigsqcup\limits_{i=1}^nA\cap B_i\right)
        $$
        Всё.
    \end{proof}
    \begin{theorem}[Формула Байеса]
        Пусть $A,B\in\A$, $P(A)>0$, $P(B)>0$. Тогда
        $$
        P(A|B)=\frac{P(B|A)P(A)}{P(B)}
        $$
    \end{theorem}
    \begin{proof}
        Очевидно из определения.
    \end{proof}
    \begin{definition}
        События $A,B\in\A$ называются \textbf{независимыми}, если
        $$
        P(AB)=P(A)P(B)
        $$
    \end{definition}
    \begin{definition}
        Говорят, что $A_1,A_2,\ldots,A_n\in\A$ \textbf{независимы в совокупности}, если
        $P(A_1A_2\cdots A_n)=P(A_1)P(A_2)\cdots P(A_n)$
    \end{definition}
    \begin{property}
        Несложно проверить, что независимость событий $A,B$ равносильна $P(A|B)=P(A)$.
    \end{property}
    \begin{property}
        Независимые в совокупности события попарно независимы. Обратное неверно.
    \end{property}
    \begin{definition}
        Пусть у нас есть два вероятностных пространства: $(\Omega_1,\A_1;P_1)$ и $(\Omega_2,\A_2;P_2)$. Рассмотрим вот такое вероятностное пространство: $(\Omega,\A;P)$, где $\Omega=\Omega_1\times\Omega_2$, $\A$~--- минимальная $\sigma$-алгебра, включающая в себя $\A_1\times\A_2$,
        $$
        P((A_1;A_2))=P_1(A_1)P_2(A_2)
        $$
        Тогда $(\Omega_1,\A_1;P_1)$ и $(\Omega_2,\A_2;P_2)$~--- независимые испытания.
    \end{definition}
    \paragraph{Схема Бернулли.}
    \begin{example}
        Схема Бернулли: $\Omega_1=\{0;1\}$, $\A_1=2^{\Omega_1}$, $P_1(1)=p$, $P_1(0)=1-p=q$. Хочется рассмотреть эту штуку в степени $n$ (то есть $n$ одинаковых независимых испытаний). Тогда что у нас получается для $\omega\in\Omega=\Omega_1^n$?
        $$
        P(\omega)=\sum\limits_{i=1}^nP_i(\omega_i)=p^{\sum\omega_i}q^{n-\sum\omega_i}
        $$
        Посчитаем тут такую вероятность: пусть $S_n$~--- количество успехов в $n$ испытаниях? Посчитаем вероятность того, что $S_n=k$? Очевидно, оно равно $\Cnk nkp^kq^{n-k}$.
    \end{example}
    \begin{claim}
        Пусть $k^*$~--- наиболее вероятное число успехов в Бернуллиевских испытаниях. Тогда
        $$
        k^*=\begin{cases}
            p(n-1)\text{ или }p(n-1)+1 & p(n-1)\in\mathbb N\\
            \lceil p(n-1)-1\rceil & p(n-1)\notin\mathbb N
        \end{cases}
        $$
    \end{claim}
    \begin{proof}
        Давайте рассмотрим вот такое частное:
        $$
        \frac{P(S_n=k+1)}{P(S_n=k)}
        $$
        Чему оно равно?
        $$
        \frac{P(S_n=k+1)}{P(S_n=k)}=\frac{\Cnk n{k+1}p^{k+1}q^{n-k-1}}{\Cnk nkp^kq^{n-k}}=\frac pq\cdot\frac{n-k}{k+1}
        $$
        Нам хочется оценить, больше это чем 1 или меньше (это позволит нам найти $K^*$). Ну,
        $$
        \frac pq\cdot\frac{n-k}{k+1}>1\Leftrightarrow p(n-k)>q(k+1)\Leftrightarrow pn-pk>k=pk+1-p\Leftrightarrow pn>k+1-p\Leftrightarrow pn+p-1>k
        $$
        То есть возрастание достигается при $k<p(n-1)-1$, а иначе убывание. Тогда где экстремум? Рассмотрим $k=p(n-1)-1$. Если это целое число, то там $P(S_n=k+1)=P(S_n=k)$, и это самое $k$ даёт значение больше остальных. То есть $k^*=p(n-1)-1$ или $k^*=p(n-1)$.\\
        А что если оно не целое? То надо куда-то округлить. А именно вверх, потому что тогда оно больше, чем следующее, а предыдущее меньше его.
    \end{proof}
    \begin{example}
        Пусть $n=10000$, $p=\frac1{10000}$. Давайте посчитаем $P(S_n>3)$. Ну, это
        $$
        1-P(S_n\leqslant 3)=1-q^{10000}-10000pq^{10000-1}-\Cnk{10000}2p^2q^{10000-2}-\Cnk{10000}3p^3q^{10000-3}
        $$
        Фиг мы такое посчитаем.
    \end{example}
    \begin{example}
        Или если взять $p=q=0.5$, то при $n=5\cdot10^3$ мы не сможем нормально посчитать $P(S_n=2349)$.
    \end{example}
    \begin{remark}
        Ну и как такое считать?
    \end{remark}
    \begin{theorem}[Теорема Пуассона]
        Пусть у нас есть несколько схем Бернулли. В первой одно испытание и вероятность успеха $p_1$, во второй~--- 2 и вероятность успеха $p_2$, в $n$-ной $n$ испытаний и вероятность $p_n$. Пусть $np_n\underset{n\to\infty}\longrightarrow\lambda>0$. Тогда
        $$
        P(S_n=k)\underset{n\to\infty}\longrightarrow e^{-\lambda}\frac{\lambda^k}{k!}
        $$
    \end{theorem}
    \begin{proof}
        Известно,
        $$
        P(S_n=k)=\frac1{k!}n(n-1)\cdots(n-k+1)p_n^k(1-p_n)^{n-k}
        $$
        Известно, что
        $$
        np_n=\lambda+o(1)\Rightarrow p_n=\frac\lambda n+o\left(\frac1n\right)
        $$
        Тогда
        $$
        P(S_n=k)=\frac1{k!}\cancelto1{n(n-1)\cdots(n-k+1)\frac1{n^k}}\cancelto{\lambda^k}{(\lambda+o(1))^k}\frac{\cancelto{e^{-\lambda}}{\left(1-\frac\lambda n+o\left(\frac1n\right)\right)^n}}{\cancelto1{\left(1-\frac\lambda n+o\left(\frac1n\right)\right)^k}}\underset{n\to\infty}\longrightarrow e^{-\lambda}\frac{\lambda^k}{k!}
        $$
    \end{proof}
    \begin{lemma}
        Пусть $p\in(0;1)$, $H(x)=x\ln\frac xp+(1-x)\ln\frac{1-x}{1-p}$. Пусть $p^*=\frac kn$. Пусть $k\rightarrow+\infty$, $n-k\rightarrow+\infty$. Тогда
        $$
        P(S_n=k)\sim\frac1{\sqrt{2\pi np^*(1-p^*)}}\exp(-nH(p^*))
        $$
    \end{lemma}
    \begin{proof}
        Мы знаем формулу Стирлинга
        $$
        n!\sim\sqrt{2\pi n}n^ne^{-n}
        $$
        Тогда
        \[\begin{split}
            P(S_n=k)&=\frac{\sqrt{2\pi n}n^ne^{-n}}{\sqrt{2\pi\underbrace{k}_{np^*}}k^ke^{-k}\sqrt{2\pi\underbrace{(n-k)}_{n(1-p^*)}}(n-k)^{n-k}e^{-n+k}}p^k(1-p)^{n-k}=\\
            &=\frac{n^np^k(1-p)^{n-k}}{\sqrt{2\pi np^*(1-p^*)}k^k(n-k)^{n-k}}=\frac1{\sqrt{2\pi np^*(1-p^*)}}\exp\underbrace{\ln\frac{n^np^k(1-p)^{n-k}}{k^k(n-k)^{n-k}}}_L
        \end{split}\]
        При этом
        \[\begin{split}
            L&=\ln\frac{n^np^k(1-p)^{n-k}}{k^k(n-k)^{n-k}}=\ln\frac{n^np^k(1-p)^n(n-k)^k}{k^k(n-k)^n(1-p)^k}=\\
            &=\ln\left(\underbrace{\frac{n^n}{(n-k)^n}}_{(1-p^*)^{-n}}(1-p)^n\right)+\ln\frac{p^k(n-k)^k}{(np^*)^k(1-p)^k}=\\
            &=n\ln\frac{1-p}{1-p^*}+k\ln\frac{p}{p^*}+k\ln\frac{(n-k)}{n(1-p)}=n\ln\frac{1-p}{1-p^*}+k\ln\frac{p}{p^*}+k\ln\frac{1-p^*}{1-p}=\\
            &=-(n-k)\ln\frac{1-p^*}{1-p}-k\ln\frac{p^*}{p}=-n\underbrace{\left(p^*\ln\frac{p^*}{p}+(1-p^*)\ln\frac{1-p^*}{1-p}\right)}_{H(p^*)}
        \end{split}\]
        Это ли не то, что нам надо?
    \end{proof}
    \begin{lemma}
        $$
        H(x)=\frac{(x-p)^2}{2p(1-p)}+O((x-p)^3)
        $$
    \end{lemma}
    \begin{proof}
        $$
        H'(x)=\ln\frac xp+x\cdot\frac px\cdot\frac1p-\ln\frac{1-x}{1-p}-1=\ln\frac xp-\ln\frac{1-x}{1-p}
        $$
        $$
        H''(x)=\frac1x+\frac1{1-x}
        $$
        Тогда $H'(p)=0$, $H''(p)=\frac1{p(1-p)}$. По Тейлору получаем искомое.
    \end{proof}
    \begin{theorem}[Локальная теорема Муавра~--- Лапласа]
        Пусть $p\in(0;1)$, $H(x)=x\ln\frac xp+(1-x)\ln\frac{1-x}{1-p}$. Пусть $p^*=\frac kn$. Пусть $k\rightarrow+\infty$, $n-k\rightarrow+\infty$.\\
        Пусть $k-np=p(n^{2/3})$. Тогда
        $$
        P(S_n=k)\sim\frac1{\sqrt{2\pi np(1-p)}}\exp\left(-\frac{(k-np)^2}{2np(1-p)}\right)
        $$
    \end{theorem}
    \begin{proof}
        Известно
        $$
        P(S_n=k)\sim\frac1{\sqrt{2\pi np*((1-p*)}}\exp(-nH(p^*))
        $$
        Отсюда
        $$
        P(S_n=k)\sim\frac1{\sqrt{2\pi np*(1-p*)}}\exp(-n\frac{(p^*-p)^2}{2p(1-p)}+n\cdot O((p^*-p)^3))
        $$
        Заметим, что $\frac kn-p=o(n^{-1/3})$, поэтому $p* \approx p$. Тогда
        \begin{align*}
            P(S_n=k)&\sim\frac1{\sqrt{2\pi np(1-p)}}\exp\left(-n\frac{(p-k/n)^2}{2p(1-p)}+O(n(k/n-p)^3)\right)\\
             &\sim\frac1{\sqrt{2\pi np(1-p)}}\exp\left(-n\frac{(np-k)^2}{2p(1-p)n^2}+o(1)\right)
        \end{align*}
        Что и требовалось доказать.
    \end{proof}
    \begin{theorem}[Интегральная теорема Муавра~--- Лапласа]
        Пусть
        $$
        \Phi(x)=\frac1{\sqrt{2\pi}}\int\limits_{-\infty}^xe^{-\frac{t^2}2}~\mathrm dt
        $$
        Далее мы будем называть эту функцию функцией стандартного нормального распределения. Тогда
        $$
        \sup\limits_{-\infty<x_1<x_2<+\infty}\left|P\left(x_1\leqslant\frac{S_n-np}{\sqrt{npq}}\leqslant x_2\right)(\Phi(x_2)-\Phi(x_1))\right|\underset{n\to\infty}\longrightarrow0
        $$
        Иными словами
        $$
        P\left(x_1\leqslant\frac{S_n-np}{\sqrt{npq}}\leqslant x_2\right)\approx\frac1{\sqrt{2\pi}}\int\limits_{x_1}^{x_2}e^{-\frac{t^2}2}~\mathrm dt
        $$
        Пока без доказательства.
    \end{theorem}
    \begin{remark}
        Оценка теоремы Пуассона.\\
        Обычно в задачах $np_n$ не стремится, а просто равно $\lambda>0$. Тогда
        $$
        \sum\limits_{k=0}^\infty \left|P(S_n=k)-e^{-\lambda}\frac{\lambda^k}{k!}\leqslant\frac\lambda n\right|\leqslant\frac{2\lambda}n\min\{2;\lambda\}
        $$
        Оценка локальной теоремы Лапласа. Если $|p^*-p|\leqslant \frac12 mn\min\{p;q\}$, то
        $$
        P(S_n=k)=\frac1{\sqrt{2\pi np(1-p)}}\exp\left(-\frac{(k-np)^2}{2np(1-p)}\right)(1+\eps(k;n))
        $$
        Где
        $$
        \eps(k;n)=\exp\left(\theta\frac{|k-np|^3}{3n^2p^2q^2}+\frac1{npq}\left(\frac16+|k-np|\right)\right)\qquad |\theta|<1
        $$
        Оценка интегральной теоремы Лапласа.
        $$
        \sum\limits_x\left|P\left(\frac{S_n-np}{\sqrt{npq}}\leqslant x\right)-\Phi(x)\right|\leqslant\frac{p^2+q^2}{\sqrt{npq}}
        $$
    \end{remark}
    \begin{example}
        Пусть у нас есть два узла связи на 2000 пользователей в каждом. И у нас есть канал связи, который пропускает $N$. Хочется минимизировать $N$, но так, чтобы вероятность перегрузки была меньше $\frac1{100}$. Будем предполагать, что люди пользуются данным каналом связи в течение двух минут из одного часа, то есть каждый пользователь может пользоваться каналом в данный момент с вероятностью $p=\frac1{30}$.\\
        Ну так и что мы хотим по сути? Мы хотим $P(S_{2000}>N)<\frac1{100}$, что равносильно $P(S_{2000}\leqslant N)\geqslant\frac{99}{100}$.\\
        Используем Пуассона: $np\approx 6.67$.
        $$
        \sum\limits_{k=0}^Ne^{-\lambda}\frac{\lambda^k}{k!}
        $$
        Это мы хрен посчитаем, но, короче, получится $N=87$.\\
        А если применить интегральную теорему Муавра~--- Лапласа, то получим мы
        $$
        N=\left\lceil q_{\frac{99}{100}}\sqrt{npq}+np\right\rceil=86
        $$
        Где $q_{\frac{99}{100}}$~--- такое число, что $\Phi(q_{\frac{99}{100}})=\frac{99}{100}$.
    \end{example}
    \begin{definition}
        Если $\alpha\in(0;1)$ и $\Phi(q_\alpha)=\alpha$, то $q_\alpha$ называется \textbf{квантилем порядка $\alpha$}.
    \end{definition}
    \section{Случайные величины.}
    \subsection{Одномерные случайные величины.}
    \paragraph{Распределение случайных величин, функция распределения случайных величин.}
    \begin{definition}
        \textbf{Борелевская сигма-алгебра}~--- это минимальная сигма-алгебра, содержащая все открытые множества.
    \end{definition}
    \begin{definition}
        Пусть $\Omega;\A$~--- множество с сигма-алгеброй. Тогда такое $\xi\colon\Omega\to\mathbb R$, что $\forall B\in\B~\xi^{-1}(B)\in\A$, называется \textbf{случайной величиной}.
    \end{definition}
    \begin{definition}
        Пусть $(\Omega;\A;P)$~--- вероятностное пространство, $\xi$~--- случайная величина. Тогда распределение $\xi$~--- функция
        $$
        P_\xi\colon\substack{\B\to\mathbb R\\B\mapsto P(\{\omega\mid \xi(\omega)\in B\})}
        $$
    \end{definition}
    \begin{remark}
        $P(\{\omega\mid \xi(\omega)\in B\})$ обозначается $P(\xi\in B)$.
    \end{remark}
    \begin{property}
        $P_\xi$~--- вероятность на $(\mathbb R;\B)$.
    \end{property}
    \begin{definition}
        Пусть $\xi$~--- случайная величина. Тогда
        $$
        F_\xi(t)=P(\xi\leqslant t)
        $$
        называется \textbf{функцией распределения} $\xi$
    \end{definition}
    \begin{property}
        Очевидно, функция распределения нестрого возрастает.
    \end{property}
    \begin{property}
        Не менее очевидно, $F_\xi(+\infty)=1$, $F_\xi(-\infty)=0$;
    \end{property}
    \begin{property}
        Функция распределения непрерывна справа.
    \end{property}
    \begin{proof}
        Возьмём $F(t+\eps_n)-F(t)$. Она равна $P(t<\xi\leqslant t+\eps_n)$. При $\eps_n\to0$, получим, что аргумент $P$ стремится к $\varnothing$, а значит $P(t<\xi\leqslant t+\eps_n)$ стремится к нулю.
    \end{proof}
    \begin{lemma}
        Пусть $P\colon\B\to\mathbb R$~--- некоторая функция. Тогда
        \[\begin{split}
            &\forall\{B_n\}_{n=1}^\infty\subset\B:B_{n+1}\subset B_n~P(B_n)\underset{n\to\infty}\rightarrow P\left(\bigcap\limits_{n=1}^\infty B_n\right)\Leftrightarrow\\
            \Leftrightarrow&\forall\{C_n\}_{n=1}^\infty\subset\B:C_{n+1}\subset C_n,\bigcap\limits_{n=1}^\infty C_n=\varnothing~P(C_n)\underset{n\to\infty}\rightarrow0
        \end{split}\]
    \end{lemma}
    \begin{proof}
        Следствие слева направо очевидно. Наоборот. Пусть
        $$
        \bigcap\limits_{n=1}^\infty B_n=B
        $$
        Возьмём $C_n=B_n\overline{B}$. Тогда, очевидно, $C_n$ подходят под условие справа, а значит $P(C_n)\longrightarrow0$.\\
        Также несложно заметить, что $P(B_n)=P(C_n)+P(B)$, а отсюда получим $P(B_n)\longrightarrow P(B)$.
    \end{proof}
    \begin{theorem}
        Пусть $F$~--- монотонно возрастающая непрерывная слева функция, равная нулю в $-\infty$ и единице в $+\infty$. Тогда существует вероятностное пространство и случайная величина в нём, что $F$~--- её функция распределения.
    \end{theorem}
    \begin{proof}
        Пусть $\Omega=\mathbb R$, $\A$~--- алгебра, состоящая из множеств вида $\bigsqcup\limits_{k=1}^n(a_k;b_k]$ или $(-\infty;b)$ или $(a;\infty)$ или $\mathbb R$.\\
        $$
        P\left(\bigsqcup\limits_{k=1}^n(a_k;b_k]\right)=\sum\limits_{k=1}^nF(b_k)-F(a_k)
        $$
        $$
        P((-\infty;b))=F(b)\qquad P((a;+\infty))=1-F(a)\qquad P(\mathbb R)=1
        $$
        Получим вероятностное пространство в широком смысле (разве что непрерывность сверху надо проверить). Ну, проверим её, используя лемму. Пусть, не умаляя общности, $A_n=(a_{n,1};a_{n,2}]$, $A_{n+1}\subset A_n$ и пересечение всех пусто.\\
        Из непрерывности $F$ справа следует, что существует $B_n=(b_{n,1};b_{n,2}]$, где $\Cl B_n\subset A_n$ и $P(A_n)-P(B_n)\leqslant\eps 2^{-n}$. Тогда пересечение всех $B_n$ также пусто.\\
        Предположим, что существует $M$, такое что $\forall n~A_n\in[-M;M]$. $[-M;M]$~--- компакт, следовательно. Заметим, что
        $$
        [-M;M]=\bigcup\limits_{k=1}^\infty [-M;M]\setminus\Cl B_n
        $$
        Справа~--- открытое покрытие компакта, значит из него можно вытащить конечное подпокрытие, то есть пересечение какого-то конечного числа $B_n$ пусто. Пусть это пересечение от $1$ до $n_0$. Тогда
        $$
        P(A_{n_0})=P(A_{n_0}\setminus\bigcap\limits_{k=1}^{n_0}B_k)+P\left(\bigcap\limits_{k=1}^{n_0}B_k\right)
        $$
        Отсюда $P\left(\bigcap\limits_{k=1}^{n_0}B_k\right)=0$. А
        $$
        P(A_{n_0}\setminus\bigcap\limits_{k=1}^{n_0}B_k)=P(\bigcap\limits_{k=1}^{n_0}A_{n_0}\setminus B_k)\leqslant P(\bigcap\limits_{k=1}^{n_0}A_k\setminus B_k)\leqslant\sum\limits_{k=1}P(A_n)-P(B_k)\leqslant\eps\sum\limits{k=1}^{n_0} 2^{-n}<\eps
        $$
        Если же мы не находимся в промежутке $[-M;M]$, то можно указать такие $M_1$ и $M_2$, что $P((-\infty;M_1)\cup(M_2;+\infty))<\frac\eps2$. Тогда
        $$
        P(A_n)=P(A_n[M_1;M_2])+P(A_n\overline{[M_1;M_2]})
        $$
        Левую часть суммы мы разобрали, а правая мала т.к. $M_2$, что $P((-\infty;M_1)\cup(M_2;+\infty))<\frac\eps2$.\\
        Осталось предъявить случайную величину $\xi(\omega)=\omega$.
    \end{proof}
    \paragraph{Типы распределений.}
    \subparagraph{Дискретные случайные величины и распределения.}
    \begin{definition}
        Случайная величина $\xi$ называется \textbf{дискретной}, если существует такое не более чем счётное множество $E$, что $P_\xi(E)=1$.
    \end{definition}
    \begin{example}
        Вырожденное: $P(\xi=c)=1$. Обозначают $I(c)$ или $I_c$.
    \end{example}
    \begin{example}
        Распределение Бернулли: $P(\xi=0)=p$, $P(\xi=1)=q=1-p$. Обозначение: $\operatorname{Bern}(p)$.
    \end{example}
    \begin{example}
        Биномиальное распределение: $P(\xi=k)=\Cnk nk p^kq^{n-k}$. Обозначение: $\operatorname{Bin}(n;p)$,
    \end{example}
    \begin{example}
        Отрицательное биномиальное распределение: $\xi=(\min n:S_n=r)-r$, где $r\in\mathbb n$. То есть
        $$
        P(\xi=k)=\Cnk{k+r-1}{r-1}p^{r}q^{k}
        $$
        Обозначение: $\operatorname{NB}(r;p)$. Также это обобщается на произвольное $r$ при помощи гамма-функции.\\
        В случае $r=1$ распределение называется геометрическим. Геометрическое распределение~--- количество неудач до первого успеха.
    \end{example}
    \begin{example}
        Распределение Пуассона:
        $$
        P(\xi=k)=e^{-\lambda}\frac{\lambda^k}{k!}\qquad k\in\mathbb Z_+
        $$
        Обозначение: $\operatorname{Pois}(\lambda)$.
    \end{example}
    \begin{definition}
        \textbf{Носителем случайной величины} $\xi$ называется минимально по включению замкнутое множество $E$, удовлетворяющее условию $P(\xi\in E)=1$.
    \end{definition}
    \subparagraph{Абсолютно непрерывные случайные величины и распределения.}
    \begin{definition}
        Величина $\xi$ (или её случайно распределение) называется \textbf{абсолютно непрерывной},  если существует $p\in L(\mathbb R\to\mathbb R)$ с неотрицательными значениями такая что $P(\xi\in B)=\int\limits_Bp(x)~\mathrm dx$. В таком случае $p$ называется \textbf{плотностью} $\xi$.
    \end{definition}
    \begin{property}
        В таком случае
        $$F(t)=\int\limits_{-\infty}^tp(x)~\mathrm dx$$
        То есть плотность почти всюду равна производной функции распределения.
    \end{property}
    \begin{property}
        $P(\xi=c)=0$.
    \end{property}
    \begin{property}
        Функция распределения абсолютно непрерывной случайной величины непрерывна на $\mathbb R$.
    \end{property}
    \begin{property}
        $$P(x_0\leqslant\xi\leqslant x_0+h)=P(x_0<\xi<x_0+h)=F(x_0+h)-F(x_0)=p(x_0)h+o(h)$$
    \end{property}
    \begin{property}
        Пусть $E=\supp p$. Тогда $E$ является носителем по нашему прошлому определению.
    \end{property}
    \begin{example}
        $$
        p(x)=\frac1{b-a}\chi_{[a;b]}
        $$
        Тогда
        $$
        F(x)=\begin{cases}
            0 & x<a\\
            \frac{x-a}{b-a} & x\in[a;b)\\
            1 & x\geqslant b
        \end{cases}
        $$
        Обозначение: $U[a;b]$.
    \end{example}
    \begin{claim}
        Пусть $\xi=U[a;b]$, $c>0$. Тогда $\eta=c\xi+d=U[ac+d;bc+d]$
    \end{claim}
    \begin{proof}
        $$
        P(\eta\leqslant t)=P(c\xi+d\leqslant t)=P\left(\xi\leqslant\frac{t-d}c\right)
        $$
        Несложно проверить, что это именно $U[ac+d;bc+d]$.
    \end{proof}
    \begin{example}
        Нормальное (гауссовское) распределение: $\operatorname{N}(\mu;\sigma^2)$:
        $$
        p(x)=\frac1{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
        $$
        \begin{figure}[H]
            \begin{tikzpicture}
                \begin{axis}[
                    width = 15cm, height = 5cm,
                    trig format plots = rad,
                    grid = both,
                    xmin = -2,
                    xmax = 2,
                    ymin = 0,
                    ymax = 1,
                    axis equal,
                    axis x line = middle,
                    axis y line = middle,
                    axis line style = {->, color=black},
                    xtick distance = 1,
                    ytick distance = 1,
                    minor tick num = 3,
                    xlabel={$x$},
                    ylabel={$p(x)$},
                    ]
                    \addplot[domain=-2:2,samples=100]{1/sqrt(pi/2)*e^(-x^2*2)};
                \end{axis}
            \end{tikzpicture}
        \end{figure}\noindent
        $\operatorname{N}(0;1)$~--- стандартное нормальное распределение. Ещё оно обозначается $\Phi(x)$.
    \end{example}
    \begin{claim}
        Пусть $\xi=\operatorname{N}(\mu;\sigma^2)$, $\eta=a\xi+b$. Тогда
        $\eta=\operatorname{N}(a\mu_b;a^2\sigma^2)$.
    \end{claim}
    \begin{proof}
        Пусть $a>0$. Обозначим $y=ax+b$. Тогда
        \[\begin{split}
            P(\eta\leqslant t)&=P(a\xi+b\leqslant t)=P(\xi\leqslant\frac{t-b}a)=F_\xi(\frac{t-b}a)=\frac1{2\pi\sigma^2}\int\limits_{-\infty}^{\frac{t-b}a}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)~\mathrm dx=\\
            &=\frac1{\sqrt{2\pi\sigma^2a^2}}\int\limits_{-\infty}^t\exp\left(-\frac{(\frac{y-b}a-\mu)^2}{2\sigma^2}\right)=\frac1{\sqrt{2\pi\sigma^2a^2}}\int\limits_{-\infty}^t\exp\left(-\frac{(y-b-a\mu)^2}{2a^2\sigma^2}\right)
        \end{split}\]
        Что и требовалось доказать. При $a<0$ аналогично.
    \end{proof}
    \begin{corollary}
        Если $\xi=\operatorname{N}(0;1)$, то $\sigma\xi+\mu=\operatorname{N}(\mu;\sigma^2)$.
    \end{corollary}
    \begin{example}
        Распределение Коши: $\operatorname{Cauchy}(x_0;\gamma)$.
        $$
        p(x)=\frac1{\pi\gamma}\cdot\frac1{1+\left(\frac{x-x_0}\gamma\right)^2}
        $$
        Тогда
        $$
        F(t)=\frac1{\pi\gamma}\int\limits_{-\infty}^t\frac{\mathrm dx}{1+\left(\frac{x-x_0}\gamma\right)^2}=\frac1\pi\atan\left(\frac{t-x_0}\gamma\right)+\frac12
        $$
    \end{example}
    \begin{example}
        Экспоненциальное распределение: $\operatorname{Exp}(\lambda)$.
        $$
        p(x)=\lambda e^{-\lambda x}\chi_{\mathbb R_+}
        $$
        Тогда
        $$
        F(t)=1-e^{-\lambda t}\chi_{\mathbb R_+}
        $$
    \end{example}
    \begin{example}
        $\Gamma$-распределение: $\Gamma(k,\lambda)$. Сначала для $k\in\mathbb N$, $\lambda>0$.
        $$
        p(x)=\frac{\lambda^kx^{k-1}}{(k-1)!}e^{-\lambda x}\qquad x\geqslant 0
        $$
        Для $k=\alpha>0$ заменим $(k-1)!$ на $\Gamma(\alpha)$.
    \end{example}
    \begin{claim}
        Пусть $\xi$~--- абсолютно непрерывная случайная величина с плотностью $p_\xi$. Пусть $g\in C^{(1)}(\mathbb R\to\mathbb R)$~--- строго монотонна. Пусть $\eta=g(\xi)$. Тогда
        $$
        p_\eta(y)=p_\xi(g^{-1}(y))\left|\frac{\mathrm d}{\mathrm dy}g^{-1}(y)\right|=p_\xi(g^{-1}(y))\left|\frac1{g'(g^{-1}(y))}\right|
        $$
    \end{claim}
    \begin{proof}
        Во-первых, у условиях теоремы $g^{-1}$ существует. Также второе равенство следует из первого в силу теоремы об обратном отображении.\\
        Не умаляя общности, $g$ строго возрастает. Тогда
        $$
        F_\eta(y)=P(g(\xi)\leqslant y)\overset{g\uparrow}=P(\xi\leqslant g^{-1}(y))=\int\limits_{-\infty}^{g^{-1}(y)}p_\xi(x)~\mathrm dx
        $$
        Продифференцировав это равенство по $y$ получим искомое равенство.
    \end{proof}
    \subparagraph{Сингулярные случайные величины и распределения.}
    \begin{definition}
        $x$ называется \textbf{точкой роста} монотонно возрастающей функции $f$, если $\forall\eps>0~f(x+\eps)-f(x-\eps)>0$.
    \end{definition}
    \begin{definition}
        Случайная величина $\xi$ (и её распределение) называются \textbf{сингулярными}, если $F_\xi\in C(\mathbb R)$ и мера множества точек $F_\xi$ роста равна нулю.
    \end{definition}
    \begin{example}
        Функция Кантора~--- функция, которая выглядит так: в нуле она равна нулю, в единице~--- единице, а во всех остальных точках строится так: отрезок $[0;1]$ делится на три части и в средней части равна среднему значению краёв (т.е. $\frac12$). И так далее.\\
        Она является функцией распределения сингулярной случайной величины.
    \end{example}
    \begin{theorem}[Теорема Лебега]
        Пусть $F$~--- функция распределения. Тогда существуют единственные $a, b, c \geq 0$, $a+b+c=1$, $F_{\mathrm{disc}}$, $F_{\mathrm{ac}}$ и $F_{\mathrm{sing}}:$ $F=aF_{\mathrm{disc}} +bF_{\mathrm{ac}}+cF_{\mathrm{sing}}$.\\
        Без доказательства.
    \end{theorem}
    \subsection{Многомерные случайные величины.}
    \paragraph{Распределение многомерных случайных величин.}
    \begin{definition}
        Вектор $\xi$ называется \textbf{случайным вектором} или \textbf{многомерной случайной величиной}, если $\xi_i$~--- случайная величина.
    \end{definition}
    \begin{definition}
        \textbf{Распределением случайного вектора} называется функция $P_\xi$, определённая на $\B^n$, заданная так:
        $$
        P_\xi(B_1;\ldots;B_n)=P(\{(\omega_1;\ldots;\omega_n)\mid\xi_1(\omega_1)\in B_1\land\cdots\land \xi_n(\omega_n)\in B_n\})
        $$
        Последнее обычно обозначается так: $P(\xi_1\in B_1,\xi_2\in B_2;\ldots;\xi_n\in B_n)$.
    \end{definition}
    \begin{definition}
        \textbf{Функцией распределения случайного вектора} $\xi$ называется функция
        $$
        F_\xi(t_1;\ldots;t_n)=P_\xi(\forall i\in[1:n]~\xi_i\leqslant t_i)
        $$
    \end{definition}
    \begin{claim}
        \[\begin{split}
            P(\forall i\in[1:n]~&a_i<\xi_i\leqslant b_i)=\\
            &F(b_1;b_2;\ldots;b_n)\\
            &-F(a_1;b_2;\ldots;b_n)-F(b_1;a_2;\ldots;b_n)-\cdots-F(b_1;b_2;\ldots;a_n)\\
            &+\vdots\\
            &\pm F(a_1;a_2;\ldots;a_n)
        \end{split}\]
        То есть в этой сумме участвует $F$ от $a_i$ и $b_i$ в произвольном сочетании, при этом минус стоит там, где нечётное количество $a_i$.
    \end{claim}
    \begin{proof}
        $$
            P(\forall i\in[1:n]~a_i<\xi_i\leqslant b_i)=P(\xi_1\leqslant b_1,\forall i\in[2:n]~a_i<\xi_i\leqslant b_i)-P(a_1\geqslant\xi_1,\forall i\in[2:n]~a_i<\xi_i\leqslant b_i)
        $$
        Проведя такую операцию несколько раз, получим искомое.
    \end{proof}
    \begin{remark}
        Если ввести обозначение $\Delta_{a_i;b_i}F=F(\cdot;\ldots;b_i;\cdot;\ldots;\cdots)-F(\cdot;\ldots;a_i;\cdot;\ldots;\cdots)$, то арифметическая сумма из утверждения выше записывается как
        $$
        \Delta_{a_1;b_1}\Delta_{a_2;b_2}\cdots\Delta_{a_n;b_n}F
        $$
    \end{remark}
    \begin{property}
        $$F(+\infty;\cdots;+\infty)=1$$
        $$F(-\infty;\cdots;-\infty)=0$$
        $F$ непрерывна справа.
    \end{property}
    \begin{theorem}
        Если функция распределения удовлетворяет трём свойствам выше, то она является функцией распределения некоторого случайного вектора.
    \end{theorem}
    \begin{proof}
        Аналогично одномерному случаю.
    \end{proof}
    \begin{definition}
        Случайный вектор (и его распределение) называется \textbf{дискретным}, если существует не более чем счётное множество $E$ такое что $P(\xi\in E)=1$.
    \end{definition}
    \begin{example}
        Полиномиальное распределение. Пусть $p=(p_1;\ldots;p_m)$, где $\sum p_i=1$. Обозначается $\operatorname{Poly}(n;p)$.\\
        Физическая интерпретация такая: мы бросаем кубик с $m$ гранями $n$ раз. Пусть $S_{n,j}$~--- количество исходов типа $j$ в $n$ независимых испытаниях. Тогда искомая случайная величина обладает распределением
        $$
        P(S_{n,1}\in B_1;S_{n,2}\in B_2;\ldots;S_{n,m}\in B_m)
        $$
        Рассмотрим $P(S_{n,1}=k_1;S_{n,2}=k_2;\ldots;S_{n,m}=k_m)$, где $\sum k_m=n$. Чему равна такая вероятность? Ну,
        $$
        \frac{n!}{k_1!k_2!\cdots k_m!}p_1^{k_1}p_2^{k_2}\cdots p_m^{k_m}
        $$
    \end{example}
    \begin{remark}
        Несложно заметить, что штука справа~--- слагаемые в сумме $(p_1+p_2+\cdots+p_m)^n$.
    \end{remark}
    \begin{definition}
        Случайный вектор $\xi$ (и его распределение) называется \textbf{абсолютно непрерывным}, если существует $p\in L(\mathbb R^n\to\mathbb R)$ с неотрицательными значениями такая что $P(\xi\in B)=\int\limits_BP~\mathrm d\mu_n$ (тут интеграл $n$-кратный). В таком случае $p$ называется \textbf{плотностью} $\xi$.
    \end{definition}
    \begin{example}
        Случайный вектор $\xi$ имеет стандартное многомерное нормальное распределение $\operatorname{N}(\mathbb0_n;E_n)$, если его плотность равна
        $$
        p(x_1;\ldots;x_n)=\frac1{(\sqrt{2\pi})^n}\exp\left(-\frac{\sum\limits_{i=1}^nx_i^2}2\right)
        $$
    \end{example}
    \begin{property}
        Несложно заметить, что это произведение плотностей одномерных стандартных нормальных распределений.
    \end{property}
    \begin{example}
        Случайный вектор $\eta$ имеет многомерное нормальное распределение $\operatorname{N}(\mu;\Sigma)$ (где $\mu\in\mathbb R^n$, $\Sigma$~--- симметричная матрица $n\times n$ с неотрицательными собственными числами), если он равен $\sqrt\Sigma\xi+\mu$, где $\xi$~--- стандартный многомерный гауссовский вектор.
    \end{example}
    \begin{remark}
        В случае $\Sigma>0$ можно написать плотность:
        $$
        p(y)=\frac1{\sqrt{2\pi}^n\sqrt{\det\Sigma}}\exp\left(-\frac12(y-\mu)^T\Sigma^{-1}(y-\mu)\right)
        $$
    \end{remark}

    \subsection{Независимые случайные величины}
    \begin{example}
    Пусть случайная величина задана таблицей\\
    \begin{tabular}{|c|c|c|c}
    \hline
    $\xi \text{\textbackslash} \eta$ & $y_1$ & $y_2$ & \dots \\
    \hline
    $x_1$ & $p_{11}$ & $p_{12}$ & \dots \\
    \hline
    $x_2$ & $p_{21}$ & $p_{22}$ & \dots \\
    \hline
    \vdots & \vdots & \vdots &  \\
    \end{tabular}

    $$ p_{ij} = P(\xi = x_i, \eta = y_j): \sum\limits_{i, j} p_{ij} = 1 $$
    Теперь хотим найти вероятность:
    $$ P(\xi = x_i) = \sum\limits_j P(\xi = x_i, \eta=y_i) = \sum\limits_j = p_{ij}$$
    Аналогично:
    $$ P(\eta = y_j) = \sum\limits_i P(\xi = x_i, \eta=y_j) = \sum\limits_i = p_{ij}$$
    Теперь рассмотрим $x_i = y_i = i, i \in \mathbb{N}$ \\
    Хотим найти $P(\xi + \eta = k)$: \\
    Во-первых $ k \ge 2$
    И теперь мы просто рассматриваем диагонали в табличке и получаем: 
    $$ P(\xi + \eta = k) = \sum\limits_{i=1}^{k-1} p_{i, k-i}$$

    Пусть теперь $p(x, y)$ -- совместная плотность на $(X, Y)$
     $$p_X(x) = \int\limits_{D_x} p(x, y) dy$$
     $$p_Y(y) = \int\limits_{D_y} p(x, y) dx$$
     Тогда:
    $$F_{X+Y}(t) = P(X+Y \le t) = \iint_{x+y \le t} p(x, y) dx dy$$
    \end{example}

    Раньше мы определяли независимость вот так:
    $$ P(AB) = P(A) \cdot P(B) $$
    \begin{definition}
        $ X_1, \dots , X_n $ -- независимы $ \Leftrightarrow \forall B_1, \dots, B_n \in \B $
        $$P(X_1 \in B_1, \dots X_n \in B_n) = P(X_1 \in B_1) \dots P(X_n \in B_n)$$
         $\{X_n\}_{n=1}^{\infty} $ -- независимы $ \forall m \in \mathbb{N} X_{1, \dots, m} $ -- независимы
    \end{definition}

    \begin{theorem}{Критерий независимости}
    $$ X_1, \dots, X_n \text{-- независимы} \Leftrightarrow F_{x_1, \dots, x_n} (t_1, \dots, t_n) = \prod_{i=1}^n F_{x_i} (t_i), \forall t_1, \dots, t_n \in \mathbb{R}$$
    \end{theorem}
    \begin{theorem}{Критерий независимости для дискретных с.в.}
    $$ X_1, \dots, X_n \text{-- дискретные нез с.в.} \Leftrightarrow P(X_1=x_{i_1}, \dots, X_n = x_{i_n}) = \prod_{i=1}^n P(X_i=x_{i_1}), \forall x_{i_j} \in \text{значения} X_i$$
    \end{theorem}
    \begin{proof}
    $(\Rightarrow)$ очевидна из определения \\
    $(\Leftarrow)$ 
    \begin{align*}
    P(X_1 \in B_1, \dots, X_n \in B_n) = P(X_1 \in \{x_{11}, \dots, x_{1, k_1}\},  X_n \in \{x_{n1}, \dots, x_{n, k_n}\} = \\
    \sum\limits_{j_1 \dots j_n} P(X_1 = x_{1j_1}, \dots , X_n = x_{nj_n}) = \dots = P(X \in B_1) \cdot \dots \cdot P(X_n \in B_n)
    \end{align*}
    \end{proof}
    
    \begin{theorem}{Критерий независимости для абсолютно непрерывных с.в.}
    $$ X_1, \dots, X_n \text{-- нез с.в.} \Leftrightarrow p(x_1 \dots, x_n) = \prod_{i=1}^n p_i(x_i), p_i \text{плотность} X_i$$
    \end{theorem}
    \begin{proof}
    $(\Rightarrow)$ очевидна из определения \\
    $(\Leftarrow)$ 
    \begin{align*}
    = F(t_1\dots t_n) = \int\limits_{-\inf}^{t_1} \dots \int\limits_{-\inf}^{t_n} p(x_1 \dots x_n) dx_1 \dots dx_n = \prod_{i=1}^n F_i(t_i) = \prod_{i=1}^n \int\limits_{-\inf}^{t_i} p_i(x_i) dx_i 
    \end{align*}
    То есть мы получили, что совместная функция распределения равна  произведению одномерных. В общем-то ровно то, что и хотели.
    \end{proof}
    \begin{example}
        $X_1, X_2 \sim Bern(p)$ независимы
        $$ X_1 + X_2 \sim Bin(2, p)$$
        \begin{proof}
            \begin{align*}  
            P(X_1 + X_2 = k) &= P(X_1 + X_2 =k | X_2=0) + P(X_1 + X_2 = k | X_2 = 1) \\ &= P(X_1 = k | X_2=0) +  P(X_1 = k - 1 | X_2 = 1)\\
                &=
                \begin{cases}
                    (1 - p) ^ 2 & k = 0 \\
                    2p(1-p) & k = 1 \\
                    p^2 & k = 2 \\
                \end{cases}
            \end{align*}
            
        Тогда, если $X_1 \dots X_n$ - $Bern$ независимы и одинаковы распределены (i.i.d)
        $$S_n = X_1 + \dots + X_n  \sim Bin(n, p)$$
        Доказывается по индукции:
        Пусть $X_1 \dots X_{n-1} \sim Bin(n - 1, p) $
        $$ P(S_n = n) = P(S_{n-1} + X_n = n) = P(S_{n - 1} = n | X_n = 0) + P(S_{n-1} = n - 1 | X_n = 1) = p^n$$
        $$ P(S_n = 0) = P(S_{n-1} + X_n = 0) = P(S_{n - 1} = 0 | X_n = 0)= (1-p)^n$$
        \begin{align*}
        P(S_n = k) &= P(S_{n-1} + X_n = k) = P(S_{n - 1} = k | X_n = 0) + P(S_{n-1} = k - 1 | X_n = 1) \\ &= \binom{n-1}{k-1} p^{k}(1-p)^{n-k} + \binom{n-1}{k} p^k (1-p)^{n-k} = \binom{n}{k} p^k (1-p)^{n-k}
        \end{align*}
        \end{proof}
    \end{example}
    \begin{example}
        $X_1 \sim Pois(\lambda_1), X_2 \sim Pois(\lambda_2)$ -- независимы
        \begin{align*}
        P(X_1 + X_2 = k) &= \sum\limits_{i=0}^k P(X_1=k-i) \cdot P(X_2=i)\\ 
        &= \sum\limits_{i=0}^k e^{-\lambda_1} \cdot \frac{\lambda_1^{k-i}}{(k-i)!} 
        \cdot e^{-\lambda_2} \frac{{-\lambda_2^i}}{i!} \\ 
        &= \frac{e^{-(\lambda_1 + \lambda_2)}}{k!} \sum\limits_{i=0}^k
        \frac{k!}{i!(k-i)!} \lambda_1^{k-i}\lambda_2{i} \\ 
        &= \frac{e^{-(\lambda_1 + \lambda_2)} (\lambda_1 + \lambda_2)^k}{k!}\\
        &\Rightarrow X_1 + X_2 \sim Pois(\lambda_1 + \lambda_2)
        \end{align*}
    \end{example}
    \begin{example}
        $X_1, X_2 \sim Geom(p) $ -- нез. $ k \in \mathbb{Z}_+ $ (количество неудач до первого успеха).
        $$ P(X_1 + X_2 = k) = \sum\limits_{i=0}^k P(X+1 = i) P(X_2 = k - i) = \sum\limits_{i=0}^k p (1-p)^i p (1 - p)^{k - i} = (k+1) p^2 (1-p) ^ k \sim NB(2, p)$$ 
    \end{example}
    \begin{example}
    Общая схема для непрерывных величин.\\
    $X, Y$ - нез. $p_{X, Y} = p_X(x) p_Y(y)$
    $$ P(X+Y \le t) = \iint_{x+y \le t} p_X(x) p_Y(y) dx dy = \int\limits_{-\infty}^{+\infty} p_X(x) \int\limits_{-\infty}^{t - x} p_Y(y) dydx = \int\limits_{-\infty}^{+\infty} p_X(x) p_Y(t-x) dx $$
        
    \end{example}
    \begin{example}
        $X_1, X_2 \sim Exp(\lambda)$ -- независимы
        $$ p_x = \lambda e^{-\lambda x} \cdot \mathbb{1} (x \ge 0) $$
        $$ p_{X+Y}(t) = \int\limits_0^{+\infty} \lambda^2 e^{-\lambda x} e^{-\lambda (t-x)} \mathbb{1} dx(t-x\ge 0) = \lambda^2 \int\limits_0^t e^{-\lambda t} dx = \lambda^2 t e^{-\lambda t} \sim \text{Г}(2, \lambda)$$
        Аналогично:
        $$ X_1 \dots X_n \sim Exp(\lambda) i.i.d \Rightarrow \sum\limits_{k=1}^n X_k \sim \text{Г}(n, \lambda)$$
    \end{example}
    \begin{definition}
        $(X_1 \dots X_n) \sim $ нез $N(0, 1)$ -- стандартный гауссовский вектор $$\Leftrightarrow p(x_1 \dots x_n) = \frac{1}{\sqrt{2\pi}^n} \exp{-\frac{1}{2} \sum\limits_{i=1}^n x_i^2} = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$$
    \end{definition}
    \begin{example}
        $X, Y \sim $ нез $N(0, 1)$
        \begin{align*}
        P_{X+Y}(t) &= \frac{1}{2\pi} \int\limits_{-\infty}^{+\infty} \exp{(-\frac{x^2}{2} - \frac{(t-x)^2}{2})}dx = \frac{1}{2\pi} \int\limits_{-\infty}^{+\infty} \exp{-\frac{1}{2}(2x^2 - 2tx + t^2)}dx= \left[y = x - \frac{t}{2}\right] \\ &=\frac{1}{2\pi} \int\limits_{-\infty}^{+\infty} \exp{-\frac{1}{2} (2y^2 + \frac{t^2}{2})}dy = \frac{1}{\sqrt {2\pi 2}} e^{-\frac{t^2}{4}}  \underbrace{\int\limits_{-\infty}^{+\infty} \frac{1}{\sqrt\pi} e^\frac{-y ^2}{1} dy}_1 =\frac{1}{\sqrt {4\pi}} e^{-\frac{t^2}{4}} \sim N(0, 2)
        \end{align*}
        В общем случае:
        $X \sim N(\nu_1, \sigma_1^2), Y \sim N(\nu_2, \sigma_2^2)$ нез $ \Rightarrow X+Y \sim N(\nu_1 + \nu_2, \sigma_1^2+\sigma_2^2)$
    \end{example}

    \subsection{Об интегралах}
    $(\Omega;\A;P)$ -- вероятностное пространство.
    $X$ -- случайный вектор, $P_X$ распределение с.в
    $$ P_X(B) = P(X \in B) = P(\{\omega: X(w) \in B\})$$
    $g: \mathbb{R}^n \to \mathbb{R} $,  измерима
    $$ \int\limits_\Omega g(X(\omega)) P(d\omega) = \int\limits_{\mathbb{R}^n} g(x) P_X(dx) = 
        \begin{cases}
            \sum\limits_k g(x_k) P(X=x_k) & X \text{- дискретная} \\
            \int g(x) p(x) dx & X \text{-- непрерывная} 
        \end{cases}
    $$
    \begin{theorem}{Теорема Фубини}
    $X, Y$ -- независимые св, $g: R^2 \to R$, изм. Тогда
    $$\iint g(x, y) P_{x, y} (dx, dy) = \int \left[ \int g(x,y) dF_X(x) \right] dF_Y(y) = \int \left[ \int g(x,y) dF_Y(y) \right] dF_X(x)$$
    \end{theorem}
    \begin{example}
        $X, Y$ -- нез
        $$ P(X+Y \le t) = \iint_{x+y\le t} P_X(dx) P_Y(dy) = \int\limits_\mathbb{R} \left[
        \int\limits_{y \le t-x} P_Y(dy) \right] dF_X(x) = \int\limits_\mathbb{R} F_Y (t-x) dF_X(x) \text{- свертка}$$
        $X, Y$ -- непр: $p(t) = \int\limits_\mathbb{R} P_Y(t-x) P_X(x) dx$
    \end{example}
\section{Характеристики случайных величин}
    \subsection{Матожидание}
    \begin{definition}
        $X$ - с.в.
        $$ EX = \int\limits_\mathbb{R} x dF(x) = 
            \begin{cases}
                \sum x_k P(X=x_k) & X \text{ -- дискретная} \\
                \int\limits_\mathbb{R} x p(x) dx & X \text{ -- непрерывная}
            \end{cases}
        $$
    \end{definition}
    \begin{property}
        $g: R^n \to R, X_1 \dots X_n$ - св
            $$ E g(X_1\dots X_n) = \int\limits_\mathbb{R} y dF_Y(y) = \int\limits_{R^n} g(x_1\dots x_n) P_{X_1 \dots X_n} (dx_1 \dots dx_n)$$
    \end{property}
    \begin{property}
        $E(c_1X_1+c_2X_2) = c_1EX_1 +c_2EX_2$
    \end{property}
    \begin{property}
        $X, Y \textbf{- нез} \Rightarrow EXY = EX\cdot EY$
    \end{property}
    \begin{property}
        $P(X\ge0) =1 \Rightarrow EX \ge 0$
    \end{property}
    \begin{theorem}{Неравенство Маркова}
       $$ X \ge 0, \exists EX \Rightarrow P(X\ge c) \le \frac{EX}{c}, \forall c > 0 $$
        \begin{proof}
            $$ P(X\le c) = \int\limits_{x \ge c} dF(x) \le \int\limits_{x \ge c} \frac{x}{c}\le \frac{1}{c} \int\limits_\mathbb{R} x dF(x) = \frac{1}{c} EX $$
        \end{proof}
    \end{theorem}
    \begin{remark}
        $$ E \mathbb{1} (X \in B) = P(X \in B)$$
    \end{remark}
        \begin{property}
        $X \ge 0, EX=0 \Rightarrow P(x=0)=1$
        \begin{proof}
        $$ 1 = P(X \ge 0) = P(X \ge c) + P(X < c) \le 0 + P(X < c) \Rightarrow P(X < c) = 1, \forall c > 0$$
        \end{proof}
    \end{property}
    \begin{property}
        $X$ - дискретная. $\mathbb{N}$ -- носитель $X$, $\Rightarrow EX = \sum\limits_{k=1}^\infty P(X \ge k)$
        \begin{proof}
            $ \sqsupset p_i = P(X=i)$
            $$ EX = p_1 + 2 p_2 + 3 p_3 \dots = (p_1 + p_2 + \dots) + (p_2 + p_3 + \dots) + \dots = \sum\limits_{k=1}^\infty P(X\ge k)$$
        \end{proof}
    \end{property}
    \begin{property}
        $X \text{ - непр}, X \ge 0  \Rightarrow \int\limits_0^{+\infty} P(X \ge r) dr$
    \end{property}
    \begin{example}
        $\sqsupset P(X = 1000) = 0.01, P(X = 10) = 0.99 \Rightarrow EX = 9.9 + 10 = 19.9$ 
        Этот пример должен показывать, что иногда реальное распределение может быть не очень "хорошим", и "среднее" aka матожидание не очень хорошо будет отображать реальное положение дел. 
    \end{example}
    \subsection{Дисперсия}
    \begin{definition}
        $X$ - с.в.
        $$ \Var X = DX = E(X-EX)^2 \text{-- дисперсия}$$
    \end{definition}
    \begin{property}
        $ \Var X \ge 0$
    \end{property}
    \begin{property}
        $$E(X-EX)^2 = E(X^2-2XEX + (EX)^2) = EX^2 - 2(EX)^2 + EX^2 = EX^2 - (EX)^2$$
    \end{property}
    \begin{property}
        $$ \Var(aX) = a^2 \Var X $$
    \end{property}
    \begin{property}
         \begin{align*}
         \Var(X \pm Y)X = E(X \pm Y)^2 - (E(X \pm Y))^2 &= EX^2 \pm 2 XY + EY^2 - (EX)^2 \mp 2EXEY - (EY)^2\\ &= \Var X + \Var Y \pm 2 (EXY - EX\cdot EY
         \end{align*}
    \end{property}
    \begin{property}
        $X, Y \text{-- нез} \Rightarrow \Var(X \pm Y) = \Var X + \Var Y$
    \end{property}
    \begin{property}
        $ \Var c = 0$
    \end{property}
    \begin{theorem}{Неравенство Чебышева}
        $X$ - с.в, $\exists EX, \Var X \Rightarrow P(|X-EX|\ge\varepsilon) \le\frac{\Var X}{\varepsilon^2}$
        \begin{proof}
            $$P(|X-EX|\ge\varepsilon) = P((X-EX)^2 \ge \varepsilon) \le_{\text{Маркова}} \frac{E(X-EX)^2}{\varepsilon^2} $$
        \end{proof}
    \end{theorem}
    \begin{remark}
        $$ P(|X-EX| \ge 3 \sqrt{\Var X})\le \frac{1}{9}$$
    \end{remark}
    \begin{property}
        $\Var X = 0 \Rightarrow P(x=c) = 1$
        \begin{proof}
            $$ P(|X-EX| \ge 0) = 1$$
            $$ P(|X-EX| \ge \varepsilon) + P(|X-EX| \ge\varepsilon) \le P(|X-EX|\le\varepsilon) \le 1$$
        \end{proof}
    \end{property}    
    \begin{property}
        $$\Var X = \min_{a\in\mathbb{R}} E(X-a)^2$$
        \begin{proof}
            $$f(a) = EX^2 - 2aEX + a^2 \Leftrightarrow a_{*} = \frac{2EX}{2} = EX- \min$$
        \end{proof}
    \end{property}
    \begin{property}
        $\Var (X +c) = \Var X$
    \end{property}
    \begin{example}
        $I_C: P(X=c) = 1; EX=c, \Var X = 0$
    \end{example}
    \begin{example}
        $Bern(p)$
        $$ EX = 1p + 0(1-p) = p$$
        $$ \Var X = EX^2 -(EX)^2 = (1^2 p + 0^2 (1-p)) - p^2 = p(1-p) = pq$$
    \end{example}
    \begin{example}
        $Bin(n, p), S_n \sim Bin(n, p), X_1 \dots X_n -- \text{i.i.d.} Bern(p) $
        $$ ES_n = \sum EX_i = np $$
        $$ \Var S_n = np(1-p) $$
    \end{example}
    \begin{example}
        $ Pois(\lambda) $
        $$ EX = \sum\limits_{k=1}^\infty e^{-\lambda} \frac{\lambda^k}{k!} = e^{-\lambda} \sum\limits_{k=1}^\infty \frac{\lambda^{k-1}}{(k-1)!} = \lambda $$
        \begin{align*}
        E(X(X-1)) &=EX^2 - EX = EX^2 - (EX)^2 + (EX)^2 -EX = \Var X + (EX)^2 -EX \\ &= \sum\limits_{k=2}^\infty k(k-1) \frac{e^{-\lambda} \lambda^k}{k!} =
        \lambda^2 \sum\limits_{k=2}^\infty \frac{\lambda^{k-2}e^{-\lambda}}{(k-2)!} = \lambda^2 \Rightarrow \Var X = \lambda
        \end{align*} 
    \end{example}
    \begin{example}
        $X_1, X_2 \sim Geom(p) $ -- нез. $ k \in \mathbb{Z}_+ $ (количество неудач до первого успеха).$EX = \sim_{k=1}^\infty P(x\ge k)$
        $$ P(X\ge k) = \sum\limits_{i=k}^\infty (1-p)p = p \frac{(1-p)^k}{1 - (1-p)} = (1 - p)^k$$
        \begin{align*}
        E(X(X-1)) &= \sum\limits_{k=2}^\infty k(k-1) (1-p)^(k-2) p = (1-p)^2 p \sum\limits_{k=2}^\infty k(k-1)(1-p)^{k - 2} = (1-p)^2 p \sum\limits_{k=2}^\infty \frac{\delta^2 (1-p)^k}{\delta p^2} \\
        &= (1-p)^2 p \frac{\delta^2}{\delta p^2} (\sum\limits_{k=2}^\infty (1-p)^k) =  (1-p)^2 p \frac{\delta^2}{\delta p^2} (\frac{(1-p)^2}{p}) = 2 \frac{(1-p)^2}{p^2} \Rightarrow \Var X = \frac{1 - p}{p} 
        \end{align*}
        Если носитель $\mathbb{N}: Y = X + 1, EY = \frac{(1-p)}{p} + 1 = \frac{1}{p}$
    \end{example}
    \begin{example}
        $U[a, b]: X \sim U[a, b], Y \sim U[0, 1] \Rightarrow X = (b-a)Y + a$
        $$ EY = \int\limits_0^1xdx = \frac{1}{2}$$
        $$ EY^2 = \int\limits_0^1x^2dx = \frac{1}{3} \Rightarrow Var Y = \frac{1}{3} -\frac{1}{4} = frac{1}{12}$$
        $$ EX = (b - a)\frac{1}{2} + a = \frac{a + b}{2}; \Var X = \frac{(b-a)^2}{12}$$
    \end{example}
    \begin{example}
        $N(\mu, \sigma^2), Y \sim N(0, 1), X \sim N(\mu, \sigma^2) \Rightarrow X = \sigma Y + \mu$
        $$ EY = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{+\infty} x e^{-x^2/2}dx = 0$$
        $$ EY^2 = \frac{1}{\sqrt{2 \pi}} \int\limits_{-\infty}^{+\infty} x^2 e^{-x^2/2}dx = 1$$
        $$ EY = 0, \Var Y = 1 \Rightarrow EX = \mu, \Var X = \sigma^2 $$
        Оценим $3-\sigma$ отклонение:
        \begin{align*} 
        P(|X - EX| > 3 \sqrt{\Var X}) &= P(|X-\mu| > 3\sigma) = P(|\frac{X -\mu}{\sigma}| > 3) = 1 -P(|\frac{X -\mu}{\sigma}| < 3) \\ &= 1 - \Phi(3) + \Phi(-3) = 2 (1 - \Phi(3)) \approx 0.00135
        \end{align*}
    \end{example}
    \begin{example}
        $Exp(\lambda), p(x) = \lambda e^{-\lambda x} \mathbb{1} (x \ge 0) $
        $$ EX = \int\limits_{0}^{+\infty} x \lambda e^{-\lambda x} dx = -xe^{-\lambda x} \bigg|_0^{+\infty} + \frac{1}{\lambda} \int\limits_{0}^{+\infty} \lambda e^{-\lambda x} dx = \frac{1}{\lambda} $$
        $$ EX^2 = \int\limits_{0}^{+\infty} x^2 \lambda e^{-\lambda x} =  -x^2e^{-\lambda x} \bigg|_0^{+\infty} + \frac{2}{\lambda} \int\limits_{0}^{+\infty} x \lambda e^{-\lambda x} dx = \frac{2}{\lambda^2} $$
        $$ \Var X = \frac{1}{\lambda^2}$$
    \end{example}
    \begin{example}
        $X_1 \dots X_n -- i.i.d. X_i \sim Exp(\lambda), S_n = X_1 + \dots + X_n \sim \Gamma (n, \lambda) $
        $$ ES_n = \frac{n}{\lambda}; \Var S_n = \frac{n}{\lambda^2}$$
    \end{example}
    \begin{example}
        $Cauchy(0, 1); \not \exists EX$
        $$ EX = \frac{1}{\pi} \int\limits_{-\infty}^{+\infty} \frac{xdx}{1+x^2}$$
    \end{example}
    \subsection{Другие характеристики случайных величин}
    \subsubsection{Моменты}
    \begin{definition}
        $k \in \mathbb{N}$.\\
        Момент порядка $k: EX^k$. \\
        Абсолютный момент порядка $k: E|X|^k$\\
        Центральный момент порядка $k: E(X - EX)^k$\\
        Абсолютный центральный момент порядка $k: E|X - EX|^k$
    \end{definition}
    \subsubsection{Коэффиценты ассиметрии/эксцесса}
    \begin{definition}
        Коэффицент ассиметрии: $ \frac{E(X-EX)^3}{\sigma^3}, \sigma = \sqrt{\Var X} $
        Если он $>0$, то правый хвост длиннее левого. Если $<0$, то наоброт.
    \end{definition}
    \begin{definition}
    Коэффицент эксцесса: $\frac{E(X-EX)^4}{\sigma^4} - 3$. \\
     Если он $>0$, то пик более крутой. Если $<0$, то более пологий.
    \end{definition}
    
    \subsubsection{Мода}
    \begin{definition}
            Дискретный случай: $x_k^*: x_k^* = arg\max_{x_k} P(X=x_k)$
            Непрерывный случай: $x^*: x_k^* = arg\max_{x \in \mathbb{R}} p(x)$
    \end{definition}
    \begin{remark}
    Но с модами бывают путаницу, потому что бывают \textbf{унимодальные} и \textbf{многомодальные} распределения. И если у первых один и глобальный, и минимальный максимум, то у вторых может быть и несколько локальных (да и глобальных) максимумов. 
    \end{remark}
    \subsubsection{Квантиль}
    \begin{definition}
        
        $\alpha \in (0, 1)$
        $q_\alpha$ - квантиль порядка $\alpha$ распределения $P_X$ $\Leftrightarrow P(X\ge q_\alpha) \ge 1-\alpha, P(X \le q_\alpha) \ge \alpha$
    \end{definition}
    \begin{remark}
        В дискретном случае квантиль определяется неоднозначно.
         $$ q_\alpha =\sup_{x\in \mathbb{R}} \{F(x) \le \alpha\} = \inf_{x\in \mathbb{R}} \{F(x) \ge \alpha\}$$
    \end{remark}
    \begin{remark}
        Важное замечание, что в непрерывных фукнциях, площадь под графиком плотности слева от $q_\alpha$ является $\alpha$, а справа $1 - \alpha$
    \end{remark}
    \begin{definition}
        Квантиль при $\alpha = \frac{1}{2}$ называется медианой.
    \end{definition}
    \begin{theorem}
        $ m = med(X), \forall a \exists E|x-a| \Leftarrow m = arg\min_{a \in \mathbb{R}} E|x-a| $
        \begin{proof}
            Не уменьшая общности, $m = 0$, иначе введем $X' = X - m$.
            $$E|X-c| \ge^? E|X|$$
            $$ с>0:  |X-c| - |X| = \begin{cases}
                -c & X> c\\
                c - 2X  & 0 < X \le c \\
                c & X\le 0
            \end{cases}
            $$
            \begin{align*}
             \Rightarrow E(|X-c| -|X|) &= E(|X-c| - |X|) \mathbb{1} (X\le 0)  + E(|X-c| -|X|) = E(|X-c| - |X|) \mathbb{1}(X>0)\\ 
             &\ge cE \mathbb{1} (X\le0) - c E \mathbb{1}(X>0) \\
             &= c(2 P(X\le 0) - 1) \ge 0
            \end{align*}
            $$ c < 0: X'' = -X, c'' = -c$$
        \end{proof}
    \end{theorem}
    \begin{example}
        $X \sim N(0, 1), Y = X^2. X, Y$ -- не независимы
        $$ P(X \le 1, Y \le 1) = P(X \le 1, -1 \le X \le 1) = \Phi(1) - \Phi (-1) = 2 \Phi(1) - 1$$
        $$ P(X \le 1) = \Phi(1), P(Y \le 1) = 2 \Phi (1) - 1$$
        Но:
        $$ \Phi(1) (2\Phi(1) - 1) \ne 2 \Phi(1) - 1$$
        
        $$ EX = 0, EY = EX^2 = 1, EXY= EX^3 = \frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{+\infty} x^3 e^{-x^2/2} dx = 0$$
    \end{example}
    \subsection{Характеристики нескольких случайных величин}
    \subsubsection{Ковариация}
    \begin{definition}{Ковариация}
        $$ \cov (X, Y) = E(X-EX)(Y-EY)$$
    \end{definition}
    \begin{property}
        $\cov (X, Y) = E(XY - (EX)Y - X(EY) + EXEY) = E(XY) - EXEY$
        $$ \Var(X \pm Y) = \Var X + \Var Y \pm 2 \cov(X, Y)$$
        $$ \Var(X_1 + \dots  + X_n) = \sum\limits_{i=1}^n \Var X_n + 2 \sum\limits_{i < j}\cov(X_i, X_j)$$
    \end{property}
    \begin{property}
        $\cov  (X, Y) = \cov (Y, X)$
    \end{property}
    \begin{property}
        $\cov (X, X) = \Var X$
    \end{property}
    \begin{property}
        $\cov (c_1X_1 + c_2X_2, Y) = c_1 \cov (X_1, Y)  + c_2 \cov(X_2, Y)$
    \end{property}
    \begin{property}
        $\cov (X, c) = 0$
    \end{property}
    \begin{property}
        $\cov(X+c, Y) = \cov (X, Y)$
    \end{property}
    \begin{property}
        $X, Y \text {-- нез} \Rightarrow \cov(X, Y) = 0 $
    \end{property}
    \subsubsection{Корреляция}
    \begin{definition}{Коэффициент корреляции}
        $$ \rho = \frac{\cov (X, Y) }{\sqrt{\Var X \cdot \Var Y}}$$
    \end{definition}
    \begin{property}
        $|\rho(X, Y)| \le 1$
        $$ \rho(X, Y) = \frac{\cov (X - EX, Y - EY) }{\sqrt{\Var X \cdot \Var Y}} = \cov {\frac{X - EX}{\sqrt{\Var X}}, \frac{Y - EY}{\sqrt{\Var Y}}} = \cov(\widetilde{X}, \widetilde{Y}) $$
        \begin{align*}
         E\widetilde{X} = 0; \Var\widetilde{X} = 1 \Rightarrow \Var(\widetilde{X} + \widetilde{Y}) = 
        \Var\widetilde{X} +  \Var\widetilde{Y} + 2\cov(\widetilde{X}, \widetilde{Y}) &\Rightarrow \rho(X, Y) \ge -1 \\
        \Var (\widetilde{X} - \widetilde{Y}) = 2 - 2 \rho (X, Y) \ge 0 &\Rightarrow \rho (X, Y) \le 1
        \end{align*}
    \end{property}
    \begin{property}
        $X, Y = aX + b$
        $$ \rho (X, Y) = \frac{\cov(aX+b, X)}{\sqrt{\Var(aX+b)\Var X}} = \frac{a \cov(X, X) }{|a| \Var X} = \sign a$$
    \end{property}
    \begin{property}
        $\rho(X, Y) = 1$
        $$ \Var (\widetilde{X} - \widetilde{Y}) = 2 - 2 = 0 \Rightarrow \widetilde{X} - \widetilde{Y} = C \Rightarrow \frac{Y - EY}{\sqrt{\Var Y}} = \frac{X - EX}{\sqrt{\Var X}} - C $$
    \end{property}
    \begin{property}
        $\rho(X, Y) = -1$
        $$ \Var (\widetilde{X} + \widetilde{Y}) = 2 - 2 = 0 \Rightarrow \widetilde{X} + \widetilde{Y} = C \Rightarrow \frac{Y - EY}{\sqrt{\Var Y}} = -\frac{X - EX}{\sqrt{\Var X}} - C$$
    \end{property}
    \begin{property}
        $|\rho(X, Y)| = 1 \Leftrightarrow Y = aX + b, a \ne 0 $
        $$
            \begin{cases}
                a > 0 \Leftrightarrow& p(X, Y) = 1\\
                a < 0 \Leftrightarrow& p(X, Y) = -1\\
            \end{cases}
        $$
        
    \end{property}
    \subsection{Характеристики случайного вектора}
    \begin{definition}
        $X = (X_1 \dots X_n)$ -- случайный вектор.\\
        $EX = (EX_1, \dots EX_n) $ --вектор матожиданий \\ 
        $ \Var X = E(X-EX)(X-EX)^T  = (\cov(X_i, X_j)_{i, j}$ -- матрица ковариаций.
    \end{definition}
    \begin{property}
        $EAX = AEX, A \in \mathbb{M}_{m\times n} (\mathbb{R})$ \\
    \end{property}
    \begin{property}
        $\Var X = (\Var X) ^T$
    \end{property}
    \begin{property}
        $\Var X \ge 0$ -- неотрицательная определенность
        \begin{proof}
            $$ t^T \Var X t = t^T (E(X-EX)(X-EX)^T) t = Et^T (X-EX)(X-EX)^T t = E (t^T (X-EX))^2 \ge 0$$
        \end{proof}
    \end{property}
    \begin{property}
        $\Var AX = EA(X-EX)(X-EX)^TA^T = A \Var X A^T$
    \end{property}
    \begin{property}
        $\Var (X + c) = \Var X$
    \end{property}
    \begin{property}
        $X, Y \text{- нез} \Rightarrow \Var(X+Y) = \Var(X) + \Var(Y)$
        \begin{proof}
            Как выглядит матрица ковариаций? 
        \begin{align*}
        i = j &\Rightarrow \cov (X_i + Y_i, X_i + Y_i) = \Var X_i + \Var Y_i \\
        i \ne j &\Rightarrow \cov (X_i + Y_i, X_j + Y_j) = \cov(X_i, X_j) + \underbrace{\cov(Y_i, X_j)}_0 + \underbrace{\cov(X_i, Y_j)}_0 + \cov (Y_i, Y_j)
        \end{align*}
        \end{proof}
    \end{property}
    \subsubsection{Полиномиальное распределение}
    \begin{definition}
        $ Poly(n, p), p = (p_1 \dots p_m) $ -- полиномиальное распределение
        $$ P(S_n^{(1)}=k_1, \dots , S_n^{(m)}=k_m = p_1^{k_1} \cdot \dots \cdot p_m^{k_m} = \frac{n!}{k_1! \cdot \dot \cdot k_m!}$$
    \end{definition}
    \begin{example}
        $S_n = \sum\limits_{i=1}^{n} X_i,  X_i \text{-- нез}, X_i \sim Poly(1, p)$
        $$ ES_n = n EX_1 = [EX_1^{(j)} = p_j] =np $$
        $$ \Var _n = n \Var (X_i), \Var(X_i) = (\cov(X_i^{(k)}, X_i^{(j)}))_{1\le k, j\le m} $$
        
        \begin{align*}
         k = j &\Rightarrow \Var X_i^{(k)} = E(X_i^{(k)})^2 - (EX_i^{(k)})^2 = p_j - p_k^2= p_k(1-p_k)\\
        k \ne j &\Rightarrow \cov (X_i^{(k)}, X_i^{(j)}) = \underbrace{E X_i^{(k)} X_i^{(j)}}_0 - EX_i^{(k)}\cdot EX_i^{(j)} = -p_jp_j\\
        \end{align*}
    \end{example}
    \subsubsection{Многомерное нормальное распределение}
    \begin{example}
        $$X = (X_1 \dot X_n), X_i \text{-- нез}, X_i \sim N(0, 1),         p(x)=\frac1{(\sqrt{2\pi})^n}\exp\left(-\frac{||x_i||^2}2\right) \Rightarrow EX=0, \Var X = I_n$$
        $$ Y = AX _ \mu, Y \sim N(\mu, AA^T) \Rightarrow EY = \mu, \Var Y = \Var AX = AI_nA_T = AA_T$$
        $$ U \sim N(\mu, \Sigma), \Sigma = \Sigma^T, \Sigma \ge 0 \Rightarrow U = \sqrt{\Sigma} X + \mu$$
    \end{example}
    \begin{claim}
        $\begin{pmatrix}
            X \\ Y
        \end{pmatrix} \sim N(\begin{pmatrix}
            \mu_1 \\ \mu_2
        \end{pmatrix}, \begin{pmatrix}
        \Sigma_{11} & \Sigma_{12}\\
        \Sigma_{21} & \Sigma_{22}
        \end{pmatrix}) \Rightarrow \begin{cases}
            X \sim N(\mu_1, \Sigma_11) \\
            Y \sim N(\mu_2, \Sigma_22)
        \end{cases}$
        \begin{proof}
            $$ \begin{pmatrix}
        \Sigma_{11} & \Sigma_{12}\\
        \Sigma_{21} & \Sigma_{22}
        \end{pmatrix} = \underbrace{\begin{pmatrix}
        U_{11} & U_{12}\\
        U_{21} & U_{22}
        \end{pmatrix}}_U \begin{pmatrix}
        \Lambda_{1} & 0\\
        0 & \Lambda_{2}
        \end{pmatrix} \underbrace{\begin{pmatrix}
         U_{11}^T & U_{21}^T\\
        U_{12}^T & U_{22}^T
        \end{pmatrix}}_{U^T}$$
        Причем имеет место следующие соотношения (так как U ортогональна
        $$
        \begin{pmatrix}
        U_{11} & U_{12}\\
        U_{21} & U_{22}
        \end{pmatrix} 
        \begin{pmatrix}
         U_{11}^T & U_{21}^T\\
        U_{12}^T & U_{22}^T
        \end{pmatrix}
        = 
        \begin{pmatrix}
        I & 0\\
        0 & I
        \end{pmatrix}
        $$
        $$
        \begin{pmatrix}
         U_{11}^T & U_{21}^T\\
        U_{12}^T & U_{22}^T
        \end{pmatrix}
        \begin{pmatrix}
        U_{11} & U_{12}\\
        U_{21} & U_{22}
        \end{pmatrix}
          = 
        \begin{pmatrix}
        I & 0\\
        0 & I
        \end{pmatrix}
        $$
        Тогда можем представить $X,Y$ вот так:
        $$
        \begin{pmatrix}
            X \\ Y
        \end{pmatrix}
        =
        \begin{pmatrix}
            \mu_1 \\ \mu_2
        \end{pmatrix}
        + 
        \begin{pmatrix}
        U_{11} & U_{12}\\
        U_{21} & U_{22}
        \end{pmatrix} 
        \begin{pmatrix}
        \sqrt{\Lambda_{1}} & 0\\
        0 & \sqrt{\Lambda_{2}}
        \end{pmatrix} 
        \begin{pmatrix}
         U_{11}^T & U_{21}^T\\
        U_{12}^T & U_{22}^T
        \end{pmatrix}
        \begin{pmatrix}
            \xi_1 \\ \xi_2
        \end{pmatrix}
        $$
        Если расписать это отдельно для X (для Y аналогично), то получится, что это линейная комбинация $\xi_1, \xi_2$:
        $$ X = \mu_1 +  \begin{pmatrix}
        U_{11} \sqrt{\Lambda_{1}} & U_{12}  \sqrt{\Lambda_{2}}\\
        U_{21}  \sqrt{\Lambda_{1}} & U_{22}  \sqrt{\Lambda_{2}}
        \end{pmatrix}
        \begin{pmatrix}
         U_{11}^T \xi_1 & U_{21}^T\xi_2 \\
        U_{12}^T \xi_1& U_{22}^T \xi_2
        \end{pmatrix} = \mu_1 + U_{11}\sqrt{\Lambda_{1}}U_{11}^T \xi_1 + \dots + U_{22}  \sqrt{\Lambda_{2}} U_{22}^T \xi_2$$
        Дальше уже чисто техническая работа: сгруппировать слагаемые и показать, что матрица $\Var X = \Sigma_{11}$ и аналогично для Y. 
        \end{proof}
    \end{claim}
    \begin{definition}{Некоррелированность}
        $X, Y$ некореллированны $\Leftrightarrow \rho(X, Y) = \cov(X, Y) = 0$
    \end{definition}
    \begin{claim}
        $X = (X_1 \dots X_n) \sim N(\mu, \Sigma)$ тогда $X_i, X_j \text{-- нез } \Leftrightarrow \cov(X_i, X_j) =0 $
        \begin{proof}
            $(\Rightarrow)$ уже знаем \\
            $(\Leftarrow)$ рассмотрим $\begin{pmatrix}
                 X_i \\ X_j
            \end{pmatrix}$
            $$ \begin{pmatrix}
                X_i \\ X_j
            \end{pmatrix}
            \sim
            N(
            \begin{pmatrix}
                \mu_i \\ \mu_j
            \end{pmatrix}
            ,
            \begin{pmatrix}
                \sigma_{x_i}^2 & 0 \\
                0 & \sigma_{x_j}^2
            \end{pmatrix}
            )
            $$
            $$\sigma_{x_i} = \sigma_{x_j} = 0 \Rightarrow \text{Оба вырожденны} \Rightarrow\text{они независимы}$$
            $$ \sigma_{x_i}, \sigma_{x_j} > 0: p(x_i, x_j) = \frac{1}{2\pi \sqrt{\sigma_{x_i}^2\sigma_{x_j}^2}} \exp (-\frac{1}{2}
            \begin{pmatrix}
                x_i - \mu_i \\ x_j - \mu_j
            \end{pmatrix}^T
            \begin{pmatrix}
                \sigma_{x_i}^2 & 0 \\
                0 & \sigma_{x_j}^2
            \end{pmatrix}
            \begin{pmatrix}
                x_i - \mu_i \\ x_j - \mu_j
            \end{pmatrix}
            ) = p_{X_i}(x_i) \cdot p_{X_j}(x_j)
            $$
            В случае, если только одно ровно нулю, то все тоже хорошо получится.
        \end{proof}
    \end{claim}
    \subsection{Вероятностные неравенства}
    Неравенство Маркова: $ X \ge 0, \exists EX \Rightarrow P(X\ge c) \le \frac{EX}{c}, \forall c > 0 $\\
    Неравенство Чебышева: $\exists EX, \Var X \Rightarrow P(|X-EX|\ge\varepsilon) \le\frac{\Var X}{\varepsilon^2}$
    \begin{claim}{Слабый закон больших чисел}
        $$X_1 \dots X_n -- i.i.d , \mu, \sigma^2, \widetilde{X} = \frac{\sum\limits_{i=1}^n x_i}{n} \Rightarrow P(|\widetilde{X} - \mu| \ge \varepsilon) \xrightarrow{n \to \infty} 0, \forall \varepsilon \ge 0$$
        \begin{proof}
            
        $ P(|\widetilde{X} - \mu| \ge \epsilon)\le \frac{\Var X}{\varepsilon^2} = \frac{\sigma^2}{n \varepsilon^2}$
        \end{proof}
    \end{claim}
    \begin{definition}{Вероятностное КБШ}
        $ L_2 = \{X: EX^2 < \infty\}$\\
        Рассмотрим функцию: $(\dots, \dots): L2\times L2 \to \mathbb R. (X, Y) = EXY$ \\
        Это очень похоже на скалярное произведение, а для него у нас есть КБШ: $|EXY| \le EX^2 \cdot EY^2$. Что неправда, а значит не скалярное произведение. 
        Но вот если рассмотрим функцию $(X, Y) = E(X-EX)(Y-EY)$, она линейна, аргументы перестановочны, ну и $(X, X) \ge 0$, а значит это скалярное произведение.
        Тогда получаем свойство $\cov^2(X, Y) \le \Var X \cdot \Var Y$
    \end{definition}
    \begin{definition}{Вероятное неравенство Гёльдера}
        $E|XY| \le (E|X|^p)^{1/p} (E|Y|^q)^{1/q},  pq = 1$
    \end{definition}
    \begin{definition}{Вероятностное неравенство Минковского}
        $(E|X+Y|^p)^{1/p} \le (E|X|^p)^{1/p} + (E|Y|^p)^{1/p}$
    \end{definition}
    \begin{definition}{Вероятностное неравенство Йенсенна}
        $g \text {-- выпукла вниз} \Rightarrow g(EX) \le Eg(X)$
    \end{definition}
    \begin{definition}{Неравенство Ляпунова}
        $(E|X|^p)^(1/p) \le (E|X|^q)^{1/q}, p < q$
    \end{definition}
    \begin{remark}
        Из неравенства Ляпунова следует, что если существует $k$-ый момент, то существуют и все предыдущие.
    \end{remark}
    \subsection{Условные распределения. Условные математические ожидания и дисперсии}
    \begin{definition}{Условное распределение}\\
        $X_1 \dots X_n \text{-- дискретные}:$ 
        $$P(X_1=x_1, \dots X_n=x_n | X_{k+1}=x_{k+1} \dots X_n=x_n) = \frac{P(X_1=x_1, \dots X_n=x_n)}{P(X_{k+1}=x_{k+1} \dots X_n=x_n)}$$
        $X_1 \dots X_n \text{-- непрерывные}:$
        $$ p(x_1, \dots x_n | x_{k+1}, \dots x_n) = \frac{p_{X_1\dots X_n}(x_1, \dots x_n)}{p_{X_{k+1}\dots X_n}(x_{k+1}, \dots x_n)}$$
    \end{definition}
    \begin{example}
        $(X, Y) \text{-- диск}: P(X=k) = \sum\limits_j P(X=k| Y=y_j) \cdot P(Y=y_j)$\\
        $(X, Y) \text{-- непр}: P_X(x) = \int p(x|y) \cdot p(y) dy$
    \end{example}
    \subsubsection{Условное матожидание}
    \begin{definition}{Условное матожидание}
        \\
        Дискретные: $E(X|Y) = \sum x_k p(X=x_k| Y)$\\
        Непрерывные: $E(X|Y) = \int x p(x|y) dx$ 
    \end{definition}
    \begin{remark}
        Обычное матожидание минимизирует: $EX = arg\min_a E(X-a)^2$ \\
        Условное: $E(X|Y) = arg\min_{f(Y)\in L_2} E(X-f(Y))^2$\\
        То есть идет минимизация по функции. 
    \end{remark}
    \begin{remark}
        Давайте рассмотрим 
        $ M = \{f(Y): Ef^2(Y) < \infty\}$\\ 
        Геометрической интерпретацией матожидания $E(X|Y)$ будет ортогональная проекция $X$ на $M$.
    \end{remark}
    \begin{property}
        $E(X|X) = X$
    \end{property}
    \begin{property}
        $X, Y \text{-- нез} \Rightarrow E(X|Y) = EX$
    \end{property}
    \begin{property}
        Формула полной вероятности для матожидания: $EX  = E_Y[E(X|Y)]$
    \end{property}
    \subsubsection{Условная дисперсия}
    \begin{definition}{Условная дисперсия}
        $$ \Var (X|Y) = E(X-E(X|Y))^2 = E(X^2|Y) - (E(X|Y))^2$$
    \end{definition}
    \begin{property}
        $\Var Y = E\Var (Y|X) + \Var E(X|Y)$
        \begin{proof}
            $\Var Y = EY^2 - (EY)^2 = E\Var(Y|X) + \underbrace{EE^2(Y|X) - E^2E(Y|X)}_{\Var E(X|Y)}$
            $$ EY^2 = E(E(Y^2|X))=E(\Var (Y|X) + E^2(Y|X))$$
            $$ EY = EE(Y|X)$$
        \end{proof}
    \end{property}
    \begin{example}
        $$\begin{pmatrix}
            X \\ Y
        \end{pmatrix} \sim N(\begin{pmatrix}
            \mu_1 \\ \mu_2
        \end{pmatrix}, \begin{pmatrix}
        \Var X & \cov(X, Y)\\
        \cov(Y, X) & \Var Y
        \end{pmatrix}) \Rightarrow E(Y|X) = \mu_2 + \frac{\cov(X, Y)}{\Var X} (X - \mu_1)$$
        \begin{proof}
            $$ U = Y -  \frac{\cov(X, Y)}{\Var X} (X - \mu_1)$$
            $$ \cov(U, X) = \cov(X, Y) - \cov(X, Y) = 0 \Rightarrow U, Y \text{-- нез}$$
            \begin{align*}
            E(Y|X) &= E( Y -  \frac{\cov(X, Y)}{\Var X} X +  \frac{\cov(X, Y)}{\Var X} X| X)\\ &= E(U|X) +  \frac{\cov(X, Y)}{\Var X} E(X|X) = EU + \frac{\cov(X, Y)}{\Var X} X\\ &= \underbrace{\mu_2}_{EY} + \frac{\cov(X, Y)}{\Var X} (X - \underbrace{\mu_1}_{EX})
            \end{align*}
        \end{proof}
    \end{example}
    \subsection{Введение в методы Монте-Карло, ЦПТ}
    Пусть мы хотим вычислить какой-то интеграл: 
    $$\int\limits_A f(x_1 \dots x_n) dx_1 \dots dx_n = \frac{\mu A}{\mu A} \int\limits_A f(x_1 \dots x_n) dx_1 \dots dx_n = \mu A \cdot E_{U(A)} f(X_1 \dots X_n) $$
    $$ \Rightarrow I_n = \frac{1}{n} \sum\limits_{i=1}^n f(X_{i1}, \dots X_{in}), (X_{i1}, \dots  X_{in}) -- i.i.d \sim U(A) $$
    Тогда по закону больших чисел $I_n \approx Ef(X_1 \dots X_n)$
    Ну и в целом методы, когда мы генерируем много каких-то случайных величин, аггрегируя результат, называются \textbf{методами Монте-Карло}.
    \begin{remark}
        У нас уже была теорема Муавра-Лапласа, которая утверждала, что
        $$
            S_n = \sum\limits_{i=1}^n X_i, X_i -- i.i.d, X_i \sim Bern(p)
        $$
        $$
        \sup_{x\in\mathbb{R}} |P(\frac{S_n-np}{\sqrt{npq}}\le x) - \Phi(x)|\underset{n\to\infty}\longrightarrow0
        $$
        
    \end{remark}
    \begin{theorem}{Центральная предельная теорема. ЦПТ}
        $$
            S_n = \sum\limits_{i=1}^n X_i, X_i -- i.i.d, \exists EX, \Var X
        $$
        $$
        \sup_{x\in\mathbb{R}} |P(\frac{S_n-nEX_1}{\sqrt{n\Var X}}\le x) - \Phi(x)|\underset{n\to\infty}\longrightarrow0
        $$
    \end{theorem}
    \subsection{Сходимости}
    \begin{definition}{почти наверное}\\
    $X_1 \dots X_n $ -- с.в, которые заданы на $(\Omega, \A, P)$\\
    $$X_n \xrightarrow{a.s.} X \Leftrightarrow \begin{cases}
        P(\{w: \lim\limits_{n\to \infty} X_n(w) = X(w)\}) = 1 \\
        P(\{w: \lim\limits_{n\to \infty} X_n(w) \ne X(w)\}) = 0 \\
    \end{cases}$$
    \end{definition}
    \begin{definition}{По вероятности. P}
        $$X_n \xrightarrow[n\to\infty]{P} X \Leftrightarrow \begin{cases}
        P(|X_n - X|\ge\varepsilon) \underset{n\to\infty}\longrightarrow 0, \forall \varepsilon > 0 \\
        P(|X_n - X|<\varepsilon) \underset{n\to\infty}\longrightarrow 1 \\
    \end{cases}$$
    \end{definition}
    \begin{definition} {В среднем порядка p, $L_p$}
        $$X_n \xrightarrow{L_p} X \Leftrightarrow E|X_n-X|^p \underset{n\to\infty}{\longrightarrow} 0$$
    \end{definition}
    \begin{definition}{По распределению, d}
        $X_n: (\Omega_n, \A_n, P_n), X: (\Omega, \A, P)$
        $$ X_n \xrightarrow{id} X \Leftrightarrow Ef(X_n) \to Ef(X), \forall f \text{--  непр и ограниченной}$$
    \end{definition}
    \begin{theorem}
        \begin{enumerate}
            \item $a.e \Rightarrow P$ из сходимости почти наверное следует сходимость по вероятности
            \item $L_p \Rightarrow P$ из сходимости в среднем следует сходимость по вероятности
            \item $P \Rightarrow d$ из сходимости по вероятности следует сходимость по распределению
        \end{enumerate}
        \begin{proof}
            $(P \Rightarrow d)$
            \begin{align*}                
            |E (f(X_n) - f(X)| &\le |E (f(X_n) - f(X)|\mathbb{1}(|X_n-X| \ge \underbrace{\delta_\varepsilon}_\text{из непр f}) + |E \underbrace{|f(X_n) - f(X)|}_{<\varepsilon}\mathbb{1}(|X_n-X| < \underbrace{\delta_\varepsilon}_\text{из непр f})\\
            &\le 2M P(|X_n-X|\ge \delta_\varepsilon) + \varepsilon
            \end{align*}
            $(L_p \Rightarrow P)$
            $$ P(|X_n - X| \ge \varepsilon) \le P(|X_n - X|^p \ge \varepsilon^p) \le \frac{E|X_n-X|^p}{\epsilon^p} \to 0 $$
             $(a.s \Rightarrow P), O = \{\omega : X_n(\omega) \not\to X(\omega)\}, P(O) = 0$
             $$ \varepsilon > 0, A_n = \bigcup_{m\ge n} \{\omega: |X_m(\omega)- X(\omega)|\ge \varepsilon\}$$
             $$ A_{n+1} \subset A_{n}, A = \bigcap A_n  \Rightarrow P(A_n) \to P(A)$$
             $$ \omega \in \overline{O} \Rightarrow \omega \not\in A_n \Rightarrow \omega \in \overline{A} \Rightarrow \overline{O} \subset \overline{A} \Rightarrow A \subset O $$
             $$ P(|X_n - X| \ge \varepsilon) \le P(A_n) $$
             
        \end{proof}
    \end{theorem}
    \begin{remark}
        Никакие другие импликации не верны. 
    \end{remark}
    \begin{example}
        $(d \not\Rightarrow P)$\\
        Возьмем $ P(\omega_1) = P(\omega_2) = \frac{1}{2}$, $X(\omega_i) =(-1)^i $, $X_n = (-1)^{i+n}$  
        $$ E(X) = \frac{f(X(\omega_1)) + f(X(\omega_2))}{2} = \frac{f(1) + f(-1)}{2} = Ef(X_n)$$
        $$ P(|X_n(w_i) - X(w_i)| > \varepsilon) \not\to 0$$
    \end{example}
    \begin{example}
        $(a.s \not\Rightarrow L_p), (P\not\Rightarrow L_p)$\\
        Возьмем пространство: $([0, 1], \B, U[0, 1]), X(w) =0, X_n(w) = \begin{cases}
            e^n & w \in [0, \frac{1}{n}] \\
            0 & \text{otherwise}
        \end{cases}$
        $$ X_n \xrightarrow{a.s.} X; E|X_n - X|^2 = \frac{e^{np}}{n} \not\to_{p > 0} 0$$
    \end{example}
    \begin{example}
        $(P \not\Rightarrow a.s.)$\\
        $ X_{2^n}(w) = \begin{cases}
            \frac{1}{2^n} & w \in w\in [0, \frac{1}{2^n}]\\
            0 & \text{otherwise}
        \end{cases}$ \\ 
        $m, \exists n, 2^n < m < 2^{n+1}, m = 2^n + k \Rightarrow X_{m} = \begin{cases}
            \frac{1}{2^n} & w \in w\in [\frac{k}{2^n}, \frac{k+1}{2^n}]\\
            0 & \text{otherwise}
        \end{cases} $
        $$ P(|X_m(w) -X(w)| > \varepsilon) \to 0$$
    \end{example}
    \begin{remark}
        Если в предыдущий пример подставить вместо $2^n$ что-то побольше, например, $n!$, то докажется ещё и $(L_p \not\Rightarrow a.s.)$\\
    \end{remark}
    \begin{definition}
        $F, F_n \text{-- функции распределения}$
        $$F_n(X) \rightharpoonup F(x) \Leftrightarrow F_n(x) \to F(x) \forall x \in C(F)$$
    \end{definition}
    \begin{theorem}
        $$X_n \xrightarrow{d} X \Leftrightarrow F_n \rightharpoonup F $$
        \begin{proof}
            $(\Leftarrow)$\\
            $x_0 \in C(F): f_\varepsilon(t) = \begin{cases}
                1, t \le x_0\\
                \text{линейно}\\
                0, t \ge x_0 + \varepsilon
            \end{cases} $\\
            $$F_n(x_0) = \int\limits_{-\infty}^{x_0} dF_n(x) = \int\limits_{-\infty}^{x_0}f_\varepsilon(t) dF_n(x) \le \underbrace{\int\limits_{-\infty}^{+\infty} f_\varepsilon(t) dF_n(x)}_{Ef_\varepsilon(X_n)} \to \underbrace{\int\limits_{-\infty}^{x_0 + \varepsilon} f_\varepsilon(t) dF_n(x)}_{Ef_\varepsilon(X)} \le F(x_0+ \varepsilon) $$
            $(\Rightarrow) f_\varepsilon^*(t) = f_\varepsilon(t+\varepsilon$
        \end{proof}
    \end{theorem}
    \begin{remark}
        $F_n(x) - F_n(y)  \rightharpoonup F(x) - F(y), \forall x, y \ in C(F)$
    \end{remark}
    \begin{remark}
        $F\in C(\mathbb{R}): \sum\limits_{x\in\mathbb{R}} |F_n(x) -F(x)| \to 0$
    \end{remark}
    \begin{remark}
        $F_n, F $ -- диск, ${x_0, x_1 \dots x_k}$ --  носитель, тогда:
        $$ \underbrace{p_{n, k}}_{P(X_n=x_k)} \to \underbrace{p_k}_{P(X=x_k)}$$
    \end{remark}
    \begin{theorem}{Свойства сходимости}
        \begin{enumerate}
            \item $x_n \xrightarrow{a.s.} x: |x_n| \le^{a.e.} Y, EY < \infty \Rightarrow EX_n \to EX$ 
            \item $X_n \xrightarrow{P} x, f\in C(\mathbb{R}) \Rightarrow f(X_n) \xrightarrow{P} f(X)$
            \item $X_n - Y_n \xrightarrow{P} 0, \begin{cases}
                X_n \xrightarrow{P} X &\Rightarrow Y_n \xrightarrow{P} Y\\
                Y_n \xrightarrow{P} Y &\Rightarrow X_n \xrightarrow{P} X\\
            \end{cases}
            $
            \item $X_n \xrightarrow{d} X, Y_n \xrightarrow{P} 0 \Rightarrow X_n Y_n \xrightarrow{P} 0$
            \item $X_n \xrightarrow{d} C \Leftrightarrow X_n \xrightarrow{P} C$
            \item $X_n \xrightarrow{d} C, Y_n \xrightarrow{P} c \Leftrightarrow X_n + Y_n \xrightarrow{d} X + c, X_n + Y_n \xrightarrow{d} cX, \frac{X_n}{Y_n} \xrightarrow{d} \frac{X}{c} (c \ne 0)$
            \item Арифметические операции сохраняют сходимость по вероятности.
        \end{enumerate}
        \begin{proof}
            Только идеи.\\
            $(1)$ по сути теорема Лебега о мажорированной сходимости\\
            $(2)$ доказывается с помощью определний сходимости по вероятности и непрерывности \\
            \dots
        \end{proof}
    \end{theorem}
    \subsubsection{Сходимости по множеству функций распределения}
    \begin{definition}
        $\mathbf{F}$ -- множество функций распределений. \\
        $\mathbf{G}$ -- множество функций, монотонно возрастающих, непр справа и $G(+\infty) \le 1, G(-\infty) \ge 0$\\
    \end{definition}
    \begin{theorem}{Хейли}
        $$\{G_n\} \in \mathbf{G} \Rightarrow \exists G_{n_k}: G_{n_k} \rightharpoonup G\in \mathbf{G}$$
    \end{theorem}
    \begin{corollary}
        $\forall G_{n_k} \rightharpoonup G \Rightarrow G_n \rightharpoonup G$
    \end{corollary}
    \begin{example}
        $$ F_n(T) = \begin{cases}
            0 & t \le -n\\
            \frac{1}{2} & -n \le t < n \\
            1 & t \ge n
        \end{cases}$$
        $$ F_n(t) \underset{n\to\infty}{\longrightarrow} \frac{1}{2}$$
    \end{example}
    \begin{definition}{плотное распределение}
        $$
        \forall \varepsilon > 0, \exists N:  \inf_n P(-N \le X_n \le N) > 1 - \varepsilon 
        $$
    \end{definition}
    \begin{definition}{Класс определяющий распределение}\\
        $\mathbf{L}$ -- какое-то подмножество непрерыных и ограниченных функций\\
        $\mathbf{L}$ определяет распределение $\Leftrightarrow F\in\mathbf{F}, G\in\mathbf{G}, \forall f\in \mathbf{L}: \int fdG = \int fdF \Rightarrow G=F $
    \end{definition}
    \begin{theorem}
        $\{F_n\}_{n=1}^\infty, F_n \in \mathbf{F}, \mathbf{L}$ -- опр распределение. Тогда:
        $$\exists \lim F_n = F \in \mathbf{F} \Leftrightarrow \begin{cases}
            \{F_n\}_{n=1}^\infty \text{ плотно}\\
            \exists \lim \int fdF_n \forall f \in \mathbf{L}
        \end{cases}$$
        \begin{proof}
            $(\Leftarrow)$ очевидно \\
            $(\Rightarrow) \exists F_{n_k} \rightharpoonup F_1 \in \mathbf{G}$
            $$ \varepsilon > 0, \exists N: F_n(N) > \underbrace{F_n(N)}_{1 \ge F_n(N) \ge 1 - \varepsilon} - \underbrace{F_n(-N)}_{F_n(-N) < \varepsilon} > 1 - \varepsilon, $$
            $x_0$ -- точка непрерывности. $F_1, x_0 \ge N: 1 \ge F_n(N) \ge 1 - \varepsilon$. \\
            $x_1 \le -N: F_n(x_1) < \varepsilon \Rightarrow (F_1(+\infty) = 1), (F_1(-\infty) = 0), 1 \ge F_1(N) \ge 1 - \varepsilon (F_1(x_1) < \varepsilon \Rightarrow F_1 \in \mathbf{F} $ \\
            $\lim \int fdF_{n_k}=\int fdF_1 = \int fdF_2 \Rightarrow F_1 = F_2 $ -- любая подпоследовательность, сходится к одному распределению $\Rightarrow \exists \lim F_n = F$ 
        \end{proof}
    \end{theorem}
    \begin{corollary}
        $\mathbf{L}$ -- опр распределение, $\int fdF_n \to \int fdF, $ $\forall f \in \mathbf{L}, F \in \mathbf{G}$\\
        Пусть верно одно из следующих: 
        \begin{enumerate}
            \item $ \{F_n\}_{n=1}^\infty \text{ плотно}$
            \item $F \in \mathbf{F}$
            \item $f(x) \equiv 1 \in \mathbf{L}$
        \end{enumerate}
        Тогда: $F\in \mathbf{F}, F_n \rightharpoonup F$
        \begin{enumerate}
            \item $C^{(k)}(\mathbb{R})$
            \item $C_0^{(k)} = \{f: \mathbb{R} \to \mathbb{R}, f \in C^{(k)}(\mathbb R), \supp f \text{-- компакт} \}$ \\
             $f \in C^{(k)}(\mathbb R): \exists f_n \in C_0^{(k)}: f_n(x) \to f(x)$\\
            $\int f_n df \to \int f dF \Rightarrow F=G$\\
            $\int f_n'' dG \to \int f''dG$
        \end{enumerate}
        
        \begin{proof}
             $F_n(x) \rightharpoonup F(x), x \in C(F)$\\
             $\int f dF_n \underbrace{=}_{\text{из инт по частям}} - \int F_n f' dx \underbrace{\longrightarrow}_{\text{тк А непр пв, то сход пв}} -\int F f' dx = \int fdF  $\\
             $3) \{e^{itx}\}_{t\in \mathbb R}$
        \end{proof}
    \end{corollary}
    \section{Характеристические функции}
    \subsection{Определение}
    \begin{definition}
        $X$ - с.в. $f(t) = Ee^{itX} = Ecos(tX) + i E sin(tX)$ -- \textbf{характеристическая функция случайной величины}, $f_X(t) = \int e^{itx} dF(x)$
    \end{definition}
    \subsection{Свойства}
    \begin{property}
        $f(0) = 1$
    \end{property}
    \begin{property}
        $|f(t)| \le E|e^{itx}| = 1$
    \end{property}
    \begin{property}
        $Y = aX + b \Rightarrow f_Y(t) = Ee^{itaX + b} = e^{itb} \cdot f_x(at)$
    \end{property}
    \begin{property}
        $U = -X \Rightarrow f_U(t) = Ee^{-itX} = \overline{f_X(t)}$ -- комплексное сопряжение
    \end{property}
    \begin{property}
        $X, Y$ -- нез $\Rightarrow f_{X+Y}(t) = Ee^{itX} \cdot Ee^{itY} = f_X(t) \cdot f_Y(t)$
    \end{property}
    \begin{property}
        $f$ -- равномерно непрерывны на $\mathbb{R}:$ 
        $$| f(t+h) - f(t)| =  |Ee^{itX + ihX} - Ee^{it}| \le E|e^{ihX} - 1| \xrightarrow{h \to 0} 0 $$
        $\Rightarrow \underset{t \in \mathbb R}{sup} | f(t+h) - f(t)| $ определен равномерно-непрерывно \\ 
        $\Rightarrow e^{ihx} \to 1, |e^{ihx} - 1| \le 2$
    \end{property}
    \begin{property}
        $\exists EX^k \Rightarrow \exists f^{(k)}(c) $
    \end{property}
    \begin{property}
        $f(t) = Ee^{itX}, f'(t) = EiXe^{itX}$
    \end{property}
    \begin{property}
        $f'0 = iEX$
    \end{property}
    \begin{property}
        $\forall t_1 \dots t_m \in \mathbb R, \lambda_1 \dots \lambda_m \in \mathbb C: \sum\limits_{k, j} f(t_k - t_j) \lambda_k \overline{\lambda_j} \ge 0$
    \end{property}
    \begin{theorem}{Бохнера-Хинчина}
        $$ P_X \text{-- симметрична} \Leftrightarrow f(t) \in \mathbb R, \forall t \in \mathbb R$$
        \begin{proof}
            $(\Rightarrow) f(t) = E\cos(tX) + i \underbrace{E \sin(tX)}_{=0}$\\
            $(\Leftarrow) f_X(t) = \overline{f_X(t)} = f_{-X}(t) \Rightarrow Y=-X, \int e^{itX}dF_X = \int e^{itY} dF_y \Rightarrow F_X = F_Y $
        \end{proof}
    \end{theorem}
    \begin{definition}{Решетчатые св}\\
        $X$ -- решетчатая $\Leftrightarrow P(X = a + kh) = 1, k \in \mathbb Z$. макс h - шаг решетки
    \end{definition}
    \begin{claim}
        $X \text{-- реш}  \Leftrightarrow | f(\frac{2\pi h}{n} | = 1, k \in \mathbb{Z}, h \text{- шаг решетки}$
    \end{claim}
    \subsection{Примеры}
    \begin{example}
        $I_c: f(t) = e^{itc}$
    \end{example}
    \begin{example}
        $Bern(p): f(t) = p + qe^{it}$
    \end{example}
    \begin{example}
        $Bin(n, p): f(t) = (p + qe^{it})^n$ % ?
    \end{example}
    \begin{example}
        $Pois(\lambda): $
        $$f(t) = \sum\limits_{k=0}^\infty e^{itk} e^{-\lambda} \frac{\lambda^k}{k!} = e^{-\lambda} \sum \frac{(e^{it}\lambda)^k}{k!} = e^{-\lambda}e^{\lambda e^{it}} = e^{\lambda(e^{it} - 1)}$$
    \end{example}
    \begin{example}
        $U[0, 1]: f(t) = \int\limits_0^1 e^{itx}dx = \frac{e^{it} - 1}{it}$
    \end{example}
     \begin{example}
        $U[-1, 1]: f(t) = \frac{1}{2} \cdot \frac{e^{it} - e^{-it}}{it} = \frac{\sin{t}}{t}$
    \end{example}
    \begin{example}
        $N(0, 1):$
        \begin{align*}
            f(t) &= \frac{1} {\sqrt{2\pi}} \int\limits_{-\infty}^{+\infty} e^{itx - \frac{x^2}{2}} dx\\
            f'(t) &= \frac{1} {\sqrt{2\pi}}  \int\limits_{-\infty}^{+\infty} ixe^{itx - \frac{x^2}{2}}dx = 
            -\frac{1}{\sqrt{2\pi}}e^{itx - \frac{x^2}{2}}\bigg|_{-\infty}^{+\infty} - t\underbrace{\frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^{+\infty} e^{itx - \frac{x^2}{2}}dx}_{f_t} = -f(t) \cdot t \\
            &\Rightarrow f(t) = e^{\frac{-t^2}{2}}
        \end{align*}
    \end{example}
    \begin{example}
        $N(\mu, \sigma^2): Y = \sigma X + \mu \Rightarrow f_Y(t) = e^{it\mu} \cdot e^{-\frac{\sigma^2t^2}{2}}$
    \end{example}
    \begin{example}
        $Exp(\lambda): f(t) = \int\limits_0^{+\infty} \lambda e^{itx - \lambda x} dx = \frac{\lambda}{it - \lambda} e^{itx - \lambda x}\bigg|_0^{+\infty} = \frac{-\lambda}{it - \lambda}$
    \end{example}
    \begin{example}
        $Г(n, \lambda): X_1 \dots X_n, X_i \sim Exp(\lambda), S_n = \sum X_i, f_{S_n}(t) = (\frac{-\lambda}{it - \lambda})^n $
    \end{example}
    \subsubsection{Многомерные}
    \begin{definition}
        $X $- с.в. $f_X(t): \mathbb{R}^n \to \mathbb C, f_X(t) = Ee^{i\langle t, X\rangle}$ - хар фун для с.в.
    \end{definition}
    \begin{example}
        $N(\mu, \Sigma): f_X(t) =\exp(i\langle t, \mu\rangle - \frac{1}{2} t^T \Sigma t)$
        \begin{proof}
            \textit{(С большой вероятностью это дичь, тк отсебятина).} Каждая из компонент стандартного гауссовского вектора независима и имеет х.ф.  $f(t_i) = \exp\left(-\frac{1}{2}t_i^2\right)$, а х.ф. по сути является матожиданием, но матожидание произведения независимых с.в. является произведением их матожиданий, тогда характеристическая функция стандартного гауссовского вектора размерности $n$ имеет вид:

            $$f(\boldsymbol{t}) = \prod_{i=1}^n \exp\left(-\frac{1}{2}t_i^2\right) = \exp\left(-\frac{1}{2}\boldsymbol{t}^T\boldsymbol{t}\right)$$
            
            где $\boldsymbol{t} = (t_1, t_2, ..., t_n)^T$ - вектор переменных.

            Ну а дальше просто воспользуемся свойствами х.ф. (\textit{откуда у нас свойства многомерной х.ф... :(}):

            $$ N(\mu, \Sigma^2): Y = \Sigma X + \mu \Rightarrow f_Y(t) = e^{i\langle t, \mu \rangle} \cdot e^{-\frac{t^T\Sigma t}{2}} = \exp(i t^T \mu - \frac{1}{2} t^T \Sigma t) $$
            
        \end{proof}
    \end{example}
    \begin{remark}
        $X$ -- многомерное норм распределение $\Leftrightarrow \sum c_k X_k$ распределено нормально или вырождено
        \begin{proof}
            Для доказательства этого свойства воспользуемся характеристической функцией. Пусть $\boldsymbol{X} = (X_1, X_2, ..., X_n)$ - многомерное нормально распределенное случайное вектор со средним $\boldsymbol{\mu}$ и ковариационной матрицей $\boldsymbol{\Sigma}$. Рассмотрим линейную комбинацию компонент вектора $\boldsymbol{X}$:

            $$Y = \sum_{k=1}^{n}c_kX_k$$
            
            Характеристическая функция случайной величины $Y$ выражается следующим образом:
            
            $$\varphi_Y(t) = \mathbb{E}[e^{itY}] = \mathbb{E}\left[e^{it\sum_{k=1}^{n}c_kX_k}\right] = \mathbb{E}\left[\prod_{k=1}^{n}e^{itc_kX_k}\right] = e^{it\boldsymbol{c}^T\boldsymbol{\mu} - \frac{1}{2}t^2\boldsymbol{c}^T\boldsymbol{\Sigma}\boldsymbol{c}}$$
            
            где $\boldsymbol{c} = (c_1, c_2, ..., c_n)$ - вектор коэффициентов линейной комбинации.
            
            Таким образом, характеристическая функция случайной величины $Y$ имеет вид нормальной плотности вероятности с параметрами $\boldsymbol{\mu}_Y = \boldsymbol{c}^T\boldsymbol{\mu}$ и $\boldsymbol{\Sigma}_Y = \boldsymbol{c}^T\boldsymbol{\Sigma}\boldsymbol{c}$. Если $\boldsymbol{\Sigma}_Y$ вырождена, то распределение $Y$ вырождено. В противном случае, $Y$ имеет многомерное нормальное распределение.
        \end{proof}
    \end{remark}
    \subsection{Формула обращения}
    \begin{theorem}{Формула обращения}
    $f$- х.ф. $X \Rightarrow$ 
    $$\forall y > x \in C(F): F(y) - F(x) = \frac{1}{2\pi}\lim_{\sigma \to 0}\int\limits_{-\infty}^{+\infty} \frac{e^{-itx} - e^{-ity}}{it} \cdot f(t) \cdot e^{-\frac{\sigma^2t^2}{2}} dt  $$
    Более того, если $\frac{f(t)}{t} $ -- интегрируема, можно внести предел под интеграл.
    \begin{remark}
        По другому: $F(y) - F(x) = \frac{1}{2\pi}\lim_{A \to \infty}\int\limits_{-A}^{+A} \frac{e^{-itx} - e^{-ity}}{it} \cdot f(t) dt $
    \end{remark}
    \begin{proof}
        $f(t) = \int\limits_\mathbb{R} e^{itx} p(x) dx$,\\ $p(z) = \frac{1}{2\pi} \int\limits_\mathbb{R} e^{-itz} f(t) dt$ \\$  \Rightarrow F(y) - F(x) = \frac{1}{2\pi} \int\limits_\mathbb{R}\frac{e^{-itz}|_x^y}{-it} f(t) dt $ \\
        Так через пару действий мы придем к тому, что нужно для X - непр. \\ 
        $U_\delta = X+ Y_\delta, X, Y_\delta$ -- нез $Y_\delta \sim N(\mu, \delta^2) \Rightarrow U_\delta$ -- непр \\ 
        $P(U_\delta \le t) = \iint_{X+Y \le t} dP(dx, dy) = \int\limits_{\mathbb{R}} dF_{Y_\delta}(y) \int\limits_{-\infty}^{t-y} dF_X(x) = \int\limits_{\mathbb{R}} \underbrace{F_X(t-y) p_\delta(y)}_{p_{U_\delta}} dy$ \\ 
        Тогда $F_{U_\delta} (y) - F_{U_\delta} (x) = \frac{1}{2\pi} \int\limits_{-\infty}^{+\infty} \frac{e^{it} - e^{-it}}{it} \cdot f(t) \cdot e^{-\frac{\sigma^2t^2}{2}} dt \text{-- тк}  т.к X \xrightarrow{d} X \Rightarrow U_\delta \xrightarrow{d} X, Y_\delta \xrightarrow{P} 0$ \\ 
        То есть мы сделали предельный переход по $\delta$ (в целом произвели "сглаживание" $X$ при помощи $Y_\delta$). 
    \end{proof}
    \end{theorem}
    \begin{theorem}{Леви}\\
    $X_n, X$ -- с.в.\\
    $f_n, f$ -- х.ф. \\
    $$ X_n \xrightarrow{d} X \Leftrightarrow f_n(t) \to f(t) \forall t $$
    \begin{proof}
        Напрямую следует из того, что обсуждалось в начале прошлой лекции.
    \end{proof}
    \end{theorem}

   \begin{lemma}
        $P(|x| > \frac{2}{U}) \le \frac{1}{U} \int\limits_{-U}^{U} (1 -f(t)) dt$, где $X$ -- св, $f$ -- хф $X$.
        \begin{proof}
            \begin{align*}        
            \frac{1}{U} \int\limits_{-U}^{U} (1 -f(t)) dt &= \frac{1}{U} \int\limits_{-U}^{U}\int\limits_{-\infty}^{+\infty} (1 -e^{itx}) dF(x) dt = \frac{1}{U} \int\limits_{-\infty}^{+\infty} \int\limits_{-U}^{U} (1 -e^{itx}) dt dF(x)\\ &=
            \frac{1}{U} \int\limits_{-\infty}^{+\infty} \left(2U - \frac{e^{iUx} - e^{-iUx}}{ix}\right) dF(x) = 2\int\limits_{-\infty}^{+\infty} (1 - \frac{\sin{Ux}}{Ux}) dF(x) \\ &\ge
            2 \cdot \int\limits_{|x| > \frac{2}{U}} \left(1 - \frac{|\sin{Ux}|}{|Ux|}\right) dF(x) \ge 2 \cdot \int\limits_{|x| > \frac{2}{U}} \frac{1}{2} dF(x) = P\left(|x| > \frac{2}{U}\right) 
            \end{align*}
        \end{proof}
    \end{lemma}
    \begin{theorem}
    $(X_n)_{n=1}^\infty$ -- с.в, $(f_n)_{n=1}^\infty$ и $ f_n(t) \to f(t), \forall t \in \mathbb{R} $ (неизвестно, является ли $f$ х.ф.!) $\Rightarrow $ (условия равносильны)
    \begin{enumerate}
        \item $f$ -- х.ф $X$
        \item $f(0) = 1 $ и $f$ непр в 0
        \item $(F_n)$ -- плотная
    \end{enumerate}
    \begin{proof} $(1 \Rightarrow 2)$ очевидно \\ 
    $1 \Leftrightarrow 3$ вытекает из доказательств раньше \\ 
    $2 \Rightarrow 3$ \\
    Воспользуемся леммой, и теоремой о среднем, тогда для произвольного $U$:
    $$ \lim_{n\to\infty} \int\limits_{|x|> \frac{2}{U}} dF_n(x) \le \lim_{n\to\infty} \frac{1}{U} \int\limits_{-U}^I (1-f_n(t_{Un})) dt =  \frac{1}{U} \int\limits_{-U}^I (1-f_n(t_U)) dt = 2(1 -f(t_U)) < \varepsilon $$
    \end{proof}
    \subsection{Слабый ЗБЧ, ЦПТ}
    \end{theorem}
        \begin{claim}{Слабый закон больших чисел}
        $$\{X_i\}_{i=1}^\infty -- i.i.d, EX_1 = \mu, S_n = \sum\limits_{i=1}^n X_i \Rightarrow \frac{S_n}{n} \xrightarrow{P} \mu$$
        \begin{proof}
           $$ \frac{S_n}{n} \xrightarrow{P} \mu \xleftrightarrow{\text{для вырожденных}} \frac{S_n}{n} \xrightarrow{d} \mu \Leftrightarrow f_{\frac{S_n}{n} }(t) \to e^{it\mu} \Leftrightarrow f_{\frac{S_n}{n} - \mu} (t) \to 1 = e^{it0}$$
           $$f_{x_0} = 1 + i\mu t + o(t), t \to 0 \text{   (Тейлор)}$$
           $$f_{S_n} = (1 + i\mu t + o(t))^n, t \to 0 $$
           $$f_{\frac{S_n}{n} }(t) = f_{S_n} (\frac{t}{n}) = (1 + i\mu \frac{t}{n} + o(\frac{t}{n}))^n \xrightarrow{n \to \infty} e^{it\mu}$$
        \end{proof}
    \end{claim}
    \begin{theorem}{ЦПТ для i.i.d с.в.}\\
    $\{X_i\}_{i=1}^\infty -- i.i.d, EX_i = \mu, \Var X_i = \sigma^2 > 0 \Rightarrow$
    $$
    \frac{S_n - n\mu}{\sqrt{n\sigma^2}} \xrightarrow{d} U \sim N(0, 1) \Leftrightarrow 
    \sup_{x \in\mathbb R} |F_n(x) - \Phi (X)| \xrightarrow{n \to\infty} 0,  (F_n(x) = P(\frac{S_n - n\mu}{\sqrt{n\sigma^2}} \le x))
    $$
    \begin{proof}
        $U_i = \frac{X_i - \mu}{\sigma}, EU_i = 0, \Var U_i = 1, f'(0)=0$
        $$\frac{\sum U_i}{\sqrt{n}} = \frac{S_n - n\mu}{\sqrt{n\sigma^2}}$$
        $$f_{U_i}(t) = 1 - \frac{t^2}{2} + o(t^2) \Rightarrow f_{\frac{\sum U_i}{\sqrt{n}}} (t) = (1 - \frac{t^2}{2n} + o(\frac{t^2}{n}))^n \to e^{-\frac{t^2}{2}}$$ 
    \end{proof}
    \end{theorem}
    \subsection{Ещё свойства}
    \begin{claim}
        $X$ -- целочисленная $\Rightarrow f $-- $2\pi$-периодическая функция        
    \end{claim}
    \begin{claim}
        Рассмотрим промежуток $[-\pi, \pi]: \{e^{ith}\}_{h\in\mathbb Z}$ -- полное и ортогональное семейство функций.
        $$\langle f, g \rangle = \int\limits_{-\pi}^{\pi} f(x) \overline{g}(x) dx$$
        $$ \sum\limits_k p_k e^{itk} = \sum\limits_k c_k e^{itk} \Rightarrow \langle f, e^{ith}\rangle = c_j \langle e^{itj}, e^{itj}\rangle = c_j\cdot 2\pi $$
        $$\Rightarrow c_j = \frac{1}{2\pi} \langle f, e^{ith}\rangle = \frac{1}{2\pi}\int\limits_{-\pi}^{\pi} f(t) e^{ith}dt = P(X=j)$$
    \end{claim}
    \subsubsection{Оценка погрещности в теорему Пуассона}
    \begin{lemma}
        $\mathbb{Re} U < 0 \Rightarrow \begin{cases}
            |e^U - 1| \le |U|\\
            |e^U - U - 1| \le \frac{|U|^2}{2}
        \end{cases}$
        \begin{proof}
            $$ |e^U - 1| = |\int\limits_0^Ue^tdt| \overset{t=Uv}{=} |\int\limits_0^1 U e^{Uv}dv| \le |U| \int\limits_0^1 |e^{Uv}| dv \le |U|$$
            $$ |e^U - U - 1| = |\int\limits_0^U(e^t - 1) dt| = \dots \le \frac{|U|^2}{2}$$
        \end{proof}
    \end{lemma}
    \begin{lemma}
        $a_k, b_k, |a_k|, |b_k| \le 1 \Rightarrow |\overbrace{\prod\limits_{k=1}^n a_k}^{A_n} - \overbrace{\prod\limits_{k=1}^n b_k}^{B_n}| \le \sum\limits_{k=1}^n|a_k-b_k|$
        \begin{proof}
            $$|A_{n - 1} a_n - B_{n-1} b_n| = |A_{n-1} a_n - A_{n-1} b_n + A_{n-1} b_n - B_{n-1} b_n| \le $$
            $$\le |A_{n-1}| \cdot |a_n - b_n | + |b_n|\cdot|A_{n-1} - B_{n-1}| \le |a_n - b_n| + |A_{n-1} - B_{n-1}|$$
        \end{proof}
    \end{lemma}
    \begin{theorem}{Об оценке погрешности в теореме Пуассона}\\
    $\{X_i\}_{i=1}^\infty$ $X_i$-- целочисленны, независимы. $p_i = P(X_i=1), 1-q_i-p_i = P(X_i=0) \Rightarrow q_i = P(X_i \not\in \{0, 1\})$
    $$S_n = \sum\limits_{i=1}^n X_i, \lambda = \sum\limits_{i = 1}^n p_i (n \text{- фикс}) \Rightarrow |P(S_n = k) - e^\lambda \frac{\lambda^k}{k!}| \le\sum\limits_{i = 1}^n p_i^2 + 2 \sum\limits_{i = 1}^n q_i$$
    \begin{proof} % я переделаю чучуть можно да..
    $$f_{X_i} = (1-p_i-q_i) + p_i e^{it} +q_i \gamma_i(t),  \gamma_i(t) \text{ -- некоторая х.ф.}  = 1 + p_i (e^{it} -1) + q_i (\gamma_i(t) - 1) $$
    $$f_{S_n} = \prod_{i=1}^n f_{X_i}$$
    $$\psi_i(t) = e^{p_i (e^{it} -1)}$$
    $$\varphi_i(t)=\prod_{i=1}^n \psi_i(t) = e^{\sum p_i (e^{it} -1)} \sim Pois(\lambda)$$
    % $\gamma_i \text{- х.ф}, f_{S_n} (t) = \prod\limits_{i=1}^n f_i(t), \psi_i(t) = e^{p_i(e^{it} - 1)}, \varphi_n = \prod\limits_{i=1}^n \psi_i(t) = e^{(\sum p_i)(e^{it} - 1)} \sim Pois(\lambda) $
    % $$ f_i(t) = (1 - q_i - p_i) + p_i e^{it} + q_i \gamma_i(t) =1 + p_i (e^{it} -1) + q_i (\gamma_i(t) - 1) $$ 
    $$ |f_i(t) - \psi_i(t)| = (1+ p_i(e^{it} - 1)) - e^{p_i(e^{it} - 1)} + q_i (\gamma_i(t) - 1) \underset{\text{по лемме 2}}{\le} p_i^2 | e^{it} - 1|^2 + 2q_i$$
    $$ |e^{it} -1|^2  = (e^it - 1) (e^{-it} -1) = (1 - e^{it} - e^{-it} + 1) = 1 - cos(t) $$
    $$ \frac{1}{2\pi} \int\limits_{-\pi}^\pi  (\frac{p_i^2}{2} 
    |e^{it} - 1|^2 + 2 q_i)dt = 2q_i + \frac{p_i^2}{4\pi} \int\limits_{-\pi}^\pi |e^{it} -1|^2 dt = 2q_i + \frac{p_i^2}{2\pi}\int\limits_{-\pi}^\pi (1 - cos(t)) dt = 2q_i + p_i^2 $$
    Тогда:
    \begin{align*}     
    |P_(S_n = k) -e^{-\lambda} \frac{\lambda^k}{k!}| &= \frac{1}{2\pi} |\int\limits_{-\pi}^\pi (f_{S_n} (t) e^{-itk} - \varphi_n (t) e^{-itk}) dt| \le \frac{1}{2\pi} |\int\limits_{-\pi}^\pi (\prod_{i=1}^n f_{i} (t) - \prod_{i=1}^n \psi_{i} (t)) dt| \\
    &\le \frac{1}{2\pi} \int\limits_{-\pi}^\pi \sum_{i=1}^n |f_i(t) -\psi_i(t)| dt \le \frac{1}{2\pi }  \sum_{i=1}^n\int\limits_{-\pi}^\pi |f_i-\psi_i|dt \le \sum_{i=1}^n p_i^2 + 2 \sum_{i=1}^n q_i
    \end{align*} 
    \end{proof}
    \end{theorem}
    \begin{corollary}{Оценка Пуассона}\\
        $$X = np, p_{i,n} = \frac{\lambda}{n}, q_i=0 \Rightarrow |P(S_n=k) - e^{-np}\frac{(np)^2}{k!}| \le \sum_{k=1}^n \frac{\lambda^2}{n^2} = \frac{\lambda^2}{n}$$
   \end{corollary}
   \subsection{Неравенства}
   \begin{theorem}{Неравенство Эссеена} \\
   $F, G$ -- функции распределения, $f, g$ -- х.ф

   $$\sup_{x\in\mathbb R} |G'(x)| \le M \Rightarrow \forall T > 0: \sup_{x \in\mathbb R} |F(x) - G(x)| \le \frac{2}{\pi} \int\limits_0^T \frac{|f(t)| - g(t)|}{|t|} dt + \frac{24}{\pi T} \sup_{} |G'(x)|$$
   \begin{proof}
       Без доказательства.
   \end{proof}
   \end{theorem}
   \begin{theorem}{Неравенство Берри-Эссеена}
   
       $(X_i)_{i=1}^\infty$ -- i.i.d. $S_n=\frac{\sum_{i=1}^n X_i - nX_1}{\sqrt{n \Var X_1}}$, $F_n(t) = P(S_n \le t)$, $E|X_1 - EX_1|^3 = \beta_3$, $\sigma = \sqrt{\Var X_1}$ 
       
       Тогда:

       $$ \sup_{x\in\mathbb R} |F_n(x) - \Phi(X)| \le \frac{c \beta_3}{\sigma^3 \sqrt{n}}$$
       \begin{proof}

            Н.У.О $(\frac{X - EX}{\sigma}): EX_1 = 0, \Var X_1 = 1$ $\Rightarrow \beta_3 = E|X_1|^3$

            И нам нужно проверить $ \sup_{x\in\mathbb R} |F_n(x) - \Phi(X)| \le \frac{c \beta_3}{\sigma^3 \sqrt{n}}$
            
            $f_{S_n}(t) = f_{X_1}^n (\frac{t}{\sqrt{n}}) $
       
            Возьмем $T = \frac{\sqrt{n}}{5\beta_3}$?

            Посчитаем х.ф: $f_{X_1} (t) = 1 - \frac{t^2}{2} + \frac{(it)^3}{6} EX_1^3 (\cos(tX\theta_1 + i sin(tX\theta_2))), |\theta_1|, |\theta_2| \le 1$

            $f_{X_1} (\frac{t}{\sqrt{n}}) = 1 - \frac{t^2}{2n} + \frac{(it)^3}{n^{\frac{3}{2}}6} EX_1^3 (\cos(\frac{t}{\sqrt{n}}X\theta_1 + i sin(\frac{t}{\sqrt{n}}X\theta_2)))$

            ценим при $t< |T|:$
           $$1 - |f_{X_1} (\frac{t}{\sqrt{n}}| \le |1 - f_{X_1} (\frac{t}{\sqrt{n}})| = | \frac{t^2}{2n} + \frac{(it)^3}{n^{3/2}6} EX_1^3 (\cos(\frac{t}{\sqrt{n}}X\theta + i sin(\frac{t}{\sqrt{n}}X\theta)))| \overset{|\cos t|, |\sin t| \le 1}{\le} |\frac{t^2}{2n} + \frac{t^3\beta_3}{3 n^{3/2}}| \le \frac{1}{25}$$

            Таким образом у нас получилось, что х.ф. отделима от нуля $|f_{X_1}(\frac{t}{\sqrt{n}})|\ge \frac{24}{25}$

            $f_{S_n} (t) = f_{X_1}^n (\frac{t}{\sqrt{n}}) = \exp(n \ln f_{X_1} (\frac{t}{\sqrt{n}}))$

            \begin{remark}
                $$f(t) = 1 = iEX\cdot t - \frac{t^2}{2} EX^2 - \frac{it^3}{6} EX^3 + \dots$$
                $$ ln f(t) = S_1it + \frac{S_2 (it)^2}{2} + \frac{S_3(it)^3}{6}$$
                $S_k$ называются семинвариантами, причем $S_1 = EX, S_2 = \sigma^2$
            \end{remark}
            $$ ln f_{X_1}(\frac{t}{\sqrt{n}}) = - \frac{t^2}{2n} + \frac{(it)^3}{6n^{3/2}}(ln f)''' (\theta \frac{t}{\sqrt{n}})$$
            $$ ln'''f(s) = \frac{f'''f^2 - 3f''f' + 2(f')^3}{f^3} = \frac{E(iX_i)^3 e^{iX_i s}f^2 - 3E(iX_i)^2e^{iX_is} - 3EX_i e^{iX_is} + 2(EiX_i e^{iX_is})^3}{f^3(s)}$$
            $$ \beta_1 \le \beta_2^{1/2} \le \beta_3^{1/3} \text{(по н-ву Ляпунова)}$$
            $$|f_{X_1}(\frac{t}{\sqrt{n}})|\ge \frac{24}{25}, |t| < \frac{sqrt{n}}{5\beta_3}, |f(s)| \le 1 \Leftarrow |ln'''(\theta \frac{t}{\sqrt{n}})| \le \frac{\beta_3 + 3 \beta_1\beta_2 + 2\beta_1^3}{(\frac{24}{25})^3} \le 7 \beta_3$$
            \begin{remark}
                Имеет место неравенство: $|e^z - 1|\le|z|e^{|z|}$
            \end{remark}
            Оценим:
             \begin{align*}
             \left|f_{X_1}^n (\frac{t}{\sqrt{n}})) -e ^{-t^2/2}\right| &= \left|e^{n lnf_{X_1}(\frac{t}{\sqrt{n}})} -e ^{-t^2/2}\right|\\ 
             &= \left|e ^{-t^2/2} - e^{- \frac{t^2}{2n} + \frac{(it)^3}{6n^{3/2}}(ln f)''' (\theta \frac{t}{\sqrt{n}})}\right|\\ 
             &\le e^{-t^2/2}\left|\frac{(it)^3}{6\sqrt{n}} (ln f''')(\theta \frac{t}{\sqrt{n}})\right| \exp\left(\left|\frac{t^3}{6\sqrt{n}} lnf'''(\theta \frac{t}{\sqrt{n}})\right|\right)\\ 
             &\le e^{\frac{-t^2}{2}} \frac{t^3}{6\sqrt{n}}7\beta_3 \exp\left(\frac{t^3}{6\sqrt{n}}\right) (ln f''')\\ 
             &\le \frac{t}{6} \frac{\beta_3 |t|^3}{\sqrt{n}} e^{\frac{t^2}{4}}
             \end{align*}
            $$
            \left|\int_0^T \frac{|f_{S_n}(t) - \Phi(t)}{|t|}dt\right| \le \frac{7}{6}\frac{\beta_3}{\sqrt{n}} \underbrace{\int_0^T {|t|}^2 e^{-\frac{t^2}{4}}dt}_{\text{заведомо сходится}}
            $$
       \end{proof}
   \end{theorem}
   \begin{corollary}
       Оценка интегральной теоремы Муавра-Лапласа:

       $$\sup_{x\in\mathbb R} \left|F_n(x) - \Phi(x\right| \le c \cdot \frac{p^2+q^2}{\sqrt{pq} \sqrt{n}}$$
       \begin{proof}
           $$E\left|X-EX\right|^3 = E\left|X-p\right|^3 = (1-p)^3p + p^3(1-p_ = pq(p^2+q^2) \Rightarrow C \frac{(p^2 + q^2) pq}{(pq)^{3/2}\sqrt{n}} = C \cdot \frac{p^2 + q^2}{\sqrt{pqn}}$$
       \end{proof}
   \end{corollary}
   \section{Моделирование случайных величин}

   На самом деле, в реальности сгенерировать реально "случайное" число практически невозможно, поэтому в современных языках программирования генерируют "псевдослучайных" чисел. Тут мы рассмотрим, как это будет происходить. 

    Самым простым примером будет \textbf{линейный генератор}, где мы просто берем число, как-то его преобразуем (складываем, умножаем), а потом берем остаток по довольно большому модулю.

    Теперь подумаем как работает ```random.random()``` из Python, которая симулирует равномерное распределение на $[0, 1]$. И как правило, когда мы говорим "возьмем случайное", подразумеваем именно равномерное распределение.

    В C++ всё сложнее и многие распределения уже реализованы как-то. 

    А что в Java? Есть класс Random. Для генерации мы используем фукнцию nextDouble().

    Мораль в чем? Обычно в языках программирования стараются симулировать в первую очередь равномерное распределение (либо дискретное, либо непрерывное).
    
    То есть, обычно $rand() \sim U[0, 1]$

    А теперь мы рассмотрим как генерировать другие распределения.

    Как получить $N(\mu, \sigma)$: ЦПТ, 4 задачка из лабы.

    $F^{-1}(rand()) \sim F$

    То есть как можно поступать? Либо пользоваться предельными соотношениями, либо через обратную функцию (по формуле выше), либо приближенными методами (rejection assembling и тд).

    Что с дискретными распределениями? Можно симулировать их через $U[0, 1]$. Например, $Bern(p)$ можно так сделать `rand() < p ? 1 : 0`. Значит автоматически научились генерировать и $Bin(n, p)$ (просто сгенерим n бернуллевских) или можно даже быстрее как-нибудь.

    Хотим ещё Пуассона $Pois(\lambda)$ симулировать: $\max \{j: \sum_{k=1}^j \ln U_k \le e^\lambda\}$, тогда получится, что $P(X \le k) = \int_k^\infty p_{\Gamma{k, n}} (x) dx$ и если как-нибудь посчитать, то это ровно то, что нужно. 

    \section{Что ещё происходит в теории вероятностей?}
    Курс почти закончился, но его, понятное дело, не хватает, чтобы обсудить всю эту науку. Что же в ней есть ещё? 

    Мы сформулировали слабый ЗБЧ, а есть ещё несколько усиленных ЗБЧ, где утверждается, что сумма сходится к матожиданию почти наверное, а не по вероятности, как в слабом. 

    Одно из достаточных условий: $\lim\limits_{n\to\infty} \frac{\sum \Var X_i}{n} < \infty $ (условие Колмогорова)

    Ещё можно рассматривать предельные теорему не только когда все св одинаковые, но и другие. Там много всего интересного. 

    Например, есть вероятности больших уклонений, например, что-то про $\lim\limits_{\sqrt{n}x_n \to\infty} P(S_n > x_n\sqrt{n})$

    Также можно упомянуть про случайные процессы, которые является ответвлением от классической теории вероятностей. Мы рассматривали случайные величины, а там, грубо говоря, индексированные случайные величины $X_t, t \in T\subset \mathbb R$. Там есть важные классы процессов: Гауссовские и Пуассоновские. 

    Не менее важным разделом является математическая статистика (с ней мы подробнее познакомимся в следующем семестре). Там мы, решаем в каком-то смысле обратную задачу теории вероятностей -- пытаемся по реальным данным понять, что распределение у нас они представляют.

    Что такое слабо-зависимые величины? Может утверждаться $\cov(X_i, X_j) = 0$ или что-то такое. В некоторых других источниках рассказывается именно в таком виде, то есть, с чуть более слабыми ограничениями.
 \end{document}
