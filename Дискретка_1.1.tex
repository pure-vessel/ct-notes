\documentclass{article}
\input{header}
\input{formal-conspectus}

\usepackage{minted}
\usemintedstyle{native}

\usetikzlibrary{circuits.logic.IEC}
\tikzset{
    every node/.style={
        not gate IEC symbol={},
        or gate IEC symbol={$\scriptstyle\lor$},
        and gate IEC symbol={$\scriptstyle\land$},
        xor gate IEC symbol={$\scriptstyle\oplus$}
    }
}
\tikzstyle{branch}=[fill,shape=circle,minimum size=3pt,inner sep=0pt]
\tikzset{treenode/.style={
        inner sep=0pt,
        text width=5mm,
        scale=.8,
        circle,
        outer sep=2mm,
        align=center,
        draw=black,
        text=black,
        fill=white,
        thin
}}

\geometry{legalpaper, paperheight=16383pt, margin=1in}
\setcounter{totalnumber}{100}
\pagestyle{empty}

\begin{document}
    \paragraph{\undercolorblack{orange}{Разная фундаментальная фигня}.}
    \subparagraph{\undercolorblack{orange}{Множества.}}
    \begin{itemize}
        \item[]\mbox{}
        \begin{Comment}
            В самом начале всего есть хрен как определяемые штуки. В нашем случае это множества. Не будем этого делать, как и на математическом анализе.\\
            Дальше есть объекты, принадлежащие множеству ($a\in A$) и не принадлежащие ($a\notin A$).\\
            Обычно удобно выбирать множество вообще всех объектов (универсум $U$), которые у нас есть. Иначе стрёмно.\\
            Но вообще есть парадокс Рассела (см. далее), согласно которому \textit{множества всего} не бывает.\\
            Как записывать множества? Ну, поэлементно ($\{1,2,3\}$, $\{\text a,\text b,\text c,\text d\}$, $\{\}$). Но так только конечные множества. А если хочется рассматривать множество чётных чисел? Ну, хз. Пусть у нас есть универсум $\mathbb Z$. Тогда вот $\{k\mid k\divby2\}$. Тут необязательно писать $k\in\mathbb Z$, ведь есть универсум.\\
            А если $\{k\mid k\in\mathbb Z\land k\divby2\}$, то можно без универсума? Хм-м. Можно ли тогда сделать такое: $\{\varnothing,\{1\},\{1,2,3\},\{\text a,\text b,\text c,\text d\},\mathbb N,\text{множество студентов группы M3131}\}$. Ну, да. А такое? $\{A\mid A\text{ --- конечное множество}\}$. Ну, ок. А может ли множество принадлежать самому себе? Хм-м. $A=\{A\}$. Ну, это что-то странное, но в целом почему нет. Поехали в парадокс Рассела. Рассмотрим $\{X\mid X\notin X\}$. Ну, а эта херь принадлежит сама себе?  Если да, то противоречие, если нет, то тоже. Полная хуйня. А если этого нельзя, то, может, и $A=\{A\}$ --- тоже? Ну, хз, может быть.\\
            Самый простой способ избежать проблем несуществования чего-то --- это универсум. А ещё есть мат. логика (можно ввести аксиоматику, где есть фундированность). Но это сложно и не для нас.\\
        \end{Comment}
        \dfn \undercolor{red}{Объединение множеств}. $A\cup B=\{x\mid x\in A\lor x\in B\}$.
        \dfn \undercolor{red}{Пересечение множеств}. $A\cap B=\{x\mid x\in A\land x\in B\}$.
        \dfn \undercolor{red}{Разность множеств}. $A\setminus B=\{x\mid x\in A\land x\notin B\}$.
        \dfn \undercolor{red}{Симметрическая разность множеств}. $A\oplus B=(A\setminus B)\cup(B\setminus A)$. Ещё иногда обозначается как $A\operatorname{\Delta}B$.
        \dfn \undercolor{red}{Подмножество}. $A\subset B$ --- $\forall x~x\in A\rightarrow x\in B$.
        \thm Давайте исследуем на опасность такую штуку: $\{B\mid B\subset A\}$.
        \begin{Proof}
            Ну, вроде, норм. Сама на себя она не ссылается, все элементы $B$ лежат у универсуме.
        \end{Proof}
        \dfn Такая штука именуется \undercolor{red}{булеаном} и обозначается $\scriptP(A)$ или $2^A$.
        \dfn \undercolor{red}{Декартово произведение множеств}. $A\times B=\{(a;b)\mid a\in A\land b\in B\}$.
        \dfn Кто такой \undercolor{red}{пара}? Ну, вообще хз, точно не элемент $A\times B$, иначе рекурсия. Вообще в мат. логике говорят, что $\{a,\{b\}\}$, но пофиг.
        \dfn \undercolor{red}{Функция}. Пусть есть $X$ и $Y$. Тогда $f\colon X\to Y$ --- подмножество $X\times Y$, причём $\forall x\in X~\exists! y\in Y~(x;y)\in f$. В таком случае $X$ называется областью определения $f$ и обозначается $D(f)$, а $Y$ --- областью прибытия и обозначается $R(f)$.
        \dfn Если $\forall x_1,x_2\in D(f)~f(x_1)\neq f(x_2)$, то $f$ называется \undercolor{red}{инъекцией}.
        \dfn Если $\forall y\in R(f)~\exists x\in D(f)~f(x)=y$, то $f$ называется \undercolor{red}{сюръекцией}.
        \dfn Если $f$ --- сюръекция и инъекция, то $f$ --- \undercolor{red}{биекция}.
        \begin{Comment}
            Если хотим функцию нескольких аргументов, то что? Ну, $f\colon X_1\times X_2\to Y$. Обычно при этом скобки при $f((x_1;x_2))$ опускают.
        \end{Comment}
        \begin{Comment}
            А если хочется тройки? Ну, вообще можно $X\times(Y\times Z)$. Или $(X\times Y)\times Z$? Вообще говоря, это разные вещи. И даже ни одна из них не $X\times Y\times Z$. Но мы не душнилы, так что будем считать это всё одним и тем же --- множеством упорядоченных троек.
        \end{Comment}
        \dfn Множества можно \undercolor{red}{декартово в степень возводить}. $A^n$ --- это $A\times A^{n-1}$, например, а $A^1=A$. А вот $A^0$ --- это кто? Ну, по-хорошему последовательность из ни одного элемента $A$. Сколько таких? Ну одна. Ну, ок.
        \begin{Comment}
            Ничего не делать есть один способ.\\
            Хотя, конечно, студенты умудряются ничего не делать разными способами, но с точки зрения математики все они --- один.
        \end{Comment}
        \dfn Пусть есть $f\colon A\to B$ и есть $X\subset A$. Тогда $Y=\{y\mid \exists x\in X~f(x)=y\}$. Тогда $Y$ --- \undercolor{red}{образ} $X$ \undercolor{red}{при отображении} $f$. Обычно его обозначают $f(X)$, хотя так-то это не вполне некорректно.
        \dfn Пусть есть $f\colon A\to B$ и есть $Y\subset B$. Тогда $X=\{x\mid f(x)\in Y\}$. Тогда $X$ --- \undercolor{red}{прообраз} $Y$ \undercolor{red}{при отображении} $f$. Обычно его обозначают $f^{-1}(Y)$, хотя это тоже не вполне некорректно.
        \begin{Example}
            $\sin^{-1}(\{1\})=\left\{\frac\pi2+2\pi k\mid k\in\mathbb Z\right\}$.
        \end{Example}
        \dfn Если $f\colon X\to Y$, а $g\colon Y\to Z$, то $g\circ f\colon\substack{X\to Z\\x\mapsto g(f(x))}$. Такое $g\circ f$ называется \undercolor{red}{композицией функций} $f$ и $g$.
        \dfn $Y^X$ --- \undercolor{red}{множество функций} из $X$ в $Y$.
        \dfn Для конечного множества $X$ его \undercolor{red}{мощностью} ($|X|$) называется количество его элементов.
        \thm $|Y^X|=|Y|^{|X|}$.
        \begin{Proof}
            Ну, действительно, каждому элементу $X$ мы сопоставляем абсолютно любой элемент $Y$. То есть для $|X|$ элементов есть ровно $|Y|^{|X|}$ вариантов сопоставления.
        \end{Proof}
        \dfn $\mathbb B=\{0,1\}$.
        \thm $|2^X|=2^{|X|}$.
        \begin{Proof}
            Каждый элемент $X$ мы можем взять в подмножество либо не взять. Так получается $2^{|X|}$ подмножеств.
        \end{Proof}
        \dfn \undercolor{red}{Характеристической функцией} множества $X$ называется любая функция вида $f\colon X\to\mathbb B$.
    \end{itemize}
    \subparagraph{\undercolorblack{orange}{Отношения}.}
    \begin{itemize}
        \dfn $n$-арное (или $n$-местное) отношение на множествах $X_1,X_2,\ldots,X_n$ --- это $\mathrm R\subset X_1\times X_2\times\cdots\times X_n$. При этом обычно отношения на $X^n$ не называют как есть, а называют $n$-арными отношениями на $X$.
        \begin{Example}
            Можно рассмотреть отношение на паре людей <<быть родителем>>. Или на парах человека и собаки --- <<быть хозяином>>. Или бинарное отношение $\leqslant$ на $\mathbb R$, как мы делали на математическом анализе. Обычно в таких случаях не пишется $(x;y)\in{\leqslant}$, а пишется $x\leqslant y$.
        \end{Example}
        \begin{Comment}
            Заметим, что мы пока не определяем, кто такой этот ваш <<$\leqslant$>>, это было на математическом анализе.
        \end{Comment}
        \dfn Бинарное отношение $\mathrm R$ на $X$ называется \undercolor{red}{рефлексивным}, если $\forall x\in X~x\mathrm Rx$.
        \dfn Бинарное отношение $\mathrm R$ на $X$ называется \undercolor{red}{антирефлексивным}, если $\forall x\in X~\neg(x\mathrm Rx)$.
        \dfn Бинарное отношение $\mathrm R$ на $X$ называется \undercolor{red}{симметричным}, если $\forall x_1,x_2\in X~x_1\mathrm Rx_2\rightarrow x_2\mathrm Rx_1$.
        \dfn Бинарное отношение $\mathrm R$ на $X$ называется \undercolor{red}{антисимметричным}, если $\forall x_1,x_2\in X~x_1\mathrm Rx_2\land x_1\neq x_2\rightarrow x_2\mathrm Rx_1$.
        \begin{Comment}
            Мы не хотим запарывать антисимметричность, если уже есть рефлексивность. Поэтому и вводится условие на то, что $x_1\neq x_2$.
        \end{Comment}
        \dfn Бинарное отношение $\mathrm R$ на $X$ называется \undercolor{red}{транзитивным}, если $\forall x_1,x_2,x_3\in X~x_1\mathrm Rx_2\land x_2\mathrm Rx_3\rightarrow x_1\mathrm Rx_3$.
        \dfn Рефлексивное симметричное транзитивное бинарное отношение называется \undercolor{red}{отношенем эквивалентности}.
        \begin{Example}
            Это не то же самое, что и равенство. Например, сравнимость по фиксированному модулю --- отношение эквивалентности.
        \end{Example}
        \thm Если $\mathrm R$ --- отношение эквивалентности на $A$, то оно разбивает всё $A$ на непересекающиеся множества, в которых все элементы попарно эквивалентны друг другу.
        \dfn Такие множества называются \undercolor{red}{классами эквивалентности}, а множество, содержащее их, ---\\\undercolor{red}{фактормножеством} $A$ по отношению $\mathrm R$. Обозначается это так: $A/{\mathrm R}$.
        \begin{Example}
            Сравнимость по фиксированному модулю даёт классы эквивалентности, называемые вычетами по этому модулю.
        \end{Example}
        \dfn Рефлексивное антисимметричное транзитивное отношение называется \undercolor{red}{частичным порядком}.
        \begin{Example}
            Обычно это для своего рода аналог $\leqslant$ или $\geqslant$.
        \end{Example}
        \dfn \undercolor{red}{Частично упорядоченное множество} --- множество, на котором задан частичный порядок.
        \dfn Антирефлексивное антисимметричное транзитивное отношение называется \undercolor{red}{строгим частичным порядком}.
        \begin{Example}
            Обычно это для своего рода аналог $<$ или $>$.
        \end{Example}
        \begin{Comment}
            Несложно заметить, что в частично упорядоченном множестве, и строгий частичный порядок тоже есть. Ровно и наоборот, в любом множестве, где есть строгий частичный порядок, нестрогий тоже есть.
        \end{Comment}
        \dfn Композицией отношений $\mathrm R\subset X\times Y$ и $\mathrm S\subset Y\times Z$ называется отношение $\mathrm R\mathrm S\subset X\times Z$ такой, что $x\mathrm R\mathrm Sz\leftrightarrow\exists y\in Y~x\mathrm Ry\land y\mathrm Sz$.
        \begin{Comment}
            Что-то похожее существует между композицией отношений и транзитивностью, не находите. Так вот это неспроста. Рассмотрим отношение $\mathrm R$ и предположим, что оно не транзитивно. Например, рассмотрим отношение:\\
            \begin{tikzpicture}[
                treenode/.style = {
                    inner sep=0pt,
                    text width=5mm,
                    scale=1.5,
                    circle,
                    outer sep=1.5mm,
                    align=center,
                    draw=white,
                    fill=black,
                    thin
                }
                ]
                \node[treenode] (A) at (0,0) {1};
                \node[treenode] (B) at (2,0) {2};
                \node[treenode] (C) at (4,0) {3};
                \node[treenode] (D) at (6,1) {4};
                \node[treenode] (D1) at (6,-1) {$4'$};
                
                \draw[->] (A) -- (B);
                \draw[->] (B) -- (C);
                \draw[->] (C) -- (D);
                \draw[->] (C) -- (D1);
            \end{tikzpicture}\\
            Это отношение близко к ``быть больше на 1'', за исключением того факта, что у нас откуда-то взялись две четвёрки.
            О'кей, как выглядит $\mathrm R^2$?\\
            \begin{tikzpicture}[
                treenode/.style = {
                    inner sep=0pt,
                    text width=5mm,
                    scale=1.5,
                    circle,
                    outer sep=1.5mm,
                    align=center,
                    draw=white,
                    fill=black,
                    thin
                }
                ]
                \node[treenode] (A) at (0,0) {1};
                \node[treenode] (B) at (2,0) {2};
                \node[treenode] (C) at (4,0) {3};
                \node[treenode] (D) at (6,1) {4};
                \node[treenode] (D1) at (6,-1) {$4'$};
                
                \draw[->] (A) to [out=-45,in=-135] (C);
                \draw[->] (B) to [out=45,in=180] (D);
                \draw[->] (B) to [out=-45,in=180] (D1);
            \end{tikzpicture}\\
            Это --- ``больше на $2$''.
            А $\mathrm R^3$ как выглядит?\\
            \begin{tikzpicture}[
                treenode/.style = {
                    inner sep=0pt,
                    text width=5mm,
                    scale=1.5,
                    circle,
                    outer sep=1.5mm,
                    align=center,
                    draw=white,
                    fill=black,
                    thin
                }
                ]
                \node[treenode] (A) at (0,0) {1};
                \node[treenode] (B) at (2,0) {2};
                \node[treenode] (C) at (4,0) {3};
                \node[treenode] (D) at (6,1) {4};
                \node[treenode] (D1) at (6,-1) {$4'$};
                
                \draw[->] (A) to [out=50,in=180,looseness=.4] (D);
                \draw[->] (A) to [out=-50,in=180,looseness=.4] (D1);
            \end{tikzpicture}\\
            К чему всё это? А к тому, что иногда хочется рассмотреть \textit{транзитивное замыкание}, то есть добавить к отношению что-то минимальное, чтобы оно стало транзитивным. Заметим, что нам нужно добавить к $\mathrm R$ $\mathrm R^2$, чтобы починить все пары отношений в $\mathrm R$, которые нарушают транзитивность. А потом придётся чинить новые пары, добавляя $\mathrm R^3$. Если бы у нас было больше элементов, пришлось бы добавлять $\mathrm R^4$. И так далее.
        \end{Comment}
        \dfn \undercolor{red}{Транзитивным замыканием} $\mathrm R$ называется отношение $\mathrm R^+=\bigcup\limits_{i=1}^\infty\mathrm R^i$.
        \dfn \undercolor{red}{Транзитивным замыканием} $\mathrm R$ называется минимальное по включению транзитивное отношение, содержащее $\mathrm R$. Его обозначают $\operatorname{TrCl}\mathrm R$.
        \thm Пересечение любого количества транзитивных отношений транзитивно.
        \begin{Proof}
            Рассмотрим $(x,y),(y;z)$, удовлетворяющие отношению $\bigcap\limits_{\alpha\in A}\mathrm R_\alpha$. Это значит, что $\forall\alpha\in A~x\mathrm R_\alpha y\land y\mathrm R_\alpha z$. А это значит, что $\forall\alpha\in A~x\mathrm R_\alpha z$, то есть $x\left(\bigcap\limits_{\alpha\in A}\mathrm R_\alpha\right)z$.
        \end{Proof}
        \thm Второе определение корректно.
        \begin{Proof}
            Давайте пересечём вообще все транзитивные отношения, содержащие $\mathrm R$. По предыдущей теореме полученное отношение будет транзитивно. По определению оно будет содержать $\mathrm R$. Заметим, что оно будет минимальным по включению из обладающих таким свойством, так как все остальные будут его содержать.
        \end{Proof}
        \thm Определения эквивалентны.
        \begin{Proof}
            Надо доказать, что равны два множества, $\mathrm R^+$ и $\operatorname{TrCl}\mathrm R$. Значит можно доказать два включения. Сначала докажем, что $\mathrm R^+$ транзитивно.
            \begin{Proof}
                Рассмотрим $(a,b),(b,c)$, такие, что $a\mathrm R^+b$ и $b\mathrm R^+c$. А это значит, что $\exists i,j\in[1:\infty)~a\mathrm R^ib\land b\mathrm R^jc$. А это значит, что $a\mathrm R^{i+j}c\rightarrow a\mathrm R^+c$.
            \end{Proof}
            А что это значит? А то, что $\mathrm R^+$ транзитивно и содержит $\mathrm R$. А значи оно содержит в себе $\operatorname{TrCl}\mathrm R$. То есть одно включение ($\mathrm R^+\supset\operatorname{TrCl}\mathrm R$) мы доказали.\\
            Теперь надо доказать, что $\mathrm R^+\subset\operatorname{TrCl}\mathrm R$, то есть $\forall i\in\mathbb N~\mathrm R^i\subset\operatorname{TrCl}\mathrm R$. Это можно доказать по индукции. База: $\mathrm R\subset\operatorname{TrCl}\mathrm R$. Это правда по определению $\operatorname{TrCl}$. Переход: пусть мы знаем, что $\mathrm R^i\subset\operatorname{TrCl}\mathrm R$. Хочется доказать, что $\mathrm R^{i+1}\subset\operatorname{TrCl}\mathrm R$. Рассмотрим пару $(a;b)$, удовлетворяющую отношению $\mathrm R^{i+1}$. Если $a\mathrm R^{i+1}b$, то $\exists c~a\mathrm R^ic\land c\mathrm Rb$. Это по предположению индукции значит, что $a\operatorname{TrCl}\mathrm Rb$ и $c\operatorname{TrCl}\mathrm Rb$. Поскольку $\operatorname{TrCl}\mathrm R$ транзитивно, $a\operatorname{TrCl}\mathrm Rb$. Это то, что нам и нужно.
        \end{Proof}
        \begin{Comment}
            Эта теорема --- абстрактный случай \textit{метаутверждения}. Оно звучит примерно так: если мы можем добавлять что-то, чтобы стремиться к свойству, то мы очень часто получим ту же штуку, которая является минимальной по включению и содержит данную.
        \end{Comment}
        \begin{Comment}
            Что такое $\mathrm R^0$? Это отношение равенства $I$, который является равенством.
        \end{Comment}
        \dfn Рефлексивно-транзитивное замыкание $\mathrm R$ --- отношение $\mathrm R^*=\bigcup\limits_{i=0}^\infty\mathrm R^i$. То есть это просто транзитивное замыкание, которое мы принудительно сделали рефлексивным.
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Булевы функции}.}
    \begin{itemize}
        \item[]\mbox{}
        \begin{Comment}
            У нас образовались две группы предметов, что-то непрерывное в математике и что-то компьютерное. Дискретка --- это то, что пытается это связать.
        \end{Comment}
        \begin{Comment}
            Вспоминаем, кто такой $\mathbb B$. Это $\{0,1\}$.
        \end{Comment}
        \dfn $n$-ичная ($n$-арная) \undercolor{red}{булева функция} --- это $f\colon\mathbb B^n\to\mathbb N$.
        \thm Таких функций $2^{2^n}$.
        \begin{Comment}
            Мы хотим рассмотреть все функции для всех $n$ от 0 до 2.\\
            $n=0$:
            $\mathbb0_0\colon{}\mapsto0$ --- тождественный ноль,
            $\mathbb1_0\colon{}\mapsto1$ --- тождественная единица.\\
            $n=1$:\\
            \begin{tabular}{|c|cccc|}
                \hline
                $x$ & $\mathbb0_1$ или $\mathbb0(x)$ & $\mathrm{id}$ или $P_1$ --- первый проектор. & $\neg$/$!$/$\mathrm{not}$/$\mathrm{neg}$ & $\mathbb1_1$ или $\mathbb1(x)$\\
                \hline
                0 & 0 & 0 & 1 & 1\\
                1 & 0 & 1 & 0 & 1\\
                \hline
            \end{tabular}\\
            $n=2$:\\
            \begin{tabular}{|cc|ccccc|}
                \hline
                $x$ & $y$ & $\mathbb0_2$/$\mathbb0(x;y)$ & $\land$/$\&$ --- конъюнкция & $\not\rightarrow$ & $P_1$ --- 1-й проектор & $\not\leftarrow$\\
                \hline
                0 & 0 & 0 & 0 & 0 & 0 & 0\\
                0 & 1 & 0 & 0 & 0 & 0 & 1\\
                1 & 0 & 0 & 0 & 1 & 1 & 0\\
                1 & 1 & 0 & 1 & 0 & 1 & 0\\
                \hline
            \end{tabular}\\
            \begin{tabular}{|cc|cccc|}
                \hline
                $x$ & $y$ & $P_2$ --- 2-й проектор & $\mathrm{xor}$/$\oplus$ & $\lor$/$|$ --- дизъюнкция & $\downarrow$/$\mathrm{nor}$ --- стрелка Пирса\\
                \hline
                0 & 0 & 0 & 0 & 0 & 1\\
                0 & 1 & 1 & 1 & 1 & 0\\
                1 & 0 & 0 & 1 & 1 & 0\\
                1 & 1 & 1 & 0 & 1 & 0\\
                \hline
            \end{tabular}\\
            \begin{tabular}{|cc|ccccccc|}
                \hline
                $x$ & $y$ & $\leftrightarrow$/$\mathrm{eq}$/$=$ & $\neg P_2$ & $\leftarrow$ & $\neg P_1$ & $\rightarrow$ & $\nabla$/$\uparrow$/$\mathrm{nand}$ --- штрих Шеффера & $\mathbb1_2$/$\mathbb1(x;y)$\\
                \hline
                0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\
                0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 1\\
                1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1\\
                1 & 1 & 1 & 0 & 1 & 0 & 1 & 0 & 1\\
                \hline
            \end{tabular}\\
            Зачем нам все эти функции? Они так или иначе используется в различных приложениях. Например, в теории сложности, базис $U_2$ --- это все они.
        \end{Comment}
        \begin{Comment}
            Про импликацию ($\rightarrow$). С точки зрения булевой алгебры никакой причинно-следственной связи она не несёт. В импликации $3>2\rightarrow 7^2>0$ между левой и правой частью утверждения нет никакой связи, тем не менее это утверждение истинно.
        \end{Comment}
        \begin{Comment}
            Запись булевых функций. Мы обычно на математике не пишем все значения таблицей значений, а пишем что-нибудь вида $f(x)=x^2$. Так давайте же формально сформулируем, что такое формула.
        \end{Comment}
        \dfn Пусть нам даны функции $f(x_1;x_2;\ldots;x_n)$ и $g(x_1;x_2;\ldots;x_m)$ и $i\in[0:n]$. Тогда \undercolor{red}{композицией} называется $h(x_1;x_2;\ldots;x_{n+m-1})=f(x_1;x_2;\ldots;x_i;g(x_{i+1};x_{i+2};\ldots;x_{i+m});x_{i+m+1};\ldots;x_{n+m-1})$.
        \dfn Пусть нам дана функция $f(x_1;x_2;\ldots;x_n)$ и упорядоченный набор $\{i_j\}$ из $n$ элементов множества $[1:m]$. Тогда \undercolor{red}{подстановкой} называется $g(x_1;x_2;\ldots;x_m)=f(x_{i_1};x_{i_2};\ldots;x_{i_n})$
        \begin{Example}
            Пусть у нас есть $f(x_1;x_2)=x_1\land x_2$, $g(x_1;x_2)=x_1\lor x_2$. Тогда $h(x_1;x_2;x_3)=x_1\land(x_2\lor x_3)$ --- композиция. А, например, $k(y_1;y_2)=h(y_1;y_1;y_2)$ --- подстановка.
        \end{Example}
        \dfn Пусть у нас есть набор булевых функций. \undercolor{red}{Формулой} в таком случае называется выражение, состоящее из переменных и этих самых функций (они называются \undercolor{red}{базовыми функциями} или \undercolor{red}{связками}).
        \dfn Если $A$ --- множество базовых функций, то $\Cl A$ --- \undercolor{red}{замыкание} $A$ --- множество всех формул, которые можно получить из функций $A$ подстановками и композициями.
        \dfn Множество $A$ называется \undercolor{red}{полным} или \undercolor{red}{базисом}, если $\Cl A$ состоит из всех булевых функций (кроме $\mathbb0_0$ и $\mathbb1_0$).
        \thm $\{\land,\lor,\neg\}$ --- полный базис.
        \begin{Proof}
            Пусть мы хотим записать $f\colon\mathbb B^n\to\mathbb B$. Пусть $f$ --- не $\mathbb0_0$ и не $\mathbb0_1$. Эти два случая мы не рассматриваем. Пусть наша функция --- не $\mathbb0_n$. Давайте рассмотрим все наборы аргументов, в которых единицы. Такие есть по предположению. Давайте тогда напишем формулу так: для каждой строчки мы напишем все аргументы, добавим отрицание к тем, которые равны 0 и соединим конъюнкцией. Все полученные формулы мы соединим дизъюнкцией. Понятно, что формула подходит. Каждую строчку наращиваем композицией. Потом из них наращиваем дизъюнкцию. Потом подставляем туда правильные аргументы. А если у нас $\mathbb0_n$ мы делаем, например $x_1\land\neg x_1\land x_2\land x_3\land\cdots\land x_n$.
        \end{Proof}
        \dfn Полученная нами формула (для всего, кроме $\mathbb0_n$) называется \undercolor{red}{совершенной дизъюнктивной нормальной формой}.
        \dfn А сам базис называется \undercolor{red}{каноническим}.
        \begin{Comment}
            Почему \textit{форма} --- понятно. Почему, \textit{нормальная}, никто не знает, \textit{дизъюнктивная} --- потому что снаружи дизъюнкция, а \textit{совершенная} --- потому что в каждой скобке есть все переменные.
        \end{Comment}
        \begin{Example}
            Пусть мы хотим выразить функцию с вот такой таблицей истинности:\\
            \begin{tabular}{|ccc|c|}
                \hline
                $x$ & $y$ & $z$ & \\
                \hline
                0 & 0 & 0 & 0\\
                0 & 0 & 1 & 1\\
                0 & 1 & 0 & 1\\
                0 & 1 & 1 & 0\\
                1 & 0 & 0 & 0\\
                1 & 0 & 1 & 0\\
                1 & 1 & 0 & 1\\
                1 & 1 & 1 & 0\\
                \hline
            \end{tabular}\\
            Берём все строчки, в которых функция равна 1:\\
            \begin{tabular}{|ccc|c|}
                \hline
                $x$ & $y$ & $z$ & \\
                \hline
                0 & 0 & 1 & 1\\
                0 & 1 & 0 & 1\\
                1 & 1 & 0 & 1\\
                \hline
            \end{tabular}\\
            Для каждой из них записываем конъюнкцию, ставя отрицание педер теми аргументами, которые равны нулю:\\
            \begin{tabular}{|c|}
                \hline
                $\neg x\land\neg y\land z$\\
                $\neg x\land y\land\neg z$\\
                $x\land y\land\neg z$\\
                \hline
            \end{tabular}\\
            Теперь соединяем это всё дизъюнкциями и радуемся жизни: $(\neg x\land\neg y\land z)\lor(\neg x\land y\land\neg z)\lor(x\land y\land\neg z)$.
        \end{Example}
        \begin{Comment}
            Существует такая удобная штука, как дерево разбора. Оно выглядит так: у нас есть операция, которая осуществляется в самом конце. Она является корнем дерева. А её аргументы --- дети корня. И так далее. Тогда композиция выглядит как приклеивание одного дерева вместо листа другого (ну, почти), а подстановка --- ...ну, как подстановка. Одного листа вместо другого.
            \begin{Example}
                Пусть у нас есть деревья для $x_1\land x_2$ и $x_1\lor x_2$. Они выглядит так:\\
                \begin{tikzpicture}[
                    treenode/.style = {
                        inner sep=0pt,
                        text width=5mm,
                        scale=1.5,
                        circle,
                        outer sep=2mm,
                        align=center,
                        draw=black,
                        fill=white,
                        thin
                    }
                ]
                    \node[treenode] (root) at (0,0) {$\land$};
                    \node[treenode] (l) at (-1,-2) {$x_1$};
                    \node[treenode] (r) at (1,-2) {$x_2$};
                    
                    \draw[->] (root) -- (l);
                    \draw[->] (root) -- (r);
                \end{tikzpicture}
                \begin{tikzpicture}[
                    treenode/.style = {
                        inner sep=0pt,
                        text width=5mm,
                        scale=1.5,
                        circle,
                        outer sep=2mm,
                        align=center,
                        draw=black,
                        fill=white,
                        thin
                    }
                ]
                    \node[treenode] (root) at (0,0) {$\lor$};
                    \node[treenode] (l) at (-1,-2) {$x_1$};
                    \node[treenode] (r) at (1,-2) {$x_2$};
                    
                    \draw[->] (root) -- (l);
                    \draw[->] (root) -- (r);
                \end{tikzpicture}\\
                Тогда композиция этих функций (например, $x_1\land(x_2\lor x_3)$) может выглядеть так:\\
                \begin{tikzpicture}[
                    treenode/.style = {
                        inner sep=0pt,
                        text width=5mm,
                        scale=1.5,
                        circle,
                        outer sep=2mm,
                        align=center,
                        draw=black,
                        fill=white,
                        thin
                    }
                ]
                    \node[treenode] (root) at (0,0) {$\land$};
                    \node[treenode] (l) at (-1,-2) {$x_1$};
                    \node[treenode] (r) at (1,-2) {$\lor$};
                    \node[treenode] (rl) at (0,-4) {$x_2$};
                    \node[treenode] (rr) at (2,-4) {$x_3$};
                    
                    \draw[->] (root) -- (l);
                    \draw[->] (root) -- (r);
                    \draw[->] (r) -- (rl);
                    \draw[->] (r) -- (rr);
                \end{tikzpicture}\\
                А если рассмотреть подстановку (например, $y_1\land(y_2\lor y_1)$), то получится:\\
                \begin{tikzpicture}[
                    treenode/.style = {
                        inner sep=0pt,
                        text width=5mm,
                        scale=1.5,
                        circle,
                        outer sep=2mm,
                        align=center,
                        draw=black,
                        fill=white,
                        thin
                    }
                ]
                    \node[treenode] (root) at (0,0) {$\land$};
                    \node[treenode] (l) at (-1,-2) {$y_1$};
                    \node[treenode] (r) at (1,-2) {$\lor$};
                    \node[treenode] (rl) at (0,-4) {$y_2$};
                    \node[treenode] (rr) at (2,-4) {$y_1$};
                    
                    \draw[->] (root) -- (l);
                    \draw[->] (root) -- (r);
                    \draw[->] (r) -- (rl);
                    \draw[->] (r) -- (rr);
                \end{tikzpicture}
            \end{Example}
        \end{Comment}
        \thm $\{\neg;\land\}$ и $\{\neg;\lor\}$ также являются полными, а $\{\lor;\land\}$ --- нет.
        \begin{Proof}
            Сначала докажем первые два утверждения. По таблице истинности можно убедиться, что $x\lor y=\neg(\neg x\land\neg y)$ и что $x\land y=\neg(\neg x\lor\neg y)$ (эти утверждения называются законами де Моргана). А значит, всё, что выразимо в $\{\neg;\land;\lor\}$ (то есть всё, как мы уже доказали), можно выразить в $\{\neg;\land\}$ и $\{\neg;\lor\}$ простой заменой недостающей базовой функции на формулу из закона де Моргана.\\
            Теперь докажем, что нельзя выразить любую формулу в $\{\land;\lor\}$. Для того, чтобы доказать, что мы не можем выразить все, достаточно найти одну. Её станет, например, $\mathbb1_n$. Как это доказать? Индукцией по дереву разбора. В данном случае удобно проводить индукцию по его высоте. Мы хотим доказать, что в строке, в которой все переменные равны 0 результат формулы будет нулём. База: дерево высоты 1. В нём есть только одна переменная, а значит при равенстве всех переменных нулю она тоже равна нулю. Переход: пусть все формулы для деревьев высоты $n$ возвращают 0, если все переменные нулевые. Тогда рассмотрим дерево высоты $n+1$. Его корень --- либо конъюнкция, либо дизъюнкция. При этом аргументами этой конъюнкции/дизъюнкции являются деревья высоты, не большей, чем $n$, а значит при равенстве всех аргументов нулю, формулы для этих деревьев возвращают 0. То есть при таких аргументах корень --- это либо конъюнкция двух нулей, либо их дизъюнкция. И то, и другое возвращает ноль.
        \end{Proof}
        \dfn Функция $f$ назвается \undercolor{red}{сохраняющей 0}, если $f(0,0,\ldots,0)=0$.
        \dfn Функция $f$ назвается \undercolor{red}{сохраняющей 1}, если $f(1,1,\ldots,1)=1$.
        \dfn Функция $f$ назвается \undercolor{red}{линейной}, если её можно представить как $x_{i_1}\oplus x_{i_2}\oplus\cdots\oplus x_{i_n}$ или $\mathbb1\oplus x_{i_1}\oplus x_{i_2}\oplus\cdots\oplus x_{i_n}$.
        \dfn Функция $f$ назвается \undercolor{red}{монотонной}, если $\forall i\in[1:n]~\forall x_1,x_2,\ldots,x_n~f(x_1;x_2;\ldots;x_{i-1};0;x_{i+1};\ldots;x_{n-1};x_n)=1\rightarrow f(x_1;x_2;\ldots;x_{i-1};1;x_{i+1};\ldots;x_{n-1};x_n)=1$. Иначе говоря, при смене одного аргумента с 0 на 1 функция не может изменится с 1 на 0.
        \dfn Функция $f$ назвается \undercolor{red}{самодвойственной}, если $\forall x_1,x_2,\ldots,x_n~f(\neg x_1;\neg x_2;\ldots;\neg x_n)=\neg f(x_1;x_2;\ldots;x_n)$.
        \begin{Comment}
            Множество функций, сохраняющих 0, обозначают $F_0$, сохраняющих 1, --- $F_1$, множество линейных функций --- $F_l$, монотонных --- $F_m$, самодвойственных --- $F_s$. Эти множества именуются классами Поста, и они нам ещё пригодятся.
        \end{Comment}
        \dfn . Рассмотрим базис: $\{\oplus,\land,\mathbb1\}$. Выражение функции в нём называется \undercolor{red}{полиномом Жегалкина} или \undercolor{red}{арифметическим полиномом}.
        \begin{Comment}
            Заметим интересный факт. $\oplus$ --- это сложение по модулю 2, а $\land$ --- умножение по модулю 2. Это как раз та причина, почему это арифметические полиномы. И именно поэтому $\land$ иногда заменяют на ничего, как в умножении (т.е. $xy$ вместо $x\land y$).
        \end{Comment}
        \thm Пусть у нас есть два базис $A$ и какое-то множество функций $B$. Тогда, если любую функцию множества $A$ можно выразить через функции множества $B$, то $B$ --- базис.
        \begin{Proof}
            Пусть мы хотим выразить функцию $f$. Рассмотрим формулу в базисе $A$. Докажем, что $f$ можно выразить в $B$ при помощи индукции по дереву разбора. Если это дерево имеет высоту 1, то это переменная, её не надо выражать. Переход: Рассмотрим корень. В нём находится функция $a\in A$. Её можно выразить в системе $B$. А все поддеревья корня являются деревьями меньшей высоты, значит их можно выразить по индукционному предположению.
        \end{Proof}
        \begin{Example}
            Рассмотрим на примере полиномов Жегалкина. $A=\{\land,\lor,\neg\}$. $B=\{\oplus,\land,\mathbb1\}$. Мы уже знаем, как выразить $\land$, она и так есть в $B$. Выразить $\neg$ довольно легко, $\neg x=\mathbb1\oplus x$. А вот $\lor$ можно и не выражать, мы уже доказали, что полином избыточен. Но всё равно давайте выразим. $x\lor y=\neg(\neg x\land\neg y)=((x\oplus\mathbb1)(y\oplus\mathbb1))\oplus\mathbb1$.
        \end{Example}
        \begin{Comment}
            $((x\oplus\mathbb1)(y\oplus\mathbb1))\oplus\mathbb1$ --- это какая-то неприятная формула. Давайте-ка заметим, что, поскольку, $\oplus$ и $\land$ --- это сложение и умножение по модулю 2, можно его по арифметическим законам упростить. $((x\oplus\mathbb1)(y\oplus\mathbb1))\oplus\mathbb1=xy\oplus x\oplus y\oplus\mathbb1\oplus\mathbb1=xy\oplus x\oplus y$.
        \end{Comment}
        \dfn У полинома Жегалкина есть каноническая форма. \undercolor{red}{Канонический (или приведённый) полином Жегалкина} --- это полином Жегалкина, который выглядит как \textit{сумма} (имеется ввиду $\oplus$), возможно, единицы некоторых произведений ($\land$) переменных.
        \thm Каждый полином Жегалкина можно сделать каноническим.
        \begin{Proof}
            Есть алгоритм, как приводить полином. Сначала надо раскрыть все скобки, потом преобразовать одинаковые множители ($xx$) в самих себя (в $x$), а из умножения $\mathbb1$ на что-то убрать $\mathbb1$. Потом попарно убирать одинаковые слагаемые.
        \end{Proof}
        \thm Каждую функцию, кроме $\mathbb0$ можно выразить ровно одним каноническим полиномом Жегалкина (с точностью до перестановки множителей и перестановки слагаемых).
        \begin{Comment}
            А чем провинился $\mathbb0$? А тем, что его можно представить как $\mathbb1\oplus\mathbb1$, но это не приведённый полином. А если его привести, получится пустая строка.
        \end{Comment}
        \begin{Proof}
            Давайте посчитаем количество полиномов Жегалкина на $n$ переменных. Сколько у нас есть мономов (т.е. одночленов)? Мы можем взять или не взять каждую переменную в произведение, а если не взять никого, получится $\mathbb1$. Это $2^n$ мономов. Окей, а теперь заметим, что каждый возможный моном можно взять или не взять. Но с пустой суммой мы бороться не умеем. Поэтому всего полиномов Жегалкина есть $2^{2^n}-1$. Да это же ровно столько же, сколько функций на $n$ переменных, кроме $\mathbb0_n$!\\
            Итого у нас есть 2 факта: каждая функция имеет полином, а всего их столько же, сколько и полиномов. Понятно, что у каждой функции полином уникален. А значит ни у одной функции не может быть 2 полинома, иначе всего полиномов было бы больше, чем функций.
        \end{Proof}
        \begin{Comment}
            Напоминаю, что базисы могут быть избыточными. Как канонический например. Например, есть $U_2$ и $T_2$ --- множество всех функций двух аргументов и множество пороговых функций двух переменных соответственно.
        \end{Comment}
        \begin{Comment}
            Возвращаемся к классам Поста.\\
            $F_0$ --- множество функций, сохраняющих 0, $F_1$ --- сохраняющих 1, $F_l$ --- множество линейных функций, $F_m$ --- монотонных, $F_s$ --- самодвойственных.
        \end{Comment}
        \thm \undercolor{darkgreen}{Теорема Поста} (или \undercolor{darkgreen}{критерий Поста}). Множество функций $F$ является базисом тогда и только тогда, когда он содержит немонотонную, несамодвойственная, нелинейную, не сохраняющую 0 и не сохраняющую 1 функцию.
        \begin{Proof}
            Сначала приведём для каждого класса пример функции, не лежащей в нём. $\mathbb1\notin F_0$, $\mathbb0\notin F_1$, $\neg\notin F_m$, $\oplus\notin F_s$, $\lor\notin F_l$. Теперь докажем следующий факт. Если $A\subset F_i$ и $f$ выражается через $A$, то $f\in F_i$. То есть что композиция функций, лежащих в каком-то классе, лежит в этом классе. Индукция по дереву разбора. С сохранением 0 и 1 всё понятно. Если каждое поддерево монотонно, то при увеличении каких-то аргументов результат вычисления каждого поддерева не уменьшится, а значит не уменьшится и результат вычисления всего дерева так как корень монотонен. Аналогично со всем остальным вообще. А значит если $F$ не содержит, скажем, немонотонной функции, то все остальные немонотонные (которые существуют) выразить нельзя. А значит мы доказали следствие \textit{вправо}.\\
            Теперь докажем следствие \textit{влево}.
            Итак, пусть у нас есть пять функций, не лежащих в соответствующих классах. $f_0$, $f_1$, $f_s$, $f_m$ и $f_l$. Рассмотрим $f_0$. Мы знаем, что $f_0(0,0,\ldots,0)=1$, но это нам не нужно. Рассмотрим, чему равно $f_0(1,1,\ldots,1)$. Если 1 (вариант \textbf a), то $f_0(x,x,\ldots,x)=\mathbb1$. В противном случае (вариант \textbf b) $f_0(x,x,\ldots,x)=\neg x$. Сделаем то же самое с $f_1$. Если $f_1(0,0,\ldots,0)=0$ (вариант \textbf 1), то $f_1(x,x,\ldots,x)=\mathbb0$. Если $f_1(0,0,\ldots,0)=1$ (вариант \textbf 0), то $f_1(x,x,\ldots,x)=\neg x$. Теперь давайте скомбинируем эти варианты. В случае \textbf{a1} у нас есть $\mathbb0$ и $\mathbb1$. В случае \textbf{a2} у нас есть $\neg$ и $\mathbb1$, а значит мы без СМС и регистрации мы получаем $\mathbb0$, как и в случае \textbf{b1}. А в случае \textbf{b2} всё грустно, у нас есть только $\neg$. Ну, очень хорошо. Давайте добавим во все варианты $\neg$ и константы, если их не хватает.\\
            Рассмотрим \textbf{a1}. У нас есть немонотонная $f_m$. Это значит, что она как-то немонотонна. А значит есть набор $x_1,x_2\ldots,x_n$, в котором $f_m(x_1,\ldots,x_{i-1},0,x_{i+1},\ldots,x_n)=1$, а\\$f_m(x_1,\ldots,x_{i-1},1,x_{i+1},\ldots,x_n)=0$. А это значит, что можно в каждый $x$, кроме $x_i$ можно подставить тождественные нули и единицы, в зависимости от того, чему они равны. А на место $x_i$ поставить единственный параметр. Таким образом у нас образовалось $\neg$.\\
            Теперь давайте разбираться с \textbf{b2}. Нам нужно откуда-то добыть $\mathbb0$ и $\mathbb1$. Добудем её из несамодвойственной функции $f_s$. Она несамодвойственна, значит для какого-то набора $f_s(x_1,\ldots,x_n)=f_s(\neg x_1,\ldots,\neg x_n)$. Хрен его знает, чему это равно, но либо 0, либо 1. У нас какие-то $x_i$ равны 0, а какие-то --- 1. Вместо первых мы подставим $x$, вместо вторых --- $\neg x$. Заметим, что вне зависимости от $x$ мы получим одно и тот же. А значит получится тождественная константа. Какая-то. Вторую мы получим из первой и $\neg$.\\
            О'кей, теперь смотрим на нелинейную функцию. Представим её как полином Жегалкина. В нем объективно есть мономы с количеством множителей, б\'{о}льшим двух. Возьмём из них такой, который содержит минимальное количество множителей. Любой из таких. Теперь возьмём любые две переменные в этом мономе. Заменим все, кроме них, на $\mathbb0$, их в других мономах --- на $\mathbb1$, а их в том самом мономе оставим переменными. Заметим, что у нас образовался полином двух переменных, в котором есть ровно один (понятно, какой) моном из умножения двух переменных, а все остальные мономы содержат 0 или 1 переменную. Упростим этот полином, если можем. А значит у нас получился один из 8 полиномов:
            \begin{enumerate}[1.]
                \item $xy$
                \item $xy\oplus\mathbb1$
                \item $xy\oplus x$
                \item $xy\oplus x\oplus\mathbb1$
                \item $xy\oplus y$
                \item $xy\oplus y\oplus\mathbb1$
                \item $xy\oplus x\oplus y$
                \item $xy\oplus x\oplus y\oplus\mathbb1$
            \end{enumerate}
            Чётные варианты можно свести к нечётным при помощи $\neg$. Варианты 1 и 7 являются $\land$ и $\lor$ соответственно, через это можно всё выразить. Варианты 3 и 5 аналогичны, поэтому рассмотрим только один из них. Например, третий. $xy\oplus x=x(y\oplus\mathbb1)$, а значит можно взять $\neg y$ вместо $y$, и получить $\land$.
        \end{Proof}
        \begin{Comment}
            А почему в двух случаях из четырёх нам не понадобились несамодвойственные и не монотонные функции? А потому, что в этих случаях $f_0$ и/или $f_1$ сами были несамодвойственными или немонотонными.
        \end{Comment}
        \begin{Comment}
            Поговорим о полиноме Жегалкина. Что такое полином/многочлен? Это сумма мономов/одночленов. А что это такое? А это произведение переменных в некоторых степенях (с коэффициентами). Это в общем случае. А у нас ну бывает степеней, это не имеет смысла. Пусть у нас есть $n$ переменных. Моном --- это произведение некоторых из них. Мы можем представить моном как набор из нулей и единиц длины $n$, в котором 0 соответствует отсутствию переменной с заданным номером, а 1 --- её наличию. Например, для переменных $x_1$, $x_2$ и $x_3$ набор битов $101$ соответствует $x_1x_3$, а набор $000$ --- $\mathbb1$.\\
            Рассмотрим обычный многочлен, $x^3-2x^2+4x-7$. Тогда у нас какие коэффициенты? $a_0=-7$, $a_1=4$, $a_2=-1$, $a_3=1$. А в полиноме $x^2+4$? $a_0=4$, $a_1=0$, $a_2=1$. А как же в полиномах Жегалкина? Ну, коэффициенты у нас бывают двух видов, 0 и 1. Непонятно только, как их нумеровать. Ну, легко, для этого мы и говорили про набор битов: $x_1\lor x_2=x_1x_2\oplus x_1\oplus x_2=a_{11}x_1x_2\oplus a_{10}x_1\oplus a_{01}x_2\oplus a_{00}$, где, как видно, $a_{11}=a_{10}=a_{01}=1$ и $a_{00}=0$. То есть полином Жегалкина можно записать как сумму $\bigoplus\limits_{(i_1,i_2,\ldots,i_n)\in\{0,1\}^n}a_{i_1i_2\cdots i_n}$ чего? Говорить, что это сумма произведений $x_{i_j}$, где $i_j=1$ --- это сложно и плохо. Поэтому мы просто скажем, что $x_k^0=1$, а $x_k^1=x_k$ и запишем $\bigoplus\limits_{(i_1,i_2,\ldots,i_n)\in\{0,1\}^n}a_{i_1i_2\cdots i_n}x_1^{i_1}x_2^{i_2}\cdots x_n^{i_n}$. Тут мы удобства ради предположили, что $0^0=1$. То есть полином Жегалкина --- это просто вектор размера $2^n$.  Например, $\left(\begin{matrix}
                0 & 1 & 1 & 1
            \end{matrix}\right)=0x_1^0x_2^0\oplus 1x_1^0x_2^1\oplus 1x_1^1x_2^0\oplus 1x_1^1x_2^1=x_1\lor x_2$.\\
            Интересный факт. Необходимо одинаковое количество памяти на хранение таблицы истинности и полинома Жегалкина. И тех, и других, $2^n$. Некоторые, посмотрев на то, что у $x_1\land x_2$ и $x_1\lor x_2$ соотвественно совпадают таблица и полином, могут подумать, что они всегда совпадают. Но это не так, у $\mathbb1_2$ полином $\left(\begin{matrix}
                1 & 0 & 0 & 0
            \end{matrix}\right)$, а таблица состоит из всех единиц.\\
            О'кей, а как построить тогда полином зная таблицу? Биективно, как минимум. Но это не вполне ответ на вопрос. Пусть $u(A)=F$, где $A$ --- полином (в виде вектора), $F$ --- таблица истинности (тоже в виде вектора), а $u$ --- то самое отображение. Кстати, у $u$ есть интересный факт, $u^{-1}=u$. Такое $u$ называется инволюцией, но мы как-то отошли от темы.\\
            Как построить $u$? Ну, на самом деле легко. $\bigoplus\limits_{(i_1,i_2,\ldots,i_n)\in\{0,1\}^n}a_{i_1i_2\cdots i_n}x_1^{i_1}x_2^{i_2}\cdots x_n^{i_n}$. В каком случае произведение переменных обнуляет $a_{i_1i_2\cdots i_n}$? Когда хотя бы для одного $j$ $x_j=0$, а $i_j=1$, ведь мы предположили, что $0^0=1$.
        \end{Comment}
        \dfn Говорят, что двоичный вектор $x$ \undercolor{red}{доминируется} вектором $y$, если $\forall i~x_i\leqslant y_i$. Обозначается это $x\preccurlyeq y$.
        \begin{Comment}
            Так вот, если $i\preccurlyeq x$, то $a_{i_1i_2\cdots i_n}$ не уничтожается, а иначе уничтожается. То есть получается сумма $\bigoplus\limits_{i\preccurlyeq x}a_{i_1i_2\cdots i_n}$.\\
            Пример: рассмотрим наш любимый $x_1\lor x_2$. Его вектор всё ещё выглядит как $\left(\begin{matrix}
                0 & 1 & 1 & 1
            \end{matrix}\right)$. Значение таблицы истинности в нуле --- $\mathrm{xor}$ всего, что доминируется комбинацией $00$. То есть никто, кроме самого $00$. Там значение 0, то есть в таблице истинности 0. Кто доминируется $01$? Оно само и $00$, то есть получается $0\oplus 1$. Аналогично с $10$. А $11$ доминируется всем, то есть получается $0\oplus 1\oplus 1\oplus 1=1$. Прекрасно. Но пока не понятно, как записать нормально $u$. Давайте введём $d_{ix}=\begin{cases}
                1 & i\preccurlyeq x\\
                0 & \mathrm{otherwise}
            \end{cases}$. Выглядит как матрица размера $2^x\times 2^n$. Тогда можно сказать, что $F=AD$. То есть $u$ --- это умножение на матрицу $D$.\\
            Просто прекрасно, а $u^{-1}$? Утверждается, что $a_i=\bigoplus\limits_{x\preccurlyeq i}f_x$. И мы уже знаем, что $f_x=\bigoplus\limits_{i\preccurlyeq x}a_i$. Запишем это как $f_x=a_x\oplus\bigoplus\limits_{i\prec x}a_i$. Докажем то, что хотим, по индукции. Если $i=0$, то $f_0=a_0$ --- верно. Переход. Пусть мы уже знаем, что $a_i=\bigoplus\limits_{y\preccurlyeq i}f_y$. Тогда $f_x=a_x\oplus\bigoplus\limits_{i\prec x}\bigoplus\limits_{y\preccurlyeq i}f_y$. От $i$ это не зависит, а значит это можно записать как $f_x=a_x\oplus\bigoplus\limits_{y\prec x}z_{xy}f_y$, где $z_{xy}$ равно количеству таких $i$, что $y\preccurlyeq i\prec x$. Посмотрим на какую-то позицию в $x$, $i$ и $y$. Если в $x$ и $y$ там одно число, то на соответствующем месте $i$ может быть только это же самое число. А если в $y$ там 0, а в $x$ --- 1, то возможны оба варианта. То есть количество таковых $i$ --- это какая-то степень двойки... могла бы быть, если бы один вариант не являлся не подходящим, $x$. То есть количество таких $i$ всегда нечётно, то есть $f_x=a_x\oplus\bigoplus\limits_{y\prec x}f_y$. Кажется, это то, что нам нужно.\\
        \end{Comment}
        \dfn Указанное преобразование $u$ называется \undercolor{red}{преобразованием Мёбиуса}.
    \end{itemize}
    \subparagraph{\undercolorblack{orange}{Схемы функциональных элементов}.}
    \begin{itemize}
        \item[]\mbox{}
        \begin{Comment}
            Тут нам понадобится понятие \textit{ациклический ориентированный граф}, которые мы будем изучать аж через год, поэтому не будем определять его сейчас нормально. Хотя можем.\\
            Вместо этого мы просто скажем, что
        \end{Comment}
        \dfn У нас есть вершинки, упорядоченные их пары --- это ориентированное ребро, получается \undercolor{red}{ориентированный граф}. Если из вершинки нельзя дойти до неё же по рёбрам, то он \undercolor{red}{ациклический}.
        \thm Топологическая сортировка. Если у нас есть ациклический ориентированный граф, то можно так пронумеровать его вершины (то есть построить биекцию в некоторый промежуток $[1:k]$), что рёбра ведут только из вершины с меньшими номерами в вершины с б\'{о}льшими.
        \begin{Proof}
            Индукция по количеству вершин. Петли нет, иначе цикл, поэтому пронумеруем её номером 1 и будем рады.\\
            Переход. Утверждается, что в графе на $n+1$ вершин есть вершина, из которой не выходит рёбер. Выберем случайную. Если из неё нет рёбер, то хорошо. Иначе пойдём по произвольному ребру из неё. За $n+1$ таких шагов мы либо придём в уже посещённую вершину (цикл, противоречие), либо найдём то, что хотим. Теперь удалим её. Получим ациклический ориентированный кусок графа размера $n$. Его по индукционному предположению можно пронумеровать. А удалённой вершине присвоим номер $n+1$. Несложно заметить, что всё будет хорошо.
        \end{Proof}
        \begin{Comment}
            Зачем? По топологической сортировке можно делать индукцию и динамику. Этим мы будем пользоваться.
        \end{Comment}
        \dfn Выберем множество базисных функций $M$. Тогда \undercolor{red}{схема из функциональных элементов} --- это ориентированный ациклический граф на $M$ (возможно, с кратными рёбрами), в котором все входные рёбра из каждой вершины пронумерованы.\\
        Пусть мы хотим построить функцию $f(x_1;\ldots;x_n)$. Тогда у нас есть вершины $x_1$, $x_2$,..., $x_n$, в которые не приходят рёбра, и вершина out, из которой, наоборот, ничего не исходит. Все остальные вершины являются функциями базиса, в которые входит ровно столько рёбер, сколько аргументов у соответствующей функции, причём они пронумерованы.
        \begin{Comment}
            Если функция базиса симметрична, то можно не нумеровать.
        \end{Comment}
        \begin{Comment}
            Как посчитать схему? Мы пишем рядом с каждой вершиной 0 или 1, а потом \textit{в порядке топологической сортировки} смотрим на вершины и пишем рядом с ними то число, чему равен результат вычисления её от уже посчитанных (по вине Т.С.). Результат находится в вершине out. Все остальные вершины --- внутренние элементы.
        \end{Comment}
        \begin{Comment}
            Обычно не удобно писать внутри вершины значок функции, поэтому люди, которые занимаются схемотехникой, любят специальные значки. Но мы не любим.
        \end{Comment}
        \thm Любую функцию можно задать схемой функциональных элементов над заданным базисом.
        \begin{Proof}
            Хм-м. Да мы же уже доказывали почти это, когда говорили про дерево разбора. Рассмотрим его. Потом обсудим, как бороться с тождественными $\mathbb1$ и $\mathbb0$ в листьях, а пока будем считать, что в листьях только переменные. Сначала склеим все одинаковые переменные, потом ориентируем все рёбра <<вверх>>, а затем добавим над корнем out. Получится то, что нужно.\\
            Хорошо, а как бороться с тождественными нулём и единицей? Да ничего, если у нас есть это в базисе, то мы справедливо не трогаем его, это функция арности 0.\\
            \begin{tikzpicture}[
                treenode/.style = {
                    inner sep=0pt,
                    text width=5mm,
                    scale=1.5,
                    circle,
                    outer sep=1.5mm,
                    align=center,
                    draw=black,
                    fill=white,
                    thin
                }
            ]
                \node[treenode] (root) at (3,0) {$\lor$};
                \node[treenode] (l) at (1,-2) {$\land$};
                \node[treenode] (ll) at (0,-4) {$\neg$};
                \node[treenode] (ll1) at (0,-6) {$x$};
                \node[treenode] (lr) at (2,-4) {$y$};
                \node[treenode] (r) at (5,-2) {$\land$};
                \node[treenode] (rl) at (4,-4) {$x$};
                \node[treenode] (rr) at (6,-4) {$\neg$};
                \node[treenode] (rr1) at (6,-6) {$y$};
                
                \draw (root) -- (l);
                \draw (root) -- (r);
                \draw (l) -- (ll);
                \draw (l) -- (lr);
                \draw (r) -- (rl);
                \draw (r) -- (rr);
                \draw (ll) -- (ll1);
                \draw (rr) -- (rr1);
            \end{tikzpicture}
            \hspace{12pt}
            \begin{tikzpicture}[
                treenode/.style = {
                    inner sep=0pt,
                    text width=5mm,
                    scale=1.5,
                    circle,
                    outer sep=1.5mm,
                    align=center,
                    draw=black,
                    fill=white,
                    thin
                }
                ]
                \node[treenode] (root) at (3,0) {$\lor$};
                \node[treenode] (l) at (1,-2) {$\land$};
                \node[treenode] (ll) at (0,-4) {$\neg$};
                \node[treenode] (ll1) at (0,-6) {$x$};
                \node[treenode] (r) at (5,-2) {$\land$};
                \node[treenode] (rr) at (6,-4) {$\neg$};
                \node[treenode] (rr1) at (6,-6) {$y$};
                
                \draw[<-] (root) -- (l);
                \draw[<-] (root) -- (r);
                \draw[<-] (l) -- (ll);
                \draw[<-] (l) -- (rr1);
                \draw[<-] (r) -- (ll1);
                \draw[<-] (r) -- (rr);
                \draw[<-] (ll) -- (ll1);
                \draw[<-] (rr) -- (rr1);
            \end{tikzpicture}
        \end{Proof}
        \begin{Comment}
            А чем, собственно, удобнее схема тогда? А тем, на самом деле, что там можно переиспользовать вычисленные значения, что даёт нам более оптимальное время. И обычно, если хочется рассматривать сложность, рассматривают именно схематическую, а не формульную.
        \end{Comment}
        \dfn Сложностью функции $f$ в базисе $A$ называется число $\operatorname{size}_A(f)$, равное минимальному количеству внутренних вершин схемы функции $f$ над базисом $A$.
        \begin{Comment}
            Иногда не рассматривают $\neg$, но ситуацию это меняет не сильно.
        \end{Comment}
        \begin{Example}
            Например, если рассмотреть $x\oplus y$ так, как нарисовано выше, то получится размер 5. Но никто не гарантирует оптимальность схемы, то есть $\operatorname{size}_{\{\land,\lor,\neg\}}(\oplus)\leqslant 5$.
        \end{Example}
        \thm Если $A$ и $B$ --- базисы, то $\exists c>0~\forall f\text{ --- булева функция}~\operatorname{size}_A(f)\leqslant c\operatorname{size}_B(f)$.
        \begin{Proof}
            Пусть $A=\{g_1;g_2;\ldots;g_n\}$, $B=\{h_1;h_2;\ldots;h_n\}$. Мы точно можем, в силу базисности $A$, можем построить схему над $A$ для каждого $h_i$. Рассмотрим схему $f$ над $B$. А значит мы совершенно точно можем вместо каждого $h_i$ в этой схеме поставить схему соответствующей $h_i$ над $A$. Если рассмотреть максимум количеств элементов в схемах $h_i$ как $c$, то мы получим, что размер полученной нами схемы не больше $c\operatorname{size}_B(f)$. Никто не гарантирует, что полученная нами схема оптимальна, но и ладно, если не оптимально, $\operatorname{size}_A(f)$ будет и того меньше, и всё равно меньше либо равен $c\operatorname{size}_B(f)$.
        \end{Proof}
        \begin{Comment}
            А вот с формулами не выполняется такая теорема. Именно поэтому формульная сложность --- это и плохо.
        \end{Comment}
        \dfn \undercolor{red}{Глубина схемы} --- это длина максимального пути в схеме.
        \begin{Comment}
            Глубина в своём роде характеризует необходимое количество времени для параллельного вычисления схемы.
        \end{Comment}
        \thm Для глубины выполнена та же самая теорема, что и для размера. Доказательство совпадает слово в слово.
        \begin{Comment}
            Окей, филиал дискретки на АрхЭВМ у нас был, а теперь давайте наоборот. Построим несколько известных штук схемами из функциональных элементов. А точнее, сложение чисел.
        \end{Comment}
        \begin{Example}
            Сначала сложим два булевских числа. Заметим, что результат может быть от 0 до 2, что значит, что нам нужно 2 бита для его представления. Обозначим старший за $c$ (от слова <<carry>>, перенос через разряд), а младший --- за $s$. Тогда схема выглядит так:
            \begin{center}\begin{tikzpicture}[circuit logic IEC]
                \node (a) at (0,.25) {};
                \node (b) at (0,-.25) {};
                \node[and gate, draw, logic gate inputs=nn, anchor=input 1] at (1.75,1) (And) {};
                \node[xor gate, draw, logic gate inputs=nn, anchor=input 1] at (1.75,-1) (Xor) {};
                \draw (a) -- ($(a)+(.75,0)$) node[branch] {} -- (And.input 1);
                \draw (a) -- ($(a)+(.75,0)$) node[branch] {} -- (Xor.input 1);
                \draw (b) -- ($(b)+(.75,0)$) node[branch] {} -- (And.input 2);
                \draw (b) -- ($(b)+(.75,0)$) node[branch] {} -- (Xor.input 2);
                \draw (And.output) -- ([xshift=0.5cm]And.output) node[above] {$c$};
                \draw (Xor.output) -- ([xshift=0.5cm]Xor.output) node[above] {$s$};
            \end{tikzpicture}\end{center}
            Полученная штука --- частичный сумматор или полусумматор.
        \end{Example}
        \begin{Example}
            Заметим про предыдущую штуку следующий факт. Там мы никогда не получаем комбинацию $c=s=1$. Поэтому мы можем не сильно меняя схему складывать три числа:
            \begin{center}\begin{tikzpicture}[circuit logic IEC]
                \node (a) at (0,.5) {};
                \node (b) at (0,0) {};
                \node (c) at (0,-.5) {};
                \node[and gate, and gate IEC symbol={$\scriptstyle\langle\hspace{2px}\rangle$}, draw, logic gate inputs=nnn, anchor=input 1] at (1.75,1.25) (Med) {};
                \node[or gate, or gate IEC symbol={$\scriptstyle\oplus$}, draw, logic gate inputs=nnn, anchor=input 1] at (1.75,-1.25) (Xor) {};
                \draw (a) -- ($(a)+(.75,0)$) node[branch] {} -- (Med.input 1);
                \draw (a) -- ($(a)+(.75,0)$) node[branch] {} -- (Xor.input 1);
                \draw (b) -- ($(b)+(.75,0)$) node[branch] {} -- (Med.input 2);
                \draw (b) -- ($(b)+(.75,0)$) node[branch] {} -- (Xor.input 2);
                \draw (c) -- ($(c)+(.75,0)$) node[branch] {} -- (Med.input 3);
                \draw (c) -- ($(c)+(.75,0)$) node[branch] {} -- (Xor.input 3);
                \draw (Med.output) -- ([xshift=0.5cm]Med.output) node[above] {$c$};
                \draw (Xor.output) -- ([xshift=0.5cm]Xor.output) node[above] {$s$};
            \end{tikzpicture}\end{center}
            Более того, мы можем третьему элементу присвоить физический смысл. Мы говорим, что это --- перенос через предыдущий разряд. Теперь это уже полный сумматор или просто сумматор. Он нам ещё понадобится.
        \end{Example}
        \begin{Example}
            А давайте теперь сложим 2 числа. $n$-битовых. Примечание. Мы хотим не универсальный код, который для любого $n$ сложит числа. Мы хотим строить свою схему для каждого $n$ по отдельности. Сначала надо понять, что вообще такое $n$-битное число, как впихнуть его в схему. Ну, как набор из $n$ двоичных значений. При этом будем обозначать за $x_0$ --- младший бит, а за $x_{n-1}$ --- старший. То есть наше число --- это $\sum\limits_{i=0}^{n-1}x_i2^i$.\\
            Хорошо. Давайте теперь посмотрим на то, как мы складываем в столбик. Ну, очень просто. Мы сначала складываем $x_0$ и $y_0$, остаток присваиваем в результат, а перенос складываем с $x_1$ и $y_1$. Делаем так до конца. Тут возникает первая идейная проблема. Когда мы складываем, например, да 32-битных числа, результат может стать 33-битным. А мы не хотим расширять числа. Поэтому этот 33-й бит мы просто игнорируем. А в процессорах в некоторых архитектурах есть специальный флаг, из которого можно получить, произошло ли в последнем сложении переполнение. Но сейчас не об этом. А о схеме. Она выглядит как-то так:
            \begin{center}\begin{tikzpicture}[circuit logic IEC]
                \node[or gate, or gate IEC symbol={$\scriptstyle\mathrm{SUM}$}, draw, logic gate inputs=nnn] at (0,0) (Sum0) {};
                \node[rotate=180, or gate, or gate IEC symbol={$\phantom{\scriptstyle\mathrm{SUM}}$}, draw, logic gate inputs=nn] at (0,0) (Sum0') {};
                \draw ($(Sum0.input 1)-(.5,0)$) node[left] {$\mathbb0$} -- (Sum0.input 1);
                \draw ($(Sum0.input 2)-(.5,0)$) node[left] {$x_0$} -- (Sum0.input 2);
                \draw ($(Sum0.input 3)-(.5,0)$) node[left] {$y_0$} -- (Sum0.input 3);
                \draw (Sum0'.input 2) -- ([xshift=0.5cm]Sum0'.input 2) node[above] {$s_0$};
                
                \node[or gate, or gate IEC symbol={$\scriptstyle\mathrm{SUM}$}, draw, logic gate inputs=nnn] at (2,-1.5) (Sum1) {};
                \node[rotate=180, or gate, or gate IEC symbol={$\phantom{\scriptstyle\mathrm{SUM}}$}, draw, logic gate inputs=nn] at (2,-1.5) (Sum1') {};
                \draw (Sum0'.input 1) -- ([xshift=0.5cm]Sum0'.input 1) node[right] {$c_1$} |- (Sum1.input 1);
                \draw ($(Sum1.input 2)-(2.5,0)$) node[left] {$x_1$} -- (Sum1.input 2);
                \draw ($(Sum1.input 3)-(2.5,0)$) node[left] {$y_1$} -- (Sum1.input 3);
                \draw (Sum1'.input 2) -- ([xshift=0.5cm]Sum1'.input 2) node[above] {$s_1$};
                
                \node[or gate, or gate IEC symbol={$\scriptstyle\mathrm{SUM}$}, draw, logic gate inputs=nnn] at (4,-3) (Sum2) {};
                \node[rotate=180, or gate, or gate IEC symbol={$\phantom{\scriptstyle\mathrm{SUM}}$}, draw, logic gate inputs=nn] at (4,-3) (Sum2') {};
                \draw (Sum1'.input 1) -- ([xshift=0.5cm]Sum1'.input 1) node[right] {$c_2$} |- (Sum2.input 1);
                \draw ($(Sum2.input 2)-(4.5,0)$) node[left] {$x_2$} -- (Sum2.input 2);
                \draw ($(Sum2.input 3)-(4.5,0)$) node[left] {$y_2$} -- (Sum2.input 3);
                \draw (Sum2'.input 2) -- ([xshift=0.5cm]Sum2'.input 2) node[above] {$s_2$};
            \end{tikzpicture}\end{center}
            Тут, кстати, совершенно бесплатно мы опять можем прибавить единичку к этой сумме, просто передав её в качестве $c_0$, который на схеме равен $\mathbb0$.\\
            Сколько тут элементов? Ну, $O(n)$. И хрен мы сделаем меньше. А сколько же тут глубина? А тоже $O(n)$. И вот это уже можно меньше.
        \end{Example}
        \dfn Полученная выше схема называется \undercolor{red}{каскадным сумматором}.
        \begin{Example}
            Каким же невероятно сложным образом можно это сделать? Сначала простоты ради скажем, что $n$ --- степень двойки. Это обычно так и есть в процессорах.\\
            В чём у нас проблема? Проблема у нас в вычислении переносов, надо как-то вычислять их быстрее. Как же? Давайте рассмотрим эту схему с точки зрения $c_i$. У нас есть начальное $c_0=0$. И в зависимости от $x_i$ и $y_i$ мы как-то преобразуем $c_i$ в $c_{i+1}$. Посмотрим, как именно. Если $x_i=y_i=0$, то $c_{i+1}$ всегда 0, независимо от $c_i$. Если $x_i=y_i=1$, то $c_{i+1}$ всегда равно 1. В противном случае $c_{i+1}=c_i$. Назовём первый вариант функцией $\mathbf k$ (kill), второй --- $\mathbf g$ (generate), а третий --- $\mathbf p$ (propagate). Заметим, что мы можем, зная $x_i$ и $y_i$ относительно легко посчитать, какая функция из этих трёх используется пля преобразования $c_i$ в $c_{i+1}$. Обозначим эту функцию за $f_i$. То есть мы знаем, что $c_{i+1}=f_i(c_i)$. А теперь заметим следующее: мы можем посчитать $c_i$ как $f_{i-1}(f_{i-2}(\ldots f_1(f_0(c_0))\cdots))$. То есть мы можем явно записать зависимость $c_i$ от $c_0$. Собственно, вот так: $\left(\bigcirc_{k=0}^{i-1}f_k\right)(c_0)$. Теперь заметим то, что композиция двух функций из набора $\{\mathbf k,\mathbf p,\mathbf g\}$ даёт функцию из этого набора. Мы можем даже сделать таблицу <<умножения>>:
            \begin{center}
                \begin{tabu}{|[1.5pt]cc|[1.5pt]c|c|c|[1.5pt]}
                    \tabucline[1.5pt]{-}
                    \multicolumn{2}{|[1.5pt]c|[1.5pt]}{\multirow{2}{*}{$f\circ g$}} & \multicolumn{3}{c|[1.5pt]}{$f$}\\
                    \cline{3-5}
                    && $\mathbf k$ & $\mathbf p$ & $\mathbf g$\\
                    \tabucline[1.5pt]{-}
                    \multicolumn{1}{|[1.5pt]c|}{\multirow{3}{*}{$g$}} & $\textbf k$ & $\textbf k$ & $\textbf k$ & $\textbf k$\\
                    \cline{2-5}
                    \multicolumn{1}{|[1.5pt]c|}{} & $\mathbf p$ & $\textbf k$ & $\textbf p$ & $\textbf g$\\
                    \cline{2-5}
                    \multicolumn{1}{|[1.5pt]c|}{} & $\mathbf g$ & $\textbf g$ & $\textbf g$ & $\textbf g$\\
                    \tabucline[1.5pt]{c}
                \end{tabu}
            \end{center}
            Это <<умножение>> можно как-нибудь уж задать логическим элементом, не суть важно, как. Главное, что за константное количество базисных функций. А теперь давайте построим двоичное дерево на этих функциях:
            \tikzset{treenode/.style={
                    inner sep=0pt,
                    minimum width=5mm,
                    scale=.8,
                    circle,
                    outer sep=2mm,
                    align=center,
                    draw=black,
                    fill=white,
                    thin
            }}
            \begin{center}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {$\scriptstyle f_0f_1f_2f_3f_4f_5f_6f_7$};
                    \node[treenode] (l) at (-4,-2) {$\scriptstyle f_0f_1f_2f_3$};
                    \node[treenode] (r) at (4,-2) {$\scriptstyle f_4f_5f_6f_7$};
                    \node[treenode] (ll) at (-6,-4) {$\scriptstyle f_0f_1$};
                    \node[treenode] (lr) at (-2,-4) {$\scriptstyle f_2f_3$};
                    \node[treenode] (rl) at (2,-4) {$\scriptstyle f_4f_5$};
                    \node[treenode] (rr) at (6,-4) {$\scriptstyle f_6f_7$};
                    \node[treenode] (lll) at (-7,-6) {$\scriptstyle f_0$};
                    \node[treenode] (llr) at (-5,-6) {$\scriptstyle f_1$};
                    \node[treenode] (lrl) at (-3,-6) {$\scriptstyle f_2$};
                    \node[treenode] (lrr) at (-1,-6) {$\scriptstyle f_3$};
                    \node[treenode] (rll) at (1,-6) {$\scriptstyle f_4$};
                    \node[treenode] (rlr) at (3,-6) {$\scriptstyle f_5$};
                    \node[treenode] (rrl) at (5,-6) {$\scriptstyle f_6$};
                    \node[treenode] (rrr) at (7,-6) {$\scriptstyle f_7$};
                    
                    \draw[<-] (root) -- (l);
                    \draw[<-] (root) -- (r);
                    \draw[<-] (l) -- (ll);
                    \draw[<-] (l) -- (lr);
                    \draw[<-] (r) -- (rl);
                    \draw[<-] (r) -- (rr);
                    \draw[<-] (ll) -- (lll);
                    \draw[<-] (ll) -- (llr);
                    \draw[<-] (lr) -- (lrl);
                    \draw[<-] (lr) -- (lrr);
                    \draw[<-] (rl) -- (rll);
                    \draw[<-] (rl) -- (rlr);
                    \draw[<-] (rr) -- (rrl);
                    \draw[<-] (rr) -- (rrr);
                \end{tikzpicture}
            \end{center}
            Несложно заметить, что тут $O(n)$ элементов и глубина $O(\log n)$. Теперь давайте запустим обратный проход по дереву. Если раньше мы, зная функции ($\mathbf k$, $\mathbf p$ или $\mathbf g$) в дочерних вершинах, считали функцию в родителе, то тут мы от родителя передадим в левого ребёнка то, что получили свыше, а в правого --- то, что получили свыше, но умножив это на то, что получили от левого. Корень будет считать, что получил $\mathbf p$, то есть нейтральную функцию. Давайте проследим, что происходит с пятой вершиной, например. Корень налево передаёт $\mathbf p$, а направо --- $\mathbf p f_0f_1f_2f_3$, то есть просто $f_0f_1f_2f_3$,  После этого правый ребёнок корня передаёт то же самое $f_0f_1f_2f_3$ налево и $f_0f_1f_2f_3f_4f_5$ налево. Родитель пятой вершины передаст в левого своего ребёнка (четвёртую вершину) $f_0f_1f_2f_3$, а в правого (пятую вершину) --- $f_0f_1f_2f_3f_4$. То есть каждой вершине остаётся лишь домножить полученное значение на свою функцию, чтобы получить итоговое преобразование $c_0$ в $c_{i+1}$. Это тоже было осуществлено за глубину $O(\log n)$. При этом мы уже знаем остаток на каждом этапе. А значит остаётся лишь к каждому $x_i$, $y_i$ и $c_i$ применить сумматор за $O(1)$, чтобы получить $i$-тый разряд ответа. Вот и логарифмическая глубина.
        \end{Example}
        \dfn Полученная штука называется \undercolor{red}{двоичным каскадным сумматором}.
        \begin{Comment}
            Несложно заметить, что двоичным каскадным сумматором можно и вычитать числа, почти не меняя его. Для этого вспомним, что мы живём в модулярной арифметике. А это значит, что разность $x$ и $y$ --- это просто сумма $x$ и ${\sim}y+1$. Причём нам даже не придётся складывать единицу отдельно от всего, мы можем просто поставить её в $c_0$. Более того, если в двоичный сумматор передать $y\oplus c_0$ вместо $y$, то мы получим, что при $c_0=0$ числа складываются, а при $c_0=1$ --- вычитаются. Таким образом мы можем управлять совершаемым действием всего одним битом.
        \end{Comment}
        \begin{Example}
            Теперь давайте научимся умножать. Складывать было сложно, умножать будет намного проще. Для этого снова вспомним, как совершаются операции в столбик. Ну, очень просто. Пусть мы умножаем $x$ на $y$. Тогда наше произведение --- это сумма $n$ чисел, каждое из которых --- это либо $0$, либо сдвинутый $x$. А точнее, если $y_i=0$, то $i$-тое число --- ноль, а если $y_i=0$, то $i$-тое число --- $x\cdot2^i$. Обозначим числа, которые нам надо сложить, за $a_i$. Несложно заметить, что $a_{ij}=x_i\land y_{j-i}$. Если $j<i$, то $a_{ij}=0$. То есть мы можем посчитать все разряды этих $n$ чисел за $O(n^2)$ элементов (элементов $\land$) и глубину $O(1)$ (все эти $\land$ не зависят друг от друга).
        \end{Example}
        \dfn То, что мы получили, называется \undercolor{red}{матричным умножителем}.
        \begin{Example}
            Остаётся лишь вопрос, как быстро складывать $n$ чисел. Очевидный вариант имеет глубину $O(\log^2n)$. Для этого сначала сложим первое число со вторым, третье --- с четвёртым, пятое --- с шестым и так далее. Получим вдвое меньше чисел. Так сделаем $\log_2n$ раз и получим то, что хотели. Надо ещё заметить, что само сложение имеет глубину $O(\log n)$, поэтому $\log n$ сложений подряд имеют глубину $O(\log^2n)$. Но можно оптимальнее.
        \end{Example}
        \begin{Example}
            В более быстром сложении $n$ чисел нам поможет элемент <<3 в 2>>. Суть его в том, что он преобразует три числа $x$, $y$ и $z$ в два других числа $s$ и $c$, сумма которых равна сумме $x$, $y$ и $z$. Это мы на самом деле уже делали, это просто полный сумматор. То есть давайте побитово сложим все разряды $x$, $y$ и $z$ и запишем сумму ($\langle x_i\,y_i\,z_i\rangle$) --- в $c$, а остаток ($x_i\oplus y_i\oplus z_i$) --- в $s$. Только надо сдвинуть $c$ на один разряд, ведь сумма должна <<переносится через разряд>>. Итого, мы получаем формулу $\substack{s_i=x_i\oplus y_i\oplus z_i\\c_{i+1}=\langle x_i\,y_i\,z_i\rangle}$. Таким образом, можно преобразовать наши $n$ чисел в 2 схемой глубины $\log_{\frac32}n$. После этого мы уже умеет складывать два числа за $O(\log n)$. Итого мы имеем глубину $O(\log n)$ и количество элементов $O(n^2)$.
        \end{Example}
        \dfn Полученный нами алгоритм называется \undercolor{red}{деревом Уоллеса}.
        \begin{Comment}
            Поговорим о том, какое количество элементов нам необходимо и достаточно для булевой функции. Оказывается, что почти для всех функций необходимо и достаточно $\frac{2^n}n$ элементов схемы (с точностью до умножению на константу). Но почему-то конкретные схемы, которые мы проходили, имели $O(n)$. И вот тут проблема между теорией общей и теорией конкретной. Если мы будем генерировать случайную функцию, то при $n\to\infty$ эту функцию с вероятностью $\to1$ нельзя будет выразить за меньшее количество элементов, чем $\frac{2^n}n$. Однако никто не может предъявить семейство, которое нельзя за меньшее построить. Тут, кстати, легко доказать нижнюю границу, а верхнюю --- сложно. То есть не так, как обычно в математике.
        \end{Comment}
        \thm Итак, докажем следующее утверждение: <<Для почти всех функций требуется $\varOmega\left(\frac{2^n}n\right)$>>. Как-то не математически звучат <<почти все функции>>. Но вообще это то же, что уже было озвучено, при $n\to\infty$ вероятность того, что для случайной выполнится, стремится к 1.
        \dfn Рассмотрим такой объект как линейная программа. Выберем базис $F$. Простоты ради выберем из одной функции (например, $\downarrow$).
        \begin{Comment}.
            На самом деле таких $F$ много. Но на самом деле <<почти все>> функции, которые ничего не сохраняют, являются базисом сами по себе.
        \end{Comment}
        Итак, пусть у нас есть какая-то функция $f(x_1;\ldots;x_n)$. Тогда линейная программа --- это набор из $t$ строчек следующего вида. $i$-тая строка выглядит как $x_{n+i}=x_a\downarrow x_b$, где $a,b\in[1:x_{n+i-1}]$. При этом говорят эта линейная программа --- это программа функции $f$, если $x_{n+t}=f$.
        \thm $f$ имеет в $F$ имеет линейную программу из $t$ строк тогда и только тогда, когда $f$ имеет схему из $t$ функциональных элементов.
        \begin{Example}
            Для $\neg$ программа выглядит как:
            \[
            x_2=x_1\downarrow x_1
            \]
            А для $\land$ --- так:
            \[\begin{split}
                x_3=x_1\downarrow x_1\\
                x_4=x_2\downarrow x_2\\
                x_5=x_3\downarrow x_4
            \end{split}\]
        \end{Example}
        \begin{Proof}
            Как перевести программу в схему, очевидно. Для каждой строчки берём новый функциональный элемент и даём ему в качестве аргументов то, что нужно.\\
            А как сделать наоборот? Да тоже легко. Рассмотрим топологическую сортировку нашей схемы. Тогда мы берём и вставляем в программу строчки именно в порядке этой сортировки.
        \end{Proof}
        \begin{Comment}
            Несложно заметить, что одной схеме можно сопоставить несколько линейных программ. Например, \[\begin{split}
                x_3=x_1\downarrow x_1\\
                x_4=x_2\downarrow x_2\\
                x_5=x_3\downarrow x_4
            \end{split}\]
            и \[\begin{split}
                x_3=x_2\downarrow x_2\\
                x_4=x_1\downarrow x_1\\
                x_5=x_4\downarrow x_3
            \end{split}\] --- это разные программы, которые порождают одну и ту же схему, просто порядок топологической сортировки разный.
        \end{Comment}
        \begin{Proof}
            Давайте возьмём $t$. Тогда у нас есть какое-то количество $\operatorname{cnt}(t)$ линейных программ ровно $t$ строк. Тогда несложно понять, что у нас количество схем (на $t$ элементов), которые мы получаем из них, не больше $\operatorname{cnt}(t)$. А количество функций меньше $\operatorname{cnt}(t)+\operatorname{cnt}(t-1)+\cdots+\operatorname{cnt}(1)$. Давайте оценим $\operatorname{cnt}(t)$. Несложно заметить, что $\operatorname{cnt}(t)=n^2+(n+1)^2+(n+2)^2+\cdots+(n+t-1)^2\leqslant(n+t)^{2t}$. Несложно заметить, что тогда сумма $\sum\limits_{i=1}^t\operatorname{cnt}(t)\leqslant t(n+t)^{2t}\leqslant(n+t)^{2t+1}$. Давайте найдём оценку на $t$, что точно не хватит на все булевы функции. То есть $(n+t)^{2t+1}<2^{2^n}$. Если мы выполним это условие, то мы получим, что зоть какая-то функция через программу размера $t$  уж точно не выражается. Но нам же надо много функций. Поэтому возьмём $\alpha\in(0;1)$, что $(n+t)^{2t+1}<\alpha2^{2^n}$. Тогда если мы возьмём $\alpha$, например, $0.01$, то у нас для $99\%$ функций не хватит программы длины $t$. Подгоним $t$ под ответ. Рассмотрим $t=\frac{c2^n}{n}$. Получим $\left(n+\frac{c2^n}{n}\right)^{2\frac{c2^n}{n}+1}<\alpha2^{2^n}$. Возьмём от обеих частей двоичный логарифм. В левой части получим $\left(2\frac{c2^n}{n}+1\right)\log_2\left(n+\frac{c2^n}{n}\right)\leqslant\left(\frac{4c2^n}n\right)\log_2\left(\frac{2c2^n}n\right)\leqslant\left(\frac{4c2^n}n\right)\log_22^n=4c2^n$. В правой --- $2^n+\log_2\alpha$. Если $c<\frac14$, то $\forall\alpha~\exists N_0~\forall n>N_0~\text{схем размера}\leqslant t\text{ достаточно, чтобы задать только}\leqslant\alpha2^{2^n}\text{ функций}$. Это же просто определение того, что предел равен $0$. То есть мы в пределе сможем выразить 0 функций, если у нас размеры схем меньше $\frac{2^n}{4n}$.
        \end{Proof}
        \thm Для любой булевой функции $n$ аргументов существует схема из функциональных элементов, содержащая не больше $\alpha\frac{2^n}n$ элементов, где $\alpha$ --- какая-то константа, зависящая от базиса, а не от функции.
        \begin{Proof}
            В нашей задаче будут появляться числа, которым мы присвоим значение потом. В предыдущем доказательстве у нас так было с $c$.\\
            Выберем $k$ последних аргументов нашей функции. Мы даже переназовём их $x_{n-k+1}=y_1$, $x_{n-k+2}=y_2$ и так далее. Теперь нарисуем таблицу размера $2^{n-k}\times2^k$, в которой столбцы озаглавлены вариантами первых $n-k$ аргументов, а строки --- последних $k$. При этом в пересечении будем писать $1$ или $0$ в зависимости от значения функции. Теперь выберем $s$ и порежем её на группы по $s$ строк. Если не делится, добьём фиктивными нулями. Несложно заметить, что мы получили $p=\left\lceil\frac{2^k}s\right\rceil$ групп. Рассмотрим $i$-тую группу и рассмотрим все её вертикали. Различных вертикалей может быть $2^s$ штук или меньше, потому что каждая вертикаль имеет высоту $s$ и содержит нули и единицы. Обозначим за $m_{ij}$ тип $j$-того столбца $i$-той группы. Под типом столбца мы обозначаем число от 0 до $2^s-1$. Давате создадим маску, в которой в $i$-том слое --- какая-то маска ($t$), а во всех остальных слоях --- нули. Назовём это $t$-маской (высоты, как не сложно заметить, $2^k$). Создадим генератор таких масок $g_{it}$, и получим функции $g_{it}(x_1;\ldots;x_k)$. Таких функций, как несложно заметить, $p2^s$. Теперь давайте заметим следующую штуку: $f(x_1;x_2;\ldots;x_k;y_1;x_2;\ldots)=\bigvee\limits_{i=1}^pg_{im_{ij}}(x_1;\ldots;x_k)$, где $j=\overline{y_1y_2\ldots y_{n-k}}$ --- число с двоичной записью $y$. Почему это правда? Ну, давайте рассмотрим конкретный набор $y$ и конкретный $i$-тый слой. Что такое в таком случае $m_{ij}$? Это <<тип>> участка таблицы с этими самыми $y$ в $i$-том слое. Что тогда такое $g_{im_{ij}}$? Это маска, которая имеет нули везде, кроме нужного слоя, а в этом слое тот самый тип вертикали, который нам и нужен. При этом заметим, что дизъюнкция с другими слоями нам ничего не меняет, потому что $g_{im_{ij}}$ там тождественно 0.\\
            Все $g_{it}$ можно реализовать за $2^kp2^s$ элементов. Чтобы собрать $f$ из $g_{it}$ мы воспользуемся следующей штукой. Для каждого набора $y$ (то есть для каждого $j$) у нас будет свой способ взять некоторые из $g_{im_{ij}}$ (те, которые нам говорит таблица истинности). Мы их берём и $p$ раз применяем $\vee$ (по формуле). То есть мы делаем это за $p2^{n-k}$. Потом мы помещаем демультиплексор, которому мы на вход даём все $y$ как индекс и все варианты собрать $f$ как значения. Понятно, что демультиплексор строится за $2^{n-k}+n-k$ элементов.\\
            Теперь давайте выбирать $k=\log_2n$. Тогда мы строим все $g$ за $\frac{n2^ss}n=2^ss$, строим $f$ --- за $\frac{2^n}s$, а демультиплексор работает за $\frac{2^n}n$. Если выбрать $s=n-2\log_2n$, то получится.
        \end{Proof}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Представление информации}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            В компьютере всё хранится в битах. Как сделать так, чтобы то, что есть в книжке, оказалось в компьютере? Нужно придумать кодирование.
        \end{Comment}
        \dfn Способ сопоставления объекту битовой строки называется \undercolor{red}{кодированием}.
        \begin{Comment}
            Теория кодирования делятся на разные области в зависимости от того, что мы хотим, точности, надёжности, секретности или чего бы то ни было ещё.\\
            Например, коды есть стандартные. Эта тема покрыта архитектурой ЭВМ, символы кодируются ASCII, Unicode, UTF-16 и кучей ещё разных способов.\\
            Есть коды, созданные для эффективной передачи информации, это сжатие. И сжатие есть общее (если мы сжимаем всё, что угодно), например сжатие Хаффмана, а есть конкретные, которые учитывают свойства того, что мы сжимаем. Картинки, например, можно сжимать с потерями и без. В наше время, кстати, текст обычно сжимают слабо, либо не сжимают вообще, потому что двухминутное видео с котиком намного больше места занимает.\\
            Есть коды избыточные. Чтобы восстанавливать ошибки, если какие-то данные потерялись. Это Ethernet, уже обсуждали на АрхЭВМ.\\
            И есть шифрование. То есть секретные коды. Они скрывают данные от несанкционированного доступа к ним.\\
            Про каждый тип нам ещё расскажут. Стандартные --- АрхЭВМ, сжатие и избыточные коды --- теория информации, шифрование --- курс криптографии. Мы же поверхностно пробежимся по этому.
        \end{Comment}
    \end{itemize}
    \subparagraph{\undercolorblack{orange}{Сжатие}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Когда мы кодируем фото и видео, мы как-то получаем данные из мира, глаз и камера почему-то видят по-разному. Потому что в камере всё становится нулями и единицами. Но это ещё не кодирование, ведь камера в 4K имеет 8 миллионов пикселей, три байта на каждый, а значит 24 Мб на кадр или 720 Мб в секунду (с частотой 30 Герц). Это прям много. И это сжимается очень сложно и специфично, поэтому мы сейчас это рассматривать не будем. Комментарий сейчас к тому, что у нас всё равно сначала происходит дискретизация, а потом мы сжимаем.
        \end{Comment}
        \begin{Comment}
            Надо немножко о формальных языках поговорить. Это будет потом, но сейчас мы всё равно чуть-чуть углубимся.
        \end{Comment}
        \dfn \undercolor{red}{Алфавитом} является непустое множество. Обычно их обозначают за $\Sigma$. $\Sigma^k$ --- это строки длины $k$, составленных из символов алфавита $\Sigma$. Множество $\bigcup_{k=0}^\infty \Sigma^k$ обозначается за $\Sigma^*$ и называется \undercolor{red}{множеством слов}, \undercolor{red}{строк} или \undercolor{red}{цепочек} $\Sigma$.
        \begin{Example}
            Если в качестве $\Sigma$ взять $\{0;1\}$, то $\Sigma^1$ --- это всё ещё $\{0;1\}$, $\Sigma^2=\{00,01,10,11\}$, а что такое $\Sigma^0$? Это множество из пустого кортежа. Его в терминах строк обычно обозначают $\varepsilon$.
        \end{Example}
        \dfn Операция $\cdot\colon\Sigma^*\times\Sigma^*\to\Sigma^*$ действует по следующим правилам: если $\alpha\in\Sigma^k$, $\beta\in\Sigma^l$, то $\alpha\cdot\beta\in\Sigma^{k+l}$ и для $1\leqslant i\leqslant k$ $(\alpha\cdot\beta)_i=\alpha_i$, а для $k+1\leqslant i\leqslant l$ $(\alpha\cdot\beta)_i=\beta_{i-k}$. Её называют \undercolor{red}{конкатенацией}.
        \thm Несложно заметить, что $\cdot$ --- ассоциативная операция с двусторонним нейтральным элементом $\varepsilon$.
        \thm Предыдущий факт значит, что $(\Sigma^*;\cdot)$ --- моноид. Более того, свободный моноид. Мы не будем говорить, что это пока, но и не важно.
        \begin{Example}
            Пусть у нас есть $\varphi\colon\Sigma^*\to\Pi^*$, где $\Sigma=\{0;1\}$, $\Pi=\{\mathrm a;\mathrm b;\mathrm c\}$. И $\varphi(0)=\mathrm a$, $\varphi(1)=\mathrm b\mathrm c$. При этом мы считаем, что $\varphi(xy)=\varphi(x)\varphi(y)$. Тогда мы по любой строке $\Sigma^*$ можем сопоставить строку $\Pi^*$.
        \end{Example}
        \dfn Условие $\varphi(xy)=\varphi(x)\varphi(y)$ говорит, что $\varphi$ --- \undercolor{red}{гомоморфизм}.
        \dfn Биективный гомоморфизм --- \undercolor{red}{изоморфизм}.
        \thm Чтобы задать гомоморфизм, достаточно задать его на символах алфавита.
        \begin{Example}
            Рассмотрим теперь наконец кодирование. Это преобразование из чего-то в $\mathbb B^2$. Например, ASCII. Есть ASCII-7 и ASCII-8, где первые 128 символов фиксированны, а остальные варьируются. И это варьирование --- codepage. Например, Windows-1251, cp866 и KOI8-R.\\
            Есть ещё Unicode (UTF-8 и UTF-16). ASCII --- это код постоянной длины (длины 8 бит), а UTF-8 имеет переменную длину, самые полезные символы имеют 8 бит, какие-то 16, 32 или вообще 64. Понятно, что с декодированием в переменной длине есть проблема.
        \end{Example}
        \dfn Код $c$ называется \undercolor{red}{однозначно декодируемым}, если он инъекция. То есть $\forall x\neq y\in\Sigma~c(x)\neq c(y)$.
        \begin{Example}
            Код, который мы рассматривали (с $\mathrm a$ и $\mathrm b\mathrm c$) однозначно декодируемый, несмотря на его переменную длину. А если взять $\varphi(0)=\mathrm{aa}$, $\varphi(1)=\mathrm a$, то однозначно декодировать не получится, несмотря на то, что это гомоморфизм.
        \end{Example}
        \dfn Коды-гомоморфизмы называются также \undercolor{red}{разделяемым}.
        \thm Совершенно очевидно, что для кода постоянной длины $c\colon\Sigma\to\mathbb B^k$ верно, что он однозначно декодируем, если $2^k\geqslant|\Sigma|$, то есть у нас хватает слов.
        \begin{Example}
            Для русского алфавита нужно 6 бит или 64 варианта. Это только если маленькие буквы. И это очень неудобно, что нам не хватает чуть-чуть. Поэтому в кодировках фиксированной длины в странном месте находится <<ё>>. В UTF-8 это пофиксили.
        \end{Example}
        \begin{Example}
            С Юникодом всё чуть сложнее. Там количество лидирующих единиц задаёт количество байтов.
        \end{Example}
        \dfn Код называется \undercolor{red}{префиксным}, если ни одно кодовое слово не является префиксом другого.
        \thm И вот такие коды уже точно однозначно декодируются.
        \begin{Proof}
            Это легко. Мы организуем наши коды в структуре данных <<бор>>. Мы делаем дерево, на рёбрах которого написаны буквы, а в листьях --- буквы. И по этому дереву можно идти вниз по символам.
        \end{Proof}
        \thm Однако, префиксный код --- это не единственное, что однозначно декодируется. Можно развернуть префиксный код, получить суффиксный, который префиксным не будет, но декодируемым --- вполне.
        \begin{Comment}
            Зачем переменная длина? Потому что мы хотим экономить длину для каких-то наших любимых символов. Вот, в русском алфавите <<Ъ>>, <<Щ>> или <<Ж>> хочется отводить побольше места, чем <<Е>> или <<П>>, ведь они встречаются чаще.\\
            Пусть в каком-то нашем тексте символ $i$ встречается $f_i$ (от слова frequency) раз. Нам хочется придумать такой код, что сумма количеств символов на длину их кода минимальна. Эту задачу можно интерпретировать двумя способами. Первый --- у нас есть большой текст (именно большой, для маленького хранение таблицы кодирования будет большим), и мы хотим закодировать его и передать. Второй --- мы в принципе исходя из статистики кодируем буквы всегда.
        \end{Comment}
        \thm Если $c\colon\Sigma\to\mathbb B^*$ --- однозначно декодируем и $l_i$ --- длина кода $i$-того символа, то $\sum_{i=1}^n2^{-l_i}\leqslant1$.
        \dfn Это неравенство называется \undercolor{red}{неравенством Крафта-Макмиллана}.
        \begin{Comment}
            Никто, кстати, не запрещает, чтобы $\Sigma$ состояло из 1 элемента, который мы кодировали бы $\varepsilon$. Но мы этого не хотим, поэтому будем считать $|\Sigma|\geqslant2$.
        \end{Comment}
        \thm Если числа $l_i$ обладают свойством $\sum_{i=1}^n2^{-l_i}\leqslant1$, то существует префиксный код с длинами кодов $l_i$.
        \begin{Comment}
            Из предыдущих двух теорем следует, что для любого не-префиксного декодируемого кода существует префиксный с такими же длинами.
        \end{Comment}
        \thm По $f_1,\ldots,f_n$ можно решить задачу о минимизации $\sum_{i=1}^nf_il_i$ жадным алгоритмом.
        \dfn Этот алгоритм называют \undercolor{red}{алгоритмом Хаффмана}.
        \begin{Proof}
            Начнём с алгоритма, он полезнее и интереснее. Итак, пусть у нас есть символы, которые встречаются какое-то количество раз. Один шаг алгоритма Хаффмана выглядит так. Ищем два самых редких символа, объединяем их в один (считая новое количество суммой количеств этих редких символов), получаем алфавит с меньшим количеством букв. У этого объединённого символа будет какой-то код, и мы присваиваем тем двум редким символам этот код + 1 и + 0.\\
            Понятно, что Хаффман строит корректный код. Но не вполне понятно, что оптимальный. Давайте докажем лемму: <<на максимальной глубине находятся два самых редких символа, причём они являются братьями>>.
            \begin{Proof}
                Понятно, что на максимальной глубине находятся хотя бы 2 каких-то листа-брата, иначе, если у листа на глубине нет братьев, то можно отрезать этот лист, переместив букву в родителя, тем самым укоротив код.
                Теперь посмотрим на то, какие это символы. Если не самые редкие, то давайте поменяем их местами с самыми редкими. Тогда сумма $\sum_{i=1}^nf_il_i$ не увеличится, потому что у символа почаще на константу уменьшится длина, а у символа пореже --- на ту же константу увеличится. Поскольку у символа почаще болше частота, сумма уменьшится.
            \end{Proof}
            Но это ещё не всё, что нам нужно доказать, что объединение кода --- это верная задумка. Ну смотрите, У нас новая сумма $\sum_{i=1}^nf_i'l_i$ будет равна $\sum_{\substack{i=1\\i\neq x,y}}^nf_il_i+(f_x+f_y)l_{xy}$. При этом $l_x=l_y=l_{xy}+1$. То есть новая сумма отличается от старой на константу $f_x+f_y$, а значит минимизировать одно --- то же самое что минимизировать другое.
        \end{Proof}
        \pagebreak
        \begin{Proof}
            Поехали в неравенство Крафта-Макмиллана. Докажем сначала второе утверждение: если числа $l_i$ обладают свойством $\sum_{i=1}^n2^{-l_i}\leqslant1$, то существует префиксный код с длинами кодов $l_i$.\\
            Давайте отсортируем $l_i$ по неубыванию. Возьмём отрезок от 0 до 1, и отложим от нуля по порядку (отсортированному) отрезочки с длинами $2^{-l_i}$. Не факт, что мы заполним весь отрезок, но по условию нам его хватит. Хочется сказать, что $0.5$ не лежит строго ни в одном отрезке. Пусть лежит. Тогда есть такое $i$, что $2^{-l_1}+\cdots+2^{-l_i}<0.5$ и $2^{-l_1}+\cdots+2^{-l_i}+2^{l_{i+1}}>0.5$. Это значит как раз то, что $0.5$ лежит в $l_{i+1}$ отрезке. Домножим оба равенства на $2^{l_{i+1}}$. Тогда мы получим $2^{l_{i+1}-l_1}+\cdots+2^{l_{i+1}-l_i}<2^{l_{i+1}-1}$ и $2^{l_{i+1}-l_1}+\cdots+2^{l_{i+1}-l_i}+1>2^{l_{i+1}-1}$. Но это странно, все слагаемые тут целые, $2^{l_{i+1}-1}$ --- тоже целое, ведь $l_{i+1}$ --- хотя бы 1. Но тогда мы прибавили 1 к целому числу, меньшему $k\in\mathbb N$ и получили число, большее $k$, не порядок. Хорошо, теперь мы можем всем штукам, которые левее $0.5$ в первый бит кода запихнуть 0, всем, кто левее --- 1, вычесть из всех $l_i$ единицу, и запуститься рекурсивно. Запускаться рекурсивно будем до тех пор, пока у нас не останется один отрезок.\\
            Теперь докажем обратное. Если в однозначно декодируемом коде длина $i$-того символа равна $l_i$, то $\sum_{i=1}^n2^{-l_i}\leqslant1$. Заметим, что после доказательства этого утверждения мы получим следующий факт. Если у нас есть какой-то код, то на длинах выполнено неравенство Крафта-Макмиллана, а значит существует префиксный код с теми же длинами, а значит для оптимального кода есть оптимальный префиксный. А оптимальный префиксный мы уже можем построить Хаффманом.\\
            Итак, доказательство. Пусть у нас есть кодовый алфавит $\{a,b\}$. Мы можем зачем-то, обозначая что-то, взять и записать $ab+aa+b$, где $ab$, $aa$ и $b$ --- это какие-то кодовые слова. Дальше больше, рассмотрим $(ab+aa+b)^L$, где степень --- это канкатенация. Вот что будет, если мы раскроем скобки для $L=2$. Получится $abab+abaa+aaab+aaaa+abb+aab+bab+baa+bb$. Поскольку у нас канкатенация не коммутативна ($ab\neq ba$) и код однозначный, мы получили 9 слагаемых. В $(ab+aa+b)^L$ мы получим $3^L$ слагаемых. Если мы все кодовые слова обозначим за $c_1,c_2,\ldots,c_k$, то $(c_1+\cdots+c_k)^L$ --- это $k^L$ слагаемых, каждое из которых --- это канкатенация каких-то $L$ (возможно, одинаковых) кодовых слов. Теперь сделаем дичь. Мы хотим подставить $\frac12$ наших кодовых слов. Почему так можно? Потому что мы пользовались только тем, что сложение дистрибутивно относительно сложения. Мы исходя из этого скобки раскрывали. Тогда в нашем примере $(ab+aa+b)^2=abab+abaa+aaab+aaaa+abb+aab+bab+baa+bb$ мы получим $(\frac14+\frac14+\frac12)^2=\frac1{16}+\frac1{16}+\frac1{16}+\frac1{16}+\frac18+\frac18+\frac18+\frac18+\frac14$. Это, вроде как, верно. Что мы получим в общем случае? $\left(\frac1{2^{l_1}}+\frac1{2^{l_2}}+\cdots+\frac1{2^{l_k}}\right)^L$.  Да это же $\left(\sum_{i=1}^n2^{-l_i}\right)^L$. А что мы получаем, если раскрыть скобки в $(c_1+\cdots+c_k)^L$? Мы получаем $k^L$ строк, длина каждой из которых не превышает $\max_{i\in[1:k]}l_i\cdot L$. Обозначим этот максимум за $m$. Обозначим ха $d_i$ количество строк из этих $k^L$, имеющих длину $i$. Тогда сумма всех этих строк --- это $\sum_{j=1}^{mL}d_i2^{-i}$. В нашем примере $d_0=d_1=0$, $d_2=1$, $d_3=d_4=4$. Заметим тот факт, что $d_i\leqslant2^i$, просто потому, что в двоичном коде есть ровно $2^i$ вариантов строки длины $i$. А это значит, что $\sum_{j=1}^{mL}d_i2^{-i}\leqslant mL$. Итого мы получили, что $\forall L~\left(\sum_{i=1}^n2^{-l_i}\right)^L\leqslant mL$. Если $\sum_{i=1}^n2^{-l_i}\leqslant 1$, то всё ОК. А если нет, то слева --- какая-то константа $z$, большая 1. И $m$ --- это тоже константа. Но $z^L$ растёт асимптотически быстрее, чем $mL$. А значит неравенство не может выполняться для любого $L$. А значит наше предположение о том, что $\sum_{i=1}^n2^{-l_i}>1$ было ложно. А значит мы доказали неравенство Крафта-Макмиллана.
        \end{Proof}
        \begin{Comment}
            Итого, мы полностью закрыли вопрос о том, что делать, если у нас сжатие без потерь, в котором каждый символ кодируется независимо. Чтобы получить более хорошее сжатие, нужно отступиться от чего-то из этого. Если мы отступаемся от независимости кодируемости символов, мы либо кодируем символы блоками, либо кодируем символы в разных местах разным образом, либо что-то ещё делаем.
        \end{Comment}
        \begin{Comment}
            Если мы рассматриваем независимо каждый символ и не учитываем то, что в русском языке чаще всего идёт согласная или то, что первая буква --- это редко <<Ы>>, то получается, что наш текст в некотором смысле неотличим от случайного источника символов. И у этого тоже есть проблема, даже если мы не кодируем все символы отдельно. Энтропийный барьер, называется. И сейчас мы рассмотрим арифметическое кодирование --- улучшение Хаффмана, которое сжимает символы все вместе, но рассматривает независимо.\\
            Про арифметическое кодирование есть мем. Оно было запатентовано. Adobe, кажется, нашли лазейку в законодательстве и таки запатентовали алгоритм. И это затормозило развитие сжатия, поскольку нельзя было сжимать изображения очень удобным для сжатия арифметическим кодированием.
        \end{Comment}
        \begin{Comment}
            Итак, как же кодировать. Мы знаем частоты встреч символов в тексте: $f_1,\ldots,f_k$, и сумма всех $f_i$ --- это длина текста $L$. Рассмотрим величины $p_i=\frac{f_i}L$. Давайте поделим отрезок $[0;1]$ на $k$ частей с длинами $p_1,p_2,\ldots,p_k$. Пусть $t_1$ --- первый символ текста --- равен, скажем, $i$. Тогда заменяем наш отрезок $[0;1]$ на $[P_i;P_{i+1}]$, где $P_i=p_1+\cdots+p_{i-1}$. И отрезок $[P_i;P_{i+1}]$ мы тоже делим на $k$ частей с длинами $p_1p_i,p_2p_i,\ldots,p_kp_i$. Теперь берём второй символ, $t_2=j$ и точно также смотрим на отрезок $[p_iP_j;p_iP_{j+1}]$, то есть аналог подотрезка $[P_j;P_{j+1}]$ для нашего уже выбранного $[P_i;P_{i+1}]$. Это наш отрезок $[p_iP_j;p_iP_{j+1}]$ мы также делим в соотношении $p_1:p_2:\cdots:p_k$ то есть на отрезки длин $p_1p_ip_j,p_2p_ip_j,\ldots,p_kp_ip_j$. Так мы делаем пока у нас не закончится текст. В итоге мы получим какой-то отрезок. Мы берём строго внутри него любое рациональное число со знаменателем, являющимся степенью двойки. Кодом нашего слова является числитель этой дроби, дополненный ведущими нулями до длины знаменателя. Например, $\frac38\mapsto011$. Как декодировать это? Ну, очень легко. Пока вам дана информация о частотах и длине текста. Вы берёте это информацию о частотах, делите всё в том же соотношении $p_1:p_2:\cdots:p_k$, смотрите, в каком отрезке лежит наша дробь, берёте его и делаете с ним то же самое. Так делаем \sout{до посинения} пока не получим $L$ символов. Давайте оценим что-нибудь. Длина нашего отрезка в конце сжатия равна $p_1^{f_1}p_2^{f_2}\cdot p_k^{f_k}$ или $p_1^{p_1L}p_2^{p_2L}\cdot p_k^{p_kL}=\left(p_1^{p_1}p_2^{p_2}\cdot p_k^{p_k}\right)^L$. Заметим, что для того, чтобы дробь с знаменателем $2^q$ была внутри нашего отрезка, достаточно, чтобы $2^{-q}$ была не больше его длины. То есть $q\geqslant-L\sum_{i=1}^kp_i\log_2p_i$. И с точки зрения теории кодирования эта величина --- это энтропия, и сделать кодирование больше её нельзя.\\
            Вот рассмотрим пример с двумя символами. Хаффман не может ничего лучше предложить, чем кодировать единичкой один и ноликом второй. А вот арифметическое кодирование может. Если один символ чаще второго в 100 раз, то арифметическое кодирование работает прекрасно.
        \end{Comment}
        \begin{Comment}
            Теперь мы рассмотрим алгоритмы, который используются сейчас во всех архиваторах и которые используют расположение символов друг относительно друга. Сначала LZ. LZ --- это семейство алгоритмов (состоящее из LZ77, LZ78, LZSS и других). Они находят повторы и говорят, что именно нужно повторить. Например, текст abacaba можно закодировать как <<abac, вернись на 4 назад и повтори 3 символа>>. Все алгоритмы семейства LZ по-разному ищут, как именно искать повторения.\\
            LZW, например, не имеет различных символов и обратных ссылок, вместо чего он строит всё сам. Как он это делает? Очень легко. Он видит в начале текста ab. И говорит, а запомню-ка я, что бывает ab. Где-нибудь потом в тексте он может найти abb, уже найти ab в словаре и добавить в словаре abb. Рассмотри строку ababbaba. Мы пишем в ответ 0 (в качестве a) и запоминаем <<2 --- ab>>. Потом находим ba, пишем в ответ 1 (это же b) и запоминаем <<3 --- ba>>. Потом у нас идёт abb, это 2 в ответ и <<4 --- abb>>. Потом bab, это 3 и <<5 --- bab>>, потом ba --- это просто 3. Итого у нас ответ 01233. Мем в том, что этот словарь не надо хранить, его можно точно также строить в процессе декодирования. Более того, можно при каком-нибудь условии сбрасывать словарь, например при превышении им какого-то фиксированного размера.\\
            Коды семейства LZ учитывают взаимное расположение букв так, что они повторяющиеся подстроки не повторяют, а ссылаются на них.
        \end{Comment}
        \begin{Comment}
            Теперь рассмотрим BWT (Burrows-Wheller Transform). Нужно сначала применить применить к тексту BWT, потом Move to front (MTF), потом Хаффман либо арифметическое кодирование. Кто такие BWT и MTF, сейчас расскажем. Берём нашу строку. В конец мы запихаем какой-нибудь терминатор --- символ, который в строке не встречается. Обычно это символ с кодом 0, но мы будем обозначать его как \$. Теперь запишем все циклические сдвиги нашего текста и отсортируем их лексикографически. Для строки abacaba получим
            \begin{enumerate}[1)]
                \item \$abacaba
                \item a\$abacab
                \item aba\$abac
                \item abacaba\$
                \item acaba\$ab
                \item ba\$abaca
                \item bacaba\$a
                \item caba\$aba
            \end{enumerate}
            И последний столбец и является результатом преобразования BWT. Утверждается, что свойство <<под строка встречается в тексте много раз>> заменяется на <<символ встречается подряд много раз или близко>>. Это логично, подстроки $\alpha$ будут рядом в отсортированных циклических сдвигах, а значит все строки вида c$\alpha$ будут иметь одинаковый символ c в своём конце.\\
            Теперь MTF. Мы берём наш алфавит и делаем так. Мы записываем в ответ код символа, а этот символ перемещаем в начало алфавита. Это даёт нам то, что подряд идущие символы превращаются в нули, а подряд идущие символы с небольшими вставками других --- в много нулей и немного единиц (если вставка из двух символов, то ещё в немного двоек). То есть в ответе получаются числа, в которых очень часто встречается что-то маленькое, что хорошо для Хаффмана и арифметического кодирования.
        \end{Comment}
    \end{itemize}
    \subparagraph{\undercolorblack{orange}{Избыточное кодирование}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Сегодня мы в каком-то смысле будем решать обратную задачу по сравнению со сжатием.\\
            Пусть у нас есть канал, куда мы отправляем 0 и 1. Но там могут происходить разные помехи, повреждения и прочий кал. Если взять русский язык, каждую десятую букву поменять на случайную, за редкими исключениями всё будет понятно. Если за окном ломают асфальт, то мы всё равно можем слышать, что говорит Андрей Сергеевич. А всё потому, что русский язык сильно избыточен. Верх этой избыточности --- алфавит ICAO, где каждой букве сопоставлены максимально различные слова, которые и передаются, чтобы всё было понятно.\\
            Итак, у нас есть кодовая последовательность. Мы будем рассматривать синхронизированную передачу, когда у нас помехи не меняют местами буквы и не меняют длину сообщения. Синхронизированную передачу можно рассматривать как то, что у нас есть тактовый генератор, согласно которому символы приходят и отправляются. Обязательно приходят и обязательно уходят, без вариантов.\\
            Итак, мы отправили что-то. И какие-то символы повредились. Обычно у нас примерно известны параметры канала. То есть мы точно знаем, что наш жёсткий диск не пихали в доменную печь, например.\\
            Заметим, что нам абсолютно не выгодно использовать код переменной длины, ведь длины символов зависят от контента, а значит при какой-нибудь ошибке символы могут склеиться, разделиться на два или ещё что похуже. Поэтому лучше рассматривать постоянную длину: $c:\Sigma\to\mathbb B^k$.\\
        \end{Comment}
        \dfn \undercolor{red}{Расстояние Хемминга} $H(x;y)$ между двумя векторами из 0 и 1 --- это количество различающихся позиций в них.
        \begin{Comment}
            То есть расстояние Хемминга между векторами $x$ и $y$ ---это количество единиц в $x\oplus y$.
        \end{Comment}
        \dfn Код $c$ \undercolor{red}{обнаруживает $d$ ошибок}, если для любых разных двух кодовых слов $a$ и $b$ верно, что $H(c(a);c(b))\geqslant d+1$.
        \begin{Comment}
            То есть мы предполагаем, что повреждено не больше, чем $d$ битов. Значит мы можем найти все штуки, код которых отличается от того, что нам пришло не сильнее, чем на $d$. Если такой есть, то он ровно один (в силу определения), а если нет --- мы нашли ошибки.
        \end{Comment}
        \thm Мы назвали $H$ громким словом <<расстояние>>. Проверим же, что это метрика.
        \begin{Proof}
            Понятно, что $H(x;y)=H(y;x)$ и $H(x;y)=0\Leftrightarrow x=y$. С третьей аксиомой метрики чуть сложнее.\\
            Рассмотрим $x$, $y$ и $z$.
            Если $x$ и $z$ где-то отличаются, то либо $x$ и $y$ отличаются в этом месте, либо $y$ и $z$. При этом отличаться в каком-то месте могут одновременно и $x$ и $y$, и $y$ и $z$. Но нам нужно доказать, что $H(x;y)+H(y;z)\geqslant H(x;z)$, это выполняется.
        \end{Proof}
        \dfn Код $c$ называется \undercolor{red}{исправляющим $d$ ошибок}, если $\forall a\neq b~H(c(a);c(b))\geqslant2d+1$.
        \begin{Comment}
            Тут мы можем, получив что-то покоцаное ($y$), найти единственный символ $b$, у которого $H(y;c(b))\leqslant d$. Такой единственный именно потому, что, если мы найдём два таких, то $H(y;c(a))\leqslant d$ и $H(y;c(b))\leqslant d$, то есть по неравенству треугольника (третьей аксиоме метрики) $H(x;y)\leqslant2d$. А это противоречие.
        \end{Comment}
        \thm Для любого $d$ существует код, исправляющий $d$ ошибок.
        \begin{Proof}
            Пусть $k$ --- минимальное целое, такое что $2^k\geqslant|\Sigma|$, где $\Sigma$ --- наш алфавит. Тогда каждому символу можно сопоставить двоичную запись его номера (длины $k$ получается код). Тогда рассмотрим другой код, в котором мы повторяем старый код $2d+1$ раз. Поскольку исходный код у всех символов различный (то есть имеет расстояние в как минимум 1), повторив его $N$ раз, получим, что коды Хемминга каждой пары символов различаются как минимум на $N$.
        \end{Proof}
        \begin{Comment}
            Выглядит как-то дороговато. Можно ли использовать поменьше символов?
        \end{Comment}
        \begin{Comment}
            Можно ли сделать код, исправляющий 1 ошибку для трёх символов, длина кодового слова которого равна 3? Давайте возьмём кодовое слово 000. У нас на расстоянии Хемминга не больше, чем 1 от него должно лежать то, что мы декодируем в него. Это слова: 000, 001, 010 и 100. Рассмотрим какое-нибудь другое слово. Например, 111. Тут мы должны декодировать в него 011, 101 и 110. А больше-то и выбрать нечего, иначе мы не сможем однозначно декодироваться.\\
            Рассмотрим $\{y\mid H(x;y)\leqslant d\}$ --- множество слов, которые декодируются в $x$, если у нас не больше, чем $d$ ошибок. Да это же шар с центром в $x$ и радиусом $d$! Обозначим его за $S(x;d)$. Если наш код исправляет $d$ ошибок, то $\forall a\neq b~S(c(a);d)\cap S(c(b);d)=\varnothing$. Будем искать объём шара радиуса $d$. Там есть центр шара, он один. Есть $k$ кодов на расстоянии 1 от центра. Ещё есть $\left(\begin{matrix}
                k\\2
            \end{matrix}\right)=C_k^2$ кодов на расстоянии 2. И так далее до $d$.
            $\left(\begin{matrix}
                n\\k
            \end{matrix}\right)=C_n^k$, если не знаете --- количество способов выбрать $k$ элементов из $n$. Оно равно $\frac{n!}{k!(n-k)!}$. Итак, объём шара получается равным $\sum\limits_{i=0}^d\left(\begin{matrix}k\\i\end{matrix}\right)$
        \end{Comment}
        \thm Если $c$ исправляет $d$ ошибок для $\Sigma:|\Sigma|=n$, и длина всех кодовых слов --- $k$, то $2^k\geqslant n\sum\limits_{i=0}^d\left(\begin{matrix}k\\i\end{matrix}\right)$.
        \begin{Proof}
            Это напрямую следует из того, что у нас есть $n$ шаров с объёмом $\sum\limits_{i=0}^d\left(\begin{matrix}k\\i\end{matrix}\right)$ каждый, которые не пересекаются.
        \end{Proof}
        \thm Следствие. Если $c$ исправляет 1 ошибку, то $2^k\geqslant n(1+k)$, то есть $n\leqslant\frac{2^k}{k+1}$.
        \begin{Comment}
            Это достаточно грубая оценка, ведь впихнуть $n$ шаров вы можете не везде, где хватает объёма, но если объёма не хватает, то впихнуть уж точно не можете.\\
            Например, код, исправляющий 1 ошибку для трёх символов, длина кодового слова которого равна 3 не существует именно поэтому, ведь $2^k=8$, $k+1=4$, а значит $n\leqslant2$, а нам хочется $n=3$.
        \end{Comment}
        \begin{Comment}
            Было бы неплохо получить оценку на $n$ с другой стороны.\\
            Рассмотрим следующий жадный алгоритм. Ему дают кодовое слово $c_1$. Какие кодовые слова точно запрещены? Все те, которые находятся в $S(c_1;2d)$. Если у нас осталось какое-то слово $c_2$ не в этом шаре, то берём и баним кодовые слова не только в $S(c_1;2d)$, но и в $S(c_2;2d)$. И так далее, пока незапрещённых слов не останется.\\
            На первом шаге мы запретили ровно $\sum\limits_{i=0}^{2d}\left(\begin{matrix}k\\i\end{matrix}\right)$ слов. После второго шага --- неизвестно, но точно не больше тоже $\sum\limits_{i=0}^{2d}\left(\begin{matrix}k\\i\end{matrix}\right)$. И если мы на шаге $n-1$ не забанили всё, то это точно значит, что $(n-1)\sum\limits_{i=0}^{2d}\left(\begin{matrix}k\\i\end{matrix}\right)<2^k$. Это условие как раз и значит, что существует код, исправляющий $d$ ошибок.\\
            Попробуем так с кодом, исправляющим 1 ошибку для трёх символов, длина кодового слова которого равна 4 (не 3). $(3-1)(1+4+6)$ меньше ли чем $2^4$? Нет. То есть у нас не гарантируется, что получится. Но давайте попробуем. Возьмём код, например 0000. Мы запретим 0001, 0010, 0100, 1000, 0011, 0101, 1001, 0110, 1010, 1100 и, разумеется, 0000. Возьмём второй код, например, 0111. Он запретит 1111, 0011, 0101, 0110, 0100, 0010, 1110, 0001, 1101, 1011. Мы не можем найти ничего ещё. Не получилось. Необходимое условие, кстати, выполнено. То есть мы всё ещё не знаем, существует ли такой код или нет. Необходимое условие работатет, достаточное --- нет.\\
            Какой вывод из этого? А такой, что в этом зазоре и работает теория избыточного кодирования. Если нам мешает упаковка шаров --- нет кода. Если жадность находит --- существует. А если не выполнено ничего из этого --- то это не так очевидно.
        \end{Comment}
        \begin{Comment}
            Приведём пример кода, который асимптотически оптимальный и исправляет 1 ошибку.\\
            Код Хемминга. Пусть размер нашего алфавита --- $2^m=n$. Давайте выпишем $m$ бит, не являющихся степенью двойки. Например, для $m=4$ не-степени 2 --- это 3, 5, 6, 7. Номер последнего бита (тут --- 7) мы берём как $k$. Как мы делаем код для какого-то символа? Те самые биты --- не степени двойки --- мы называем \textit{информационными}, а остальные --- \textit{контрольными}. Мы записываем наш номер символа в двоичной системе счисления. В информационные биты мы это двоичное число и запишем. Контрольные биты мы заполняем так: в бит с номером $2^i$ записывается такое число, чтобы $\mathrm{xor}$ всех битов, $i$-тый бит индекса которых --- единица, был равен 0. То есть $\bigoplus\limits_{p:j\text{-тый бит }p\text{ --- единица}}c[p]=0$.\\
            Пусть мы уже записали число $1011_2$. То есть третий, шестой и седьмой бит --- 1, а пятый --- 0. Что мы пишем, например, в первый бит? Такое число, чтобы $c[1]\oplus c[3]\oplus c[5]\oplus c[7]$ было равно 0. $c[3]\oplus c[5]\oplus c[7]$, как мы знаем, уже 0 (так как это $1\oplus0\oplus1$). А значит в $c[1]$ --- ноль. Так дальше с битами с номерами 2 и 4. В итоге должно получиться 0110011.\\
            Как декодировать код Хемминга? Сначала мы проверяем контрольное соотношение. Если они все верны, ошибок нет, можно просто брать информационные биты. Если контрольное соотношение нарушено, то что? У нас инвертировался какой-то бит. Мы знаем, в каких позициях нарушены контрольные соотношения. Например, если нарушилось 1 и 2, но не 4, значит у нас инвертировался тот бит, в котором 1-й и 2-й бит --- единица, в 4-й --- 0. Это третий бит. Инвертируем его, после этого всё выполняется, можно снова убирать контрольные биты.\\
            Хорошо, давайте докажем, что всё работает.
        \end{Comment}
        \thm Код Хемминга исправляет 1 ошибку.
        \begin{Proof}
            Докажем, что расстояние между любыми двумя кодами --- не меньше 3. Пусть у нас есть 2 двоичных числа --- i и j. $H(i;j)\geqslant3$, то у нас в информационных битах уже расстояние как минимум 3, дальше можно не смотреть.\\
            Если $H(i;j)=2$. Пусть они различаются в $p_1$-том и $p_2$-том информационных битах. В $p_1\oplus p_2$ есть ненулевой бит. Пусть он имеет номер $q$. Значит в $p_1$ $q$-тый бит --- единица, а в $p_2$ --- 0 (или наоборот, не суть). А значит $q$-тый контрольный бит различен для $i$ и $j$, ведь все биты, кроме $p_1$-того и $p_2$ --- одинаковые, а на $q$-тый контрольный влияет только $p_1$.\\
            Если $H(i;j)=1$, то угадайте, что такое $p$. Поскольку $p$ --- не степень двойки, двоичная запись $p$ содержит хотя бы 2 единицы, значит он хотя бы на 2 контрольных бита влияет, а значит они разные в $i$ и $j$.\\
            Прекрасно.
        \end{Proof}
        \begin{Comment}
            Чем же так хорош код Хемминга? А тем, что $k\approx m+\log m$, значит $\frac{2^k}{k+1}\approx\frac{2^{m+\log m}}{m+\log m}\approx\frac{n\log n}{\log n}=n$.
        \end{Comment}
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Комбинаторика}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
           Кто есть такой комба? Комба изучает разные \textit{комбинаторные} объектики и их взаимодейтсвия. У нас есть какие-то примитивные объекты. Числа, там, или точки на плоскости. И мы не лезем в их внутреннюю структуру. А есть графы, например, у него нам важны и вершинки, и взаимодействия. Можно вводить другие объекты. Множество, там, или массив. А можно взять массив из различных элементов. И вот когда есть какие-то взаимодействия у частей объекта --- этот объект может быть комбинаторным. Можно дать и более формальное определение, но мы начнём не с этого, начинать с этого сложно. Можно изучать кванты до ньютоновской физики, но зачем? И вот мы начнём с наивного вхгляда на комбу, а потом уже пойдём в математику.
        \end{Comment}
        \begin{Comment}
            Начнём с биологии. То есть возьмём и посмотрим на примеры, пытаясь их как-то систематизировать. По систематике Линнея.
        \end{Comment}
        \dfn \undercolor{red}{Вектор фиксированной длины}. Задана длина $n$ и какое-то множество $M$ длины $m$. И вот вектор фиксированной длины --- это элемент $M^n$.
        \begin{Comment}
            Это не очень удачное определение, ведь в математике и программировании <<длина вектора>> значит разное.
        \end{Comment}
        \thm Несложно заметить, что различных векторов фиксированной длины есть $m^n$.
        \dfn Важную роль среди векторов фиксированной длины играют \undercolor{red}{двоичные вектора}. Это когда $M=\mathbb B$.
        \begin{Comment}
            А как это изучать? Что вообще такое изучение в комбинаторике? Обычно это
            \begin{enumerate}
                \item Подсчёт количества.
                \item Перечисление.
                \item Нумерация.
            \end{enumerate}
            Количество векторов фиксированной длины мы уже сказали. А как их перечислить? Для начала надо зафиксировать $m$ и $n$. И тогда за конечное время можно-таки перечислить все. Вектора фиксированной длины можно легко перечислять в хаотическом порядке (особенно если $m$ и $n$ небольшие), а вот деревья уже сложно. Поэтому всё перечисляют в лексикографическом порядке. Что это?
        \end{Comment}
        \dfn Пусть у нас есть алфавит $\Sigma$ с линейным порядком $\leqslant$ (то есть когда $\forall x\neq y~(x\geqslant y)\oplus(x\leqslant y)$). Тогда на $\Sigma^*$ можно ввести понятие <<\undercolor{red}{лексикографический порядок}>> ($\leqslant_{lex}$ или $\leqslant$), работающий следующим образом: $\alpha\leqslant\beta$, если $\alpha$ --- префикс $\beta$ или $\exists j<\min\{|\alpha|;|\beta|\}~\alpha_j<\beta_j\land\forall i<j~\alpha_i=\beta_i$.
        \thm Лексикографический порядок --- линейный порядок на $\Sigma^*$.
        \begin{Proof}
            Рефлексивность выполнена по определению, антисимметричность --- тоже (оба условия антисимметричны), транзитивность остаётся в домашнее задание читателю. Линейность также очевидно.
        \end{Proof}
        \begin{Comment}
            Кстати, можно упростить определение лексикографического порядка, вставив в $\Sigma$ новый символ, меньший всех и руками вставить его в конец каждой строки из $\Sigma^*$.
        \end{Comment}
        \begin{Comment}
            Что ж, давайте наделим $M$ каким-нибудь линейным порядком. Если линейный порядок нам уже дан, то чилл. А он может быть не дан, мы не знаем, как сортировать цвета. Вот, красный цвет больше или меньше синего? А хрен его знает. Ну, вот что-то можно ввести. На конечных множествах (да и счётных тоже) легко ввести частичный порядок. А о том, что любое множество вообще можно упорядочить --- это теорема Цермело. Более того, она также утверждает, что можно ввекти такой порядок, чтобы в каждом множестве был минимум.\\
            Вернёмся на землю и упорядочим двоичные вектора длины 3.
            \begin{enumerate}
                \addtocounter{enumi}{-1}
                \item 000
                \item 001
                \item 010
                \item 011
                \item 100
                \item 101
                \item 110
                \item 111
            \end{enumerate}
            Кстати, лексикографический порядок --- он же на $\mathbb B^*$. А у нас $\mathbb B^3$. Так вот можно очень легко отсортировать всё, что является помдножеством $\mathbb B^*$, просто выкинув то, что нам не нужно.
        \end{Comment}
        \begin{Comment}
            Что ж, а что с нумерацией. Нас просят $k$-тый в лексикографическом порядке, а мы должны его дать. Если внимательно подумать, то это просто $m$-ичная запись числа $k$.
        \end{Comment}
        \begin{Comment}
            Вопрос: всегда ли лексикографический порядок самый хороший? Вопрос на вопрос: а что такое <<хороший>>? А вот в этом и проблема. Иногда, например, удобен код Грея, а не лексикография.\\
            С этим кодом есть два мема, которые мы узнаем, когда узнаем, что это, собственно, за код.\\
            Кстати. А почему это \textit{код}? Ну, мы объекту сопоставили двоичные строки. Вроде похоже на предыдущую тему.\\
            Итак, что же за код? Если посмотреть на 2 и 3, то их коды похожи. А если посмотрим на 3 и 4, то вообще разные числа. И это не всегда хорошо. 
        \end{Comment}
        \dfn \undercolor{red}{Кодом Грея} называется упорядочивание двоичных векторов, что два соседних вектора отличаются ровно в одном разряде
        \dfn Код Грея называется \undercolor{red}{циклическим}, если первый вектор отличается от последнего тоже в одном разряде.
        \thm Всегда существует циклический код Грея.
        \begin{Proof}
            Построим его. Одним из примеров является зеркальный код.\\
            Для векторов длины 1 код есть только один:
            \begin{enumerate}
                \addtocounter{enumi}{-1}
                \item 0
                \item 1
            \end{enumerate}
            Давайте напишем сначала его, а потом его же, но в обратном порядке. И первой половине припишем в начале ноль, а второй --- единицу:
            \begin{enumerate}
                \addtocounter{enumi}{-1}
                \item 00
                \item 01
                \item 11
                \item 10
            \end{enumerate}
            Сделаем это ещё раз.
            \begin{enumerate}
                \addtocounter{enumi}{-1}
                \item 000
                \item 001
                \item 011
                \item 010
                \item 100
                \item 101
                \item 111
                \item 110
            \end{enumerate}
            Несложно по индукции доказать, что получается именно циклический код Грея.
        \end{Proof}
        \begin{Proof}
            Так а на кой это нужно? Зачем нужны двоичные вектора? Чтобы кодировать подмножества, например. Если мы перебираем по коду Грея, соседние множества отличаются добавляением или удалением одного элемента. Это удобно.\\
            Также это позволяет избегать гонок в мультиразрядных устройствах. Пусть у нас есть какие-то состояния. И мы их зачем-то пронумеровали в порядке перехода от одного к другому. Проблема в том, что при переходе от 3 до 4 нужно переключить все разряды одновременно, чтобы не перейти случайно в какое-то другое состояние в процессе переключения. В наших компьютерах есть тактовый генератор, но нельзя встроить такой в большой дата-центр или в подводную лодку, ограничение скоростью света есть.
        \end{Proof}
        \dfn \undercolor{red}{Перестановка}. Это комбинаторный объект, на который можно много с какой стороны посмотреть. Если смотреть с точки зрения комбинаторики, то это массив длины $n$, хранящий элементы множества длины $n$, каждое по одному разу.
        \begin{Comment}
            Поскольку внутренняя структура элементов нам не интересна, не важно, состоит наша перестановка из чисел $\{1,2,3\}$ или из $\{\square,\triangle,\star\}$.
        \end{Comment}
        \begin{Comment}
            Сколько у нас таких штук? На первое место $n$ вариантов, на второе --- $n-1$, ..., на последнее --- 1. То есть их количество $n\cdot(n-1)\cdot\cdots\cdot1=n!$. Также можно получить эту формулу реккурентно: мы выбираем первым элементом один из $n$, а потом остаётсяя $n-1$ различный элемент для $n-1$ места. То есть если ответ --- это $P_n$, то $P_n=n\cdot P_{n-1}$. Чем-то это похоже на динамическое программирование.\\
            Исторически динамическое программирование --- это совсем другое, но мы не историки, по этому это у нас именно подсчёт по реккурентной формуле.
        \end{Comment}
        \begin{Comment}
            Теперь за $P_n$ обозначим сами перестановки. Отсортировать их как некоторые строки какого-то конечного алфавита мы не можем, но и ладно, у нас будет бесконечный --- $\mathbb N$. И перестановки --- это подмножество $\mathbb N^n$.\\
            Чтобы пронумеровать $P_n$, можно использовать факториальную систему счисления, когда вес $i$-того разряда --- $i!$, но это остаётся как домашнее задание читателю.
        \end{Comment}
        \dfn В перестановке \undercolor{red}{инверсией} называется пара $(i;j)~i<j\land a_i>a_j$.
        \dfn \undercolor{red}{Таблицей инверсий} называется набор множеств $b_j=|\{i\mid i<j\land a_i>a_j\}|$.
        \begin{Example}
            Вот так выглядит таблица инверсий на перестановках длины 3.
            \begin{enumerate}
                \item[123:] 000
                \item[132:] 001
                \item[213:] 010
                \item[231:] 002
                \item[312:] 011
                \item[321:] 012
            \end{enumerate}
        \end{Example}
        \dfn \undercolor{red}{Размещением} называется массив различных элементов длины $k$, элементы которого --- элементы множества длины $n$. Их количество может обозначаться $A_n^k$.
        \begin{Comment}
            Сколько их? $n\cdot(n-1)\cdot\cdots\cdot(n-k+1)$. Это обозначают $n^{\underline k}$. Или так: $\frac{n!}{(n-k)!}$.
        \end{Comment}
        \dfn $k$-той \undercolor{red}{убывающей степенью} числа $a$ называется $a^{\underline k}=a\cdot(a-1)\cdot\cdots\cdot(a-k+1)$.
        \dfn $k$-той \undercolor{red}{возрастающей степенью} числа $a$ называется $a^{\overline k}=a\cdot(a+1)\cdot\cdots\cdot(a+k-1)$.
        \begin{Example}
            Например, $n!=1^{\overline n}=n^{\underline n}$.
        \end{Example}
        \dfn \undercolor{red}{Сочетание}. Пусть дано $A$ размера $n$. Тогда $\{B\subset A\mid |B|=k\}$ --- множество сочетаний из $n$ по $k$. В русской традиции они обозначаются $C_n^k$, везде иначе --- $\left(\begin{matrix}
            n\\k
        \end{matrix}\right)$.
        \begin{Comment}
            То есть это как размещение, но сами элементы вектора не упорядочены.\\
            А как их сортировать тогда, если это не массивы? А так, что у \textit{неупорядоченных массивов} может быть каноническое линейное представление. Можно сказать, что это те и только те размещения, которые возрастают. И теперь у каждого сочетания есть способ записать его как массив, а значит есть и линейный порядок.
        \end{Comment}
        \begin{Comment}
            Давайте двумя способами посчитаем количество сочетаний. Посмотрим на размещение. Сколько размещений после сортировки дают одно и то же? $k!$. То есть $\Cnk{n}{k}=\frac{A_n^k}{k!}=\frac{n!}{(n-k)!k!}$.\\
            Мы можем либо взять $n$-ный элемент, либо нет. В первом случае мы выбираем $k-1$ из $n-1$, а во втором --- $k$ из $n-1$. То есть $\Cnk{n}{k}=\Cnk{n-1}k+\Cnk{n-1}{k-1}$. Эта формула называется треугольником Паскаля.
        \end{Comment}
        \thm $\Cnk{n}{k}=\Cnk{n}{n-k}$.
        \begin{Proof}
            Ну, очевидно, мы можем выбирать $k$ элементов, которые мы берём или же $n-k$, которые не берём. А значит количество способов выбрать $k$ и количество способов выбрать $n-k$ равны.
        \end{Proof}
        \begin{Comment}
            Давайте посмотрим на бином Ньютона: $(a+b)^n=\sum_{i=0}^n\Cnk nia^ib^{n-i}$. Откуда она берётся? А оттуда, что у нас есть $(a+b)(a+b)\cdots(a+b)$. И мы для слагаемого $a^ib^{n-i}$ можем взять штуку, которая дать именно его как раз $\Cnk ni$ способами.
        \end{Comment}
        \begin{Example}
            Рассмотрим следующее утверждение: $|A\cup B|=|A|+|B|-|A\cap B|$.\\
            Казалось бы, какая разница, мы свели мощность объединения к мощности пересечения. А на самом деле пересечение проще. Посчитать количество чисел, делящихся на 3 или 5 сложнее, чем количество чисел, делящихся на 5 и 3.
        \end{Example}
        \thm \undercolor{darkgreen}{Формула включения и исключения}.
        Пусть есть множества $A_1,A_2,\ldots,A_n$. Тогда $|A_1\cup A_2\cup\cdots\cup A_n|=\sum\limits_{\substack{I\subset[1:n]\\I\neq\varnothing}}(-1)^{|I|+1}\left|\bigcap\limits_{i\in I}A_i\right|$.
        \begin{Proof}
            По индукции, используя формулу для двух.
            \[\begin{split}
                |A_1\cup A_2\cup\cdots\cup A_n|=\\
                |(A_1\cup A_2\cup\cdots\cup A_{n-1})\cup A_n|=\\
                \sum\limits_{\substack{I\subset[1:n-1]\\I\neq\varnothing}}(-1)^{|I|+1}\left|\bigcap\limits_{i\in I}A_i\right|+|A_n|-|(A_1\cup A_2\cup\cdots\cup A_{n-1})\cap A_n|=\\
                \sum\limits_{\substack{I\subset[1:n-1]\\I\neq\varnothing}}(-1)^{|I|+1}\left|\bigcap\limits_{i\in I}A_i\right|+|A_n|-|(A_1\cup A_n)\cap(A_2\cup A_n)\cap\cdots\cap (A_{n-1}\cup A_n)|=\\
                \sum\limits_{\substack{I\subset[1:n-1]\\I\neq\varnothing}}(-1)^{|I|+1}\left|\bigcap\limits_{i\in I}A_i\right|+\sum\limits_{I=[n:n]}(-1)^{1+1}\left|\bigcap\limits_{i\in I}A_i\cap A_n\right|-\sum\limits_{\substack{I\subset[1:n-1]\\I\neq\varnothing}}(-1)^{|I|+1}\left|\bigcap\limits_{i\in I}A_i\cap A_n\right|=\\
                \sum\limits_{\substack{I\subset[1:n-1]\\I\neq\varnothing}}(-1)^{|I|+1}\left|\bigcap\limits_{i\in I}A_i\right|+\sum\limits_{I=[n:n]}(-1)^{1+1}\left|\bigcap\limits_{i\in I}A_i\cap A_n\right|-\sum\limits_{\substack{I\subset[1:n-1]\\I\neq\varnothing}}(-1)^{|I\cup\{n\}|+1}\left|\bigcap\limits_{i\in I\cup\{n\}}A_i\right|
            \end{split}\]
            Это же и есть то, что нужно.
        \end{Proof}
        \begin{Comment}
            Теперь нам хочется поговорить от том, как, если у нас пронумерованы объекты, получить по объекту номер, по номеру объект, следующий после данного объект и тому подобное. Начнём с двух примеров.\\
            Начинаем с двоичных векторов длины $n$ ($\mathbb B^n$). Они уже отсортированы в лексикографическом порядке. Вектора можно поделить на 2 равные части, те, в начале которых ноль, и те, в начале которых единица. Значит генерация всех возможны векторов состоит из двух частей --- сначала генерируем все с нулём в начале, а потом --- все с единицей. Все вектора, начинающиеся с 0 начинаются либо с 00, либо с 01, и этих тоже одинаковое количество. Аналогично с единицей. У нас получается некоторое двоичное дерево генерации наших объектов:
            \begin{center}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (l) at (-2,-2) {0};
                    \node[treenode] (r) at (2,-2) {1};
                    \node[treenode] (ll) at (-3,-4) {00};
                    \node[treenode] (lr) at (-1,-4) {01};
                    \node[treenode] (rl) at (1,-4) {10};
                    \node[treenode] (rr) at (3,-4) {11};
                    
                    \draw[->] (root) -- (l);
                    \draw[->] (root) -- (r);
                    \draw[->] (l) -- (ll);
                    \draw[->] (l) -- (lr);
                    \draw[->] (r) -- (rl);
                    \draw[->] (r) -- (rr);
                \end{tikzpicture}
            \end{center}
            Чтобы генерировать объекты, его можно обходить dfs'ом. Например, в состоянии 00 мы пытаемся приписать сначала ноль, а потом единицу. При этом мы так делаем до тех пор, пока глубина не будет равной необходимой длине вектора. То есть код можно записать на Python'е как-то так:
            \begin{minted}{python}
                def gen(p):
                    if len(p) == n:
                        print(p)
                        return
                    gen(p + [0])
                    gen(p + [1])
                
                gen([])
            \end{minted}
            Это не код, а полное говно, но это не суть важно. Важно, что эта функция генерирует вектора в лексикографическом порядке (это можно доказать по индукции).\\
            Теперь давайте в перестановки. Например, длины 3. Опять мы видим, что у нас в начале перестановки, начинающиеся с 1, потом --- с 2, потом --- с 3. Проблема в том, что, если, например, у нас в начале 1, то дальше идут не совсем перестановки двух элементов, а нечто более сложное. То есть дерево (точнее, его кусок) выглядит как-то так:
            \begin{center}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (l) at (-3,-2) {1};
                    \node[treenode] (m) at (0,-2) {2};
                    \node[treenode] (r) at (3,-2) {3};
                    \node[treenode] (ll) at (-3.75,-4) {12};
                    \node[treenode] (lr) at (-2.25,-4) {13};
                    \node[treenode] (ml) at (-.75,-4) {21};
                    \node[treenode] (mr) at (.75,-4) {23};
                    \node[treenode] (rl) at (2.25,-4) {31};
                    \node[treenode] (rr) at (3.75,-4) {32};
                    \node[treenode] (ll') at (-3.75,-6) {123};
                    \node[treenode] (lr') at (-2.25,-6) {132};
                    
                    \draw[->] (root) -- (l);
                    \draw[->] (root) -- (m);
                    \draw[->] (root) -- (r);
                    \draw[->] (l) -- (ll);
                    \draw[->] (l) -- (lr);
                    \draw[->] (m) -- (ml);
                    \draw[->] (m) -- (mr);
                    \draw[->] (r) -- (rl);
                    \draw[->] (r) -- (rr);
                    \draw[->] (ll) -- (ll');
                    \draw[->] (lr) -- (lr');
                \end{tikzpicture}
            \end{center}
            Это реализовать в коде уже сложнее, но всё ещё возможно:
            \begin{minted}{python}
                def gen(p):
                    if len(p) == n:
                        print(p)
                        return
                    for i in range(1, n + 1):
                        if not i in p:
                            gen(p + [i])
                
                gen([])
            \end{minted}
            Что-то похожее есть у этих алгоритмов. Возникает вопрос, а можно ли всё обобщать? Прежде чем это сделать, рассмотрим ещё пример.\\
            Все двоичные строки длины, не больше $n$. То есть $\bigcup\limits_{k=0}^n\mathbb B^k$. Пока не очень понятно, кто это. А вот смотрите. Самая лексикографически ранняя строка --- $\varepsilon$ (то есть пустая). А дальше все строки начинаются либо с 0, либо с 1. При этом есть одна строка, начинающаяся на 0, у которой дальше ничего нет, а остальные также имеют своим вторым знаком 0 и 1. То есть у нас получается пример, такой же как первый, но в нелистовых вершинах дерева мы также должны выводить ответ.
            \begin{minted}{python}
                def gen(p):
                    print(p)
                    if len(p) == n:
                        return
                    gen(p + [0])
                    gen(p + [1])
                
                gen([])
            \end{minted}
            Итак, пусть у нас есть произвольные комбинаторные объекты. Ну, как. Почти произвольные. Нам хочется, чтобы они были линеаризованы, то есть записанные в одну строку, чтобы мы знали, как их лексикографически сортировать. Тогда наш генератор \mintinline{python}{gen} будет принимать на вход префикс того, что мы в текущий момент генерируем. У нас есть следующий инвариант: если мы вызвали \mintinline{python}{gen(p)}, то у нас хотя бы один объект с префиксом \mintinline{python}{p} существует, то есть нет ложных вызовов. Начнём с того, что иногда мы выводили сам \mintinline{python}{p} всегда (последний пример), а иногда --- нет. Как эту идею обобщить? В каком случае мы выводим \mintinline{python}{p}? Если это готовый комбинаторный объект. Дальше мы выходим по длине. Но это так себе идея, честно говоря, вдруг у нас какие-то дополнительные условия не позволяют так сделать. Давайте делать по-другому. Если к \mintinline{python}{p} нельзя ничего приписать, то и вызвать от него \mintinline{python}{gen} бессмысленно. Совместим эту проверку с попыткой приписать. Пусть наш алфавит --- $\Sigma$. В первом и последнем примерах $\Sigma=\mathbb B$, в во втором --- $[1:n]$. Переберём элементы $\Sigma$, но не как попало, а в том порядке, как они у нас упорядочены. Напоминаю, что лексикографический порядок на $\Sigma^*$ требует некоторого строгого порядка на $\Sigma$, и именно согласно ему мы должны перебирать. Каждый символ пытаемся приписать, и если получается корректный префикс, то приписываем:
            \begin{minted}{python}
                def gen(p):
                    if valid_object(p): # Является ли p комбинаторным объектом?
                        print(p)
                    for c in Sigma: # Sigma должно быть отсортировано.
                        if valid_prefix(p + [c]):
                            gen(p + [c])
                
                gen([])
            \end{minted}
            Как это работает, например, в примере 1? Когда длина \mintinline{python}{p} меньше \mintinline{python}{n}, первый \mintinline{python}{if} будет в любом случае ложен, а второй --- истинен для любого \mintinline{python}{c}. Когда же длина \mintinline{python}{p} станет равной \mintinline{python}{n}, то будет наоборот, ведь ни одна последовательность длины \mintinline{python}{n + 1} не будет являться префиксом ни одного вектора длины \mintinline{python}{n}.\\
            Итак, этот код работает. Он, разумеется, не оптимален, поэтому в некоторых частных случаях можно его оптимизировать.
        \end{Comment}
        \begin{Comment}
            У всех программ была проблема --- мы на каждом вызове генерирующей функции создавали новый список на стеке. Это очень долго и бесполезно, давайте это оптимизируем. Можно хранить глобальный массив \mintinline{python}{a}, в котором мы и будем генерировать наши объекты, а \mintinline{python}{p} --- это длина нужного нам префикса \mintinline{python}{a}. Как тогда будет выглядеть код, например, для перестановок? Плохо. Всё ещё плохо, потому что там мы ищем в массиве \mintinline{python}{a} следующий элемент, чтобы проверять на валидность. Это плохо, поэтому можно завести множество ещё не использованных элементов. Например, при помощи \mintinline{cpp}{std::bitset} в C++ или простого массива \mintinline{python}{used} где угодно. Второй вариант выглядит так:
            \begin{minted}{python}
                def gen(p):
                    global a
                    global used
                    if p == n:
                        print(a)
                        return
                    for i in range(1, n + 1):
                        if not used[i]:
                            used[i] = True
                            a[p] = i
                            gen(p + 1)
                            used[i] = False
                
                a = [0 for _ in range(n)]
                used = [False for _ in range(n)]
                gen(0)
            \end{minted}
            Если не нужен лексикографический порядок, можно применить финт: в <<незаполненной части>> массива \mintinline{python}{a} хранить неиспользованные элементы. Тогда можно по этой части и бежать, перед каждым рекурсивным вызовом \mintinline{python}{swap}'ая \mintinline{python}{p}'тый элемент с найденным в остатке.
        \end{Comment}
        \begin{Comment}
            Теперь мы хотим научиться решать следующую задачу: у нас есть чиселко, и мы хотим, какой объект имеет такой номер в лексикографическом порядке. Во-первых, мы нумеруем с нуля. Потому что так надо. Сделаем нечто странное. Сгенерируем все объекты, пропускаем нужное количество объектов, когда пропустим --- печатаем. То есть так:
            \begin{minted}{python}
                def gen(p):
                    global a
                    global used
                    if p == n:
                        if k == 0:
                            print(a)
                        k -= 1
                        return
                    for i in range(1, n + 1):
                        if not used[i]:
                            used[i] = True
                            a[p] = i
                            gen(p + 1)
                            used[i] = False
            \end{minted}
            Полученный код будем медленным как улитка, но будет работать. Но теперь давайте посмотрим на рекурсивный вызов \mintinline{python}{gen}. Он либо выводит правильный объект, либо нет. То есть в нашем дереве:
            \begin{center}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (l) at (-3,-2) {1};
                    \node[treenode] (m) at (0,-2) {2};
                    \node[treenode] (r) at (3,-2) {3};
                    \node[treenode] (ll) at (-3.75,-4) {12};
                    \node[treenode] (lr) at (-2.25,-4) {13};
                    \node[treenode] (ml) at (-.75,-4) {21};
                    \node[treenode] (mr) at (.75,-4) {23};
                    \node[treenode] (rl) at (2.25,-4) {31};
                    \node[treenode] (rr) at (3.75,-4) {32};
                    \node[treenode] (ll') at (-3.75,-6) {123};
                    \node[treenode] (lr') at (-2.25,-6) {132};
                    
                    \draw[->] (root) -- (l);
                    \draw[->] (root) -- (m);
                    \draw[->] (root) -- (r);
                    \draw[->] (l) -- (ll);
                    \draw[->] (l) -- (lr);
                    \draw[->] (m) -- (ml);
                    \draw[->] (m) -- (mr);
                    \draw[->] (r) -- (rl);
                    \draw[->] (r) -- (rr);
                    \draw[->] (ll) -- (ll');
                    \draw[->] (lr) -- (lr');
                \end{tikzpicture}
            \end{center}
            ...которое мы обходим в глубину, рассматриваемое поддерево либо содержит наш \mintinline{python}{k}'тый элемент, либо нет. Если оно не содержит, смысла в него идти нет. Мы хотим пойти в некоторого нашего сына. Там будет всего $(\mintinline{python}{n}-\mintinline{python}{p}-1)!$ листьев. Если \mintinline{python}{k} больше этого, пропустим их все в одно действие. А иначе пойдём в этого ребёнка, как и должны. При этом после этого обхода мы найдём нужный нам лист, а значит после рекурсивного вызова можно делать \mintinline{python}{return}. А ещё в таком случае мы придём в условие \mintinline{python}{if p == n:} только тогда, когда дойдём до правильного листа, а значит нам совершенно не нудна проверка \mintinline{python}{k} на равенство нулю и уменьшение его на единицу:
            \begin{minted}{python}
                def gen(p):
                    global a
                    global used
                    if p == n:
                        print(a)
                        return
                    for i in range(1, n + 1):
                        if not used[i]:
                            if k >= factorial(n - p - 1):
                                k -= factorial(n - p - 1)
                            else:
                                used[i] = True
                                a[p] = i
                                gen(p + 1)
                                #used[i] = False  <-- Бесполезно
                                return
            \end{minted}
            Этот код можно оптимизировать ещё дальше, поэтому мы к нему ещё вернёмся. А сейчас вернёмся к общему случаю:
            \begin{minted}{python}
            def gen(p):
                if valid_object(p):
                    if k == 0:
                        print(p)
                        return
                    k -= 1
                for c in Sigma:
                    if valid_prefix(p + [c]):
                        t = count_with_prefix(p + [c])
                        # t --- количество объектов с заданным префиксом.
                        if k >= t:
                            k -= t
                        else:
                            gen(p + [c])
                            return
            \end{minted}
            В общем случае, кстати, \mintinline{python}{if k == 0:} и \mintinline{python}{k -= 1} всё ещё нужны, просто посмотрите на вектора длины не большей \mintinline{python}{n} (проблемы, когда один объект --- префикс другого).\\
            Если мы из каких-то соображений считаем \mintinline{python}{t} за $O(1)$, то работа программы в худшем случае --- $O(|\Sigma|\cdot l)$, где $l$ --- длина нужного нам объекта.\\
            Ещё заметим следующий факт. В случае перестановок \mintinline{python}{t} не только считается за $O(1)$, но ещё и всегда одинаковое для всех \mintinline{python}{i}. А это значит, что мы можем просто нацело (с округлением вниз) поделить \mintinline{python}{k} на это самое \mintinline{python}{t}, чтобы понять, на каком этапе у нас условие станет ложным. То есть можно изменить код для перестановок так:
            \begin{minted}{python}
                def gen(p):
                    global a
                    global used
                    if p == n:
                        print(a)
                        return
                    c = k // factorial(n - p - 1) + 1
                    i = # c-тое неиспользованное число
                    k = k % factorial(n - p - 1)
                    used[i] = True
                    a[p] = i
                    gen(p + 1)
            \end{minted}
            Как быстро брать \mintinline{python}{c}-тый неиспользованных элемент? Дерево отрезков/декартово дерево вам в помощь, оба они позволяют это делать за логарифм.\\
            Теперь давайте рассмотрим ситуацию, когда \mintinline{python}{t} всё-таки разное. Для этого берём сочетания. Мы генерируем $\left(\begin{matrix}
                n\\m
            \end{matrix}\right)$ и мы ищем $k$-тый объект. Сначала вспоминаем, что чтобы линейное представление сочетания было единственным, можно хранить его в канонической (отсортированной) форме. Сначала смотрим на то, как перебирать следующий элемент \mintinline{python}{c}. Понятно, что он больше предыдущего (\mintinline{python}{a[p-1]}). Также понятно, что для сохранения инварианта нужно, чтобы мы могли вставить ещё \mintinline{python}{m - p - 1} число после данного, а значит максимальное значение может быть \mintinline{python}{n - m + p + 1}. Сколько для заданного \mintinline{python}{c} будет вариантов с требуемым префиксом? У нас остаётся позиций \mintinline{python}{m - p - 1} и числа можно ставить туда не меньше, чем \mintinline{python}{n - c}. То есть это количество --- это $\left(\begin{matrix}
                \mintinline{python}{n - c}\\\mintinline{python}{m - p - 1}
            \end{matrix}\right)$
            \begin{minted}{python}
            def gen(p):
                if p == m:
                    print(a)
                    return
                for c in range((1 if p == 0 else a[p - 1] + 1), n - m + p + 2):
                    t = # Тот самый биномиальный коэффициент
                    if k >= t:
                        k -= t
                    else:
                        a[p] = c
                        gen(p + 1)
                        return
            \end{minted}
            Проблема в том, что биномиальные коэффициенты не так уж и легко считаются. Но с помощью треугольника Паскаля их можно предподсчитать. Суммарное время получается $O(nm)$ (как предподсчёт, так и сам алгоритм). То есть длина на размер алфавита, как и должно быть.
        \end{Comment}
        \begin{Comment}
            А как наоборот, получить номер по объекту? Тут уже только в целом рассмотрим. Давайте снова писать неоптимизированный код. Пусть у нас есть наш объект \mintinline{python}{z}. Тогда можно так:
            \begin{minted}{python}
                def gen(p):
                    if p == z:
                        print(k)
                    else:
                        k += 1
                    for c in Sigma:
                        if valid_prefix(p + [c]):
                            gen(p + [c])
            \end{minted}
            Тут то же самое, каждый рекурсивный вызов либо найдёт \mintinline{python}{z}, либо пропустит \mintinline{python}{t} объектов:
            \begin{minted}{python}
                def gen(p):
                    if p == z:
                        print(k)
                        return
                    else:
                        k += 1
                    for c in Sigma:
                        if valid_prefix(p + [c]):
                            t = # Количество объектов с префиксом p + [c]
                            if (p + [c]).is_prefix_of(z):
                                gen(p + [c])
                                return
                            else:
                                k += t
            \end{minted}
            Тут всё абсолютно то же самое.
        \end{Comment}\
        \begin{Comment}
            Теперь рассмотрим <<следующий комбинаторный объект>>. Тут можно воспользоваться задачей о чайнике (получить номер, увеличить на 1, получить объект), но в целом это такое себе, это медленно, а ещё там возникают огромные числа, которые тоже долго работают. Поэтому есть простой алгоритм. Рассмотрим множество всех объектов в лексикографическом порядке. Там есть наш и есть следующий. У них есть какой-то общий префикс. Временно не будем рассматривать ситуацию, когда первый --- префикс второго. Тогда за этим общим префиксом идёт какое-то символ, который у текущего (символ a) меньше, чем у следующего (символ b). Во-первых, нет объекта после нашего, у которого общий префикс больше. Во-вторых, b --- 
            минимально возможный символ после a. В-третьих, из всего, что начинается на то же префикс и потом b, наш кандидат обладает минимальным <<хвостом>>. То есть по сути говоря мы берём максимально большой префикс, который можно не изменять, потом изменяем следующий элемент минимальным образом, а в конец приписываем так мало, как возможно. Как это сделать, зависит от объекта, все эти три шага могут быть специфичными.\\
            Рассмотрим двоичный вектор. Какой максимальный префикс можно сохранить, чтобы элемент было возможно увеличить? Ну, увеличивать мы можем только нули, так что ищем последний из таковых. Как его минимальным образом увеличить? Ну, превратить этот ноль в единицу. А как после этого взять минимальный <<хвост>>? Забить его нулями. Прекрасно, зумеры изобрели increment.\\
            Теперь рассмотрим перестановки. Например, из семи элементов вот такую: 3576421. Если мы возьмём какой-то префикс (скажем, \underline{35764}21), то увеличивать следующий элемент мы можем только через перестановку всего, кроме этого префикса (так, выделенный префикс не подходит, например). То есть нужно, чтобы из всего после префикса наш следующий элемент был не максимальным. Поэтому мы не можем сохранить ни \underline{3576}421, ни \underline{357}6421, ни \underline{35}76421, а только \underline{3}576421 (то есть в алгоритме мы идём с конца и грустим, пока элементы увеличиваются). Хорошо, что дальше мы можем поставить (вместо 5)? Ну, 6, раз уж она у нас есть. После этого нужен минимальный хвост --- а это возрастающий порядок. То есть ответ --- 3612457.
        \end{Comment}
    \end{itemize}
    \subparagraph{\undercolorblack{orange}{Скобочные последовательности и их друзья}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Вообще, из скобочных последовательностей выросло что только не. Количество классов объектов, количество которых равно числам Каталана, оно огромно.\\
            Сначала скобочные последовательности надо определить. Это можно делать наивно (арифметическое выражение, из которого выкинули всё, кроме скобок), но тогда придётся понять, что такое арифметическое выражение. Это сложнее. Поэтому есть два определения.
        \end{Comment}
        \dfn \undercolor{red}{Правильными скобочными последовательностями} называются следующие последовательности из символов открывающей и закрывающей скобок:
        \begin{enumerate}
            \item Пустая строка --- П.С.П.
            \item Если \mintinline{python}{S} --- П.С.П., то \mintinline{python}{(S)} --- тоже П.С.П.
            \item Если \mintinline{python}{S1} и \mintinline{python}{S2} --- П.С.П., то \mintinline{python}{S1S2} --- тоже П.С.П.
        \end{enumerate}
        \dfn При этом говорят, что \undercolor{red}{длина правильной скобочной последовательности} --- это количество скобок одного типа в ней (а не количество скобок всего).
        \begin{Comment}
            Первое определение такое себе. По нему сложно что-то посчитать. Понято, что пустая строка у нас одна. А длины $n$ у нас может быть первый случай, а может быть второй. И вот второй случай считается плохо, потому что склеить \mintinline{python}{()()()} можно двумя способами:
            \begin{enumerate}
                \item \mintinline{python}{()()} и \mintinline{python}{()}.
                \item \mintinline{python}{()} и \mintinline{python}{()()}.
            \end{enumerate}
            Это потому, что наша грамматика неоднозначная. Зато есть концепт <<соответствующей парной скобки>>. Рассмотрим первую скобку. Она открывающая. У неё есть какая-то закрывающая. При этом между ними П.С.П. и после закрывающей тоже П.С.П. Теперь утверждается, что мы задаём последовательность единственным образом. Какая у нас формула? $C_0=1$ (есть одна последовательность длины 0). А $C_n=\sum\limits_{i=0}^{n-1}C_iC_{n-i-1}$, ведь мы можем взять $i$ последовательностей между первой парой скобок и $n-i-1$ --- после этой пары.
            \begin{center}
                \begin{tabular}{|c|c|c|c|c|c|c|}
                    \hline
                    $n$ & 0 & 1 & 2 & 3 & 4 & 5\\
                    \hline
                    $C_n$ & 1 & 1 & 2 & 5 & 14 & 42\\
                    \hline
                \end{tabular}
            \end{center}
        \end{Comment}
        \begin{Example}
            \usemintedstyle{tango}
            Скобочная последовательность длины 1 есть только одна --- \mintinline{python}{()}, длины 2 уже есть две: \mintinline{python}{(())} и \mintinline{python}{()()}, а длины 3 уже есть пять:
            \begin{enumerate}
                \item \mintinline{python}{((()))}
                \item \mintinline{python}{(()())}
                \item \mintinline{python}{(())()}
                \item \mintinline{python}{()(())}
                \item \mintinline{python}{()()()}
            \end{enumerate}
            \usemintedstyle{native}
        \end{Example}
        \begin{Comment}
            Эта последовательность --- своего рода <<отпечаток пальца чисел Каталана>>. Если вы её видите, есть ненулевая вероятность того, что где-то есть числа Каталана.\\
            Например, рассмотрим двоичные деревья у упорядоченными детьми. Сколько у нас с одной вершиной? Одно:
            \begin{center}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                \end{tikzpicture}
            \end{center}
            А с двумя? Ну, у корня есть ребёнок, он либо левый, либо правый, то есть 2:
            \begin{center}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (l) at (-2,-2) {};
                    
                    \draw (root) -- (l);
                \end{tikzpicture}
                \hspace{15px}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (r) at (2,-2) {};
                    
                    \draw (root) -- (r);
                \end{tikzpicture}
            \end{center}
            А на 3 вершины? Их 5:
            \begin{center}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (l) at (-2,-2) {};
                    \node[treenode] (r) at (2,-2) {};
                    
                    \draw (root) -- (l);
                    \draw (root) -- (r);
                \end{tikzpicture}
                \hspace{10px}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (l) at (-2,-2) {};
                    \node[treenode] (ll) at (-3,-4) {};
                    
                    \draw (root) -- (l);
                    \draw (l) -- (ll);
                \end{tikzpicture}
                \hspace{10px}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (l) at (-2,-2) {};
                    \node[treenode] (lr) at (-1,-4) {};
                    
                    \draw (root) -- (l);
                    \draw (l) -- (lr);
                \end{tikzpicture}
                \hspace{10px}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (r) at (2,-2) {};
                    \node[treenode] (rl) at (1,-4) {};
                    
                    \draw (root) -- (r);
                    \draw (r) -- (rl);
                \end{tikzpicture}
                \hspace{10px}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (r) at (2,-2) {};
                    \node[treenode] (rr) at (3,-4) {};
                    
                    \draw (root) -- (r);
                    \draw (r) -- (rr);
                \end{tikzpicture}
            \end{center}
            Если не лениться и нарисовать на 4 вершины, получим 14 штук.
        \end{Comment}
        \thm Количество деревьев на $n$ вершин равно $n$-ному числу Каталана.
        \begin{Proof}
            Самый простой способ это доказать --- привести биекцию между П.С.П. и двоичными деревьями. Посмотрим на рекурсивное определение П.С.П. У нас П.С.П. --- это кусок между первой парой скобок и кусок после неё. Давайте рекурсивно построим эту биекцию. Левым ребёнком корня подвешиваем П.С.П., которая между первой парой скобок, а правым ребёнком --- после этой пары. Аналогично наоборот.
        \end{Proof}
        \begin{Proof}
            Но биекцию бывает сложно строить, поэтому можно по индукции доказать. Пусть количество деревьев --- $T_i$. $T_1$, как мы знаем, равно 1. А $T_n$ --- это корень и $n-1$ вершина ещё, при этом из этих $n-1$ вершин можно взять $T_i$ и $T_{n-i-1}$. И та же формула получится.
        \end{Proof}
        \begin{Comment}
            А давайте рассмотрим деревья с произвольным количеством детей, но с порядком на них. Тогда на 1 или 2 вершины по 1 дереву:
            \begin{center}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                \end{tikzpicture}
                \hspace{15px}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (m) at (0,-2) {};

                    \draw (root) -- (m);
                \end{tikzpicture}
            \end{center}
            На 3 --- либо двоичный кусочек, либо бамбук:
            \begin{center}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (l) at (-1,-2) {};
                    \node[treenode] (r) at (1,-2) {};
                    
                    \draw (root) -- (l);
                    \draw (root) -- (r);
                \end{tikzpicture}
                \hspace{15px}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (m) at (0,-2) {};
                    \node[treenode] (mm) at (0,-4) {};
                    
                    \draw (root) -- (m);
                    \draw (m) -- (mm);
                \end{tikzpicture}
            \end{center}
            Деревьев с 4 вершинами --- опять 5 штук:
            \begin{center}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (l) at (-2,-2) {};
                    \node[treenode] (m) at (0,-2) {};
                    \node[treenode] (r) at (2,-2) {};
                    
                    \draw (root) -- (l);
                    \draw (root) -- (m);
                    \draw (root) -- (r);
                \end{tikzpicture}
                \hspace{10px}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (l) at (-1,-2) {};
                    \node[treenode] (r) at (1,-2) {};
                    \node[treenode] (ll) at (-1,-4) {};
                    
                    \draw (root) -- (l);
                    \draw (root) -- (r);
                    \draw (l) -- (ll);
                \end{tikzpicture}
                \hspace{10px}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (l) at (-1,-2) {};
                    \node[treenode] (r) at (1,-2) {};
                    \node[treenode] (rr) at (1,-4) {};
                    
                    \draw (root) -- (l);
                    \draw (root) -- (r);
                    \draw (r) -- (rr);
                \end{tikzpicture}
                \hspace{10px}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (m) at (0,-2) {};
                    \node[treenode] (ml) at (-1,-4) {};
                    \node[treenode] (mr) at (1,-4) {};
                    
                    \draw (root) -- (m);
                    \draw (m) -- (ml);
                    \draw (m) -- (mr);
                \end{tikzpicture}
                \hspace{10px}
                \begin{tikzpicture}[scale=.5]
                    \node[treenode] (root) at (0,0) {};
                    \node[treenode] (m) at (0,-2) {};
                    \node[treenode] (mm) at (0,-4) {};
                    \node[treenode] (mmm) at (0,-6) {};

                    \draw (root) -- (m);
                    \draw (m) -- (mm);
                    \draw (mm) -- (mmm);
                \end{tikzpicture}
            \end{center}
            А если не полениться и нарисовать на 5, получится 14 штук. То есть получается число Каталана, но не $C_n$, а $C_{n-1}$. Тут тоже можно строить биекцию. Например, между деревьями и последовательностями. Опять рекурсивно. Давайте теперь делать так: ставить скобки, а всех детей записать внутрь них. В после этого у корня (и только у него) внешние скобки откинуть, потому что у нас получатся все П.С.П., но заключённые в скобки. Например, для второго дерева на 5 мы сначала получим \mintinline{python}{((())())}, а потом откинем внешние скобки: \mintinline{python}{(())()}. Как раз из-за такого <<откидывания>> у нас и получается сдвинутая на один шаг последовательность Каталана.
        \end{Comment}
        \begin{Comment}
            Есть проблема с генерацией таких штук. Мы не умеем узнавать количество последовательностей с данным префиксом. Поэтому решим сначала такую задачу: как проверить, что данная нам скобочная последовательность верна? Считаем префиксные суммы в нашей последовательности (открывающая скобка --- $1$, закрывающая --- $-1$). Это величину называют балансом.
        \end{Comment}
        \dfn \undercolor{red}{Правильной скобочной последовательностью} называется последовательность из символов открывающей и закрывающей скобок, удовлетворяющая следующим двум свойствам:
        \begin{enumerate}
            \item На каждом префиксе открылось не меньше скобок, чем закрылось (баланс всегда неотрицательный).
            \item Количество открывающих скобок во всей строке равно количеству закрывающих (в конце баланс равен нулю).
        \end{enumerate}
        \thm Это утверждение эквивалентно первому, но доказывать мы это не будем, это просто и бессмысленно.
        \begin{Comment}
            Это позволяет нам по-другому взглянуть на П.С.П. Рассмотрим плоскость. Мы идём из начала координат по диагоналям. Если скобка открывающая --- вверх и вправо, если закрывающая --- вниз и вправо. Тогда наше определение значит, что мы никогда не переходим ниже оси абсцисс и заканчиваем наш путь на ней. Чтобы найти парную к нашей открывающей скобке, идём вдоль $Ox$, пока не упрёмся в закрывающую скобку.
        \end{Comment}
        \dfn \undercolor{red}{Путь Дика} --- это то, что мы сотворили в комментарии выше.
        \begin{Example}
            \usemintedstyle{tango}
            Вот так выглядит путь для \mintinline{python}{(()(()))()}:
            \usemintedstyle{native}
            \begin{center}
                \begin{tikzpicture}[scale=.5]
                    \draw[step=1cm,very thin] (-.5,-.5) grid (10.5,4.5);
                    \draw[very thick,->] (-.5,0) -- (10.5,0);
                    \draw[very thick,->] (0,-.5) -- (0,4.5);
                    \draw[red,thick,->] (0,0) -- (1,1);
                    \draw[red,thick,->] (1,1) -- (2,2);
                    \draw[red,thick,->] (2,2) -- (3,1);
                    \draw[red,thick,->] (3,1) -- (4,2);
                    \draw[red,thick,->] (4,2) -- (5,3);
                    \draw[red,thick,->] (5,3) -- (6,2);
                    \draw[red,thick,->] (6,2) -- (7,1);
                    \draw[red,thick,->] (7,1) -- (8,0);
                    \draw[red,thick,->] (8,0) -- (9,1);
                    \draw[red,thick,->] (9,1) -- (10,0);
                \end{tikzpicture}
            \end{center}
        \end{Example}
        \begin{Comment}
            Эти пути помогают нам посчитать всё. Пусть $a_{x,y}$ --- количество путей, которые не опускаются ниже 0, но заканчиваются не на оси абсцисс, а в $(x;y)$. Их мы можем легко посчитать. $a_{x,y}=a_{x-1,y-1}+a_{x-1,y+1}$. При этом считается, что $a_{x,-1}=0$ и $a_{x,y}$ при $y>x$ равно 0. Тогда несложно заметить, что $C_n=a_{2n,0}$.\\
            А теперь давайте посчитаем $\hat a_{x,y}$ --- количество суффиксных П.С.П., начинающихся в $(x;y)$. То есть это $a_{2n-x,y}$. Они считаются чуть ли не так же: $\hat a_{x,y}=\hat a_{x+1,y+1}+\bigg[+\hat a_{x+1,y-1}\,\mathrm{if}\,y>0\bigg]$. И при этом $\hat a_{2n,0}=1$.
        \end{Comment}
        \begin{Comment}
            И вот как эту штуку генерировать? С проверкой на корректность всё легко (в условии префиксности, конечно) --- проверим на длину, и готово. А как проверить на корректность префикса? Ну, закрывающую скобку мы можем поставить тогда, когда она первому условию не противоречит, и дело с концом. А с открывающей сложнее. Первому условию она не противоречит никогда, но, например, ставить на последнее место открывающую скобку --- плохая затея. Да и в принципе всегда плохая, когда мы не сможем в конце свести баланс закрывающих и открывающих скобок в ноль. То есть, если мы после данного префикса можем поставит все скобки закрывающие, и тогда у нас баланс станет нулевым или отрицательным, то всё хорошо. А иначе --- не префикс. Давайте не будем писать совсем уж общую функцию, а немножко её адаптируем. Сначала мы знаем, что достигнув длины \mintinline{python}{2 * n} мы уж точно ничего дальше не сгенерируем. А потом можем передавать в функцию текущую разность количеств открывающих и закрывающих скобок, чтобы её не пересчитывать:
            \begin{minted}{python}
                def gen(p, bal):
                    if len(p) == 2 * n:
                        print(p)
                        return
                    if bal + 1 <= 2 * n - len(p) - 1:
                        gen(p + "(")
                    elif bal != 0:
                        gen(p + ")")

                gen("", 0)
            \end{minted}
        \end{Comment}
        \begin{Comment}
            Перейдём к разбиениям. У нас есть число $n$. И нам нужно количество способов представить его в виде суммы некоторого количества натуральных слагаемых. При этом у нас либо важен порядок, либо нет. Если важен, то всё довольно скучно, их $2^{n-1}$. Почему? Потому что у нас есть $n$ шариков, которые надо разделить на упорядоченные непустые кучи. Так давайте втыкать разделители куч в места, куда можно. Этих мест $n-1$, и в каждое мы можем либо вставить перегородку, либо нет.\\
            Поэтому давайте рассматривать неупорядоченные разбиения. Нам хочется (как в сочетаниях) выбрать какое-то каноническое представление разбиения. По историческим причинам берут числа по убыванию.
        \end{Comment}
        \dfn \undercolor{red}{Числом разбиений} числа $n$ (обозначается $p_n$) называется, собственно, количество канонических неупорядоченных разбиений числа $n$ на слагаемые.
        \begin{Comment}
            Вот это считать не просто сложно, а отвратительно. Там участвует пентагональная теорема Эйлера и выглядит там всё как-то так: $-\sum\limits_{k=1}^\infty(-1)^k\left(p_{n-\frac{3k^2-k}2}+p_{n-\frac{3k^2+k}2}\right)$. (При этом $p_{k}$ при $k<0$ считается нулём.) Эта формула довольно быстро считается ($O(n\sqrt n)$), но мы его рассматривать не будем, оно сложное и не может быть нами доказано. Поэтому мы введём дополнительный параметр и будем считать это проще.\\ Пусть $p_{n,k}$ --- количество разбиений $n$ на слагаемые, где максимальное не больше $k$. (Тогда $p_n=p_{n,n}$.) А для $p_{n,k}$ есть реккурента: $p_{n,k}=\sum\limits_{j=1}^kp_{n-j,j}$. Это считается за куб, то есть долго. Поэтому давайте подумаем. В $p_{n,k}$ у нас первое слагаемое --- либо $k$, либо строго меньше. В первом случае имеем $p_{n-k,k}$ вариантов, а во втором --- $p_{n,k-1}$. Вот, теперь квадрат и чилл.
        \end{Comment}
        \begin{Comment}
            Разбиению соответствует такая штука как диаграмма Юнга. Пусть у нас есть представление $9=4+2+2+1$, то мы чертим такую табличку:
            \begin{center}
                \begin{tikzpicture}
                    \draw[step=1cm,very thin] (0,0) grid (4,-1);
                    \draw[step=1cm,very thin] (0,-1) grid (2,-2);
                    \draw[step=1cm,very thin] (0,-2) grid (2,-3);
                    \draw[step=1cm,very thin] (0,-3) grid (1,-4);
                \end{tikzpicture}
            \end{center}
            С ними можно делать разные прикольные вещи, чтобы получать различные интересные биекции между комбинаторными объектами. Например, её можно транспонировать:
            \begin{center}
                \begin{tikzpicture}
                    \draw[step=1cm,very thin] (0,0) grid (4,-1);
                    \draw[step=1cm,very thin] (0,-1) grid (3,-2);
                    \draw[step=1cm,very thin] (0,-2) grid (1,-3);
                    \draw[step=1cm,very thin] (0,-3) grid (1,-4);
                \end{tikzpicture}
            \end{center}
            Количество чего и как это позволяет соотнести --- домашнее задание читателю.\\
            Рассмотрим разбиение на различные слагаемые. И будем смотреть на следующее: топаем вниз пока разница строк --- единица. Например, пусть у нас $6+5+3$.
            \begin{center}
                \begin{tikzpicture}
                    \draw[step=1cm,very thin] (0,0) grid (6,-1);
                    \draw[step=1cm,very thin] (0,-1) grid (5,-2);
                    \draw[step=1cm,very thin] (0,-2) grid (3,-3);
                \end{tikzpicture}
            \end{center}
            Тогда мы рассматриваем верхние две строки, а потом разрыв между третьей и второй --- 2, а не 1.
            \begin{center}
                \begin{tikzpicture}
                    \draw[step=1cm,very thin] (0,0) grid (6,-1);
                    \node[circle,fill=red] at (5.5,-.5) {};
                    \draw[step=1cm,very thin] (0,-1) grid (5,-2);
                    \node[circle,fill=red] at (4.5,-1.5) {};
                    \draw[step=1cm,very thin] (0,-2) grid (3,-3);
                \end{tikzpicture}
            \end{center}
            И потом мы можем либо снести последнюю строку и приклеить её по одной ячейке к первым, либо наоборот. Тут можем сделать только первое действие:
            \begin{center}
                \begin{tikzpicture}
                    \draw[step=1cm,very thin] (0,0) grid (5,-1);
                    \draw[step=1cm,very thin] (0,-1) grid (4,-2);
                    \draw[step=1cm,very thin] (0,-2) grid (3,-3);
                    \draw[step=1cm,very thin] (0,-3) grid (2,-4);
                    \node[circle,fill=darkgreen] at (.5,-3.5) {};
                    \node[circle,fill=darkgreen] at (1.5,-3.5) {};
                \end{tikzpicture}
            \end{center}
            Иногда не можем сделать ни одного. И именно эта штука используется в той страшной формуле (оказывается именно для $\frac{3k^2\pm k}2$ это преобразование применить нельзя). Короче, не суть. Суть в том, что диаграмма Юнга бывает полезной.
        \end{Comment}
    \end{itemize}
    \subparagraph{\undercolorblack{orange}{Разбиение на множества}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Теперь мы хотим разбить 4 человек на 2 (необязательно равные, но непустые) неупорядоченные команды. Либо кто-то один идёт в свою уникальную команду (4 варианта), либо --- кто-то вместе с первым (3 варианта).
        \end{Comment}
        \dfn \undercolor{red}{Числа Стирлинга второго рода} $S_2(n;k)$ или $\left\{\begin{matrix}n\\k\end{matrix}\right\}$ --- это количество способов разбить $n$ человек на $k$ непустых неупорядоченных команд.
        \thm $\left\{\begin{matrix}n\\k\end{matrix}\right\}=\left\{\begin{matrix}n-1\\k-1\end{matrix}\right\}+k\left\{\begin{matrix}n-1\\k\end{matrix}\right\}$
        \begin{Proof}
            Первое слагаемое --- если первый человек один в своей команде. Второе --- если мы как-то разобьём всех остальных, а первого человека запихаем в одну из $k$ уже имеющихся команд.
        \end{Proof}
        \begin{Comment}
            Также можно построить треугольник Стирлинга второго рода, он будет выглядеть как-то так:
            \begin{center}
                \begin{tabular}{|c|ccccc|}
                    \hline
                    n\textbackslash k & 0 & 1 & 2 & 3 & 4\\
                    \hline
                    0 & 1 & 0 & 0 & 0 & 0\\
                    1 & 0 & 1 & 0 & 0 & 0\\
                    2 & 0 & 1 & 1 & 0 & 0\\
                    3 & 0 & 1 & 3 & 1 & 0\\
                    4 & 0 & 1 & 7 & 6 & 1\\
                    \hline
                \end{tabular}
            \end{center}
            Последняя строка --- это признак того, что тут откуда-то числа Стирлинга 2 рода.
        \end{Comment}
        \begin{Comment}
            А что же такое числа Стирлинга первого рода? Давайте разобьём наших людей не на команды, а в каждой команде построим из них хоровод.
        \end{Comment}
        \dfn \undercolor{red}{Числа Стирлинга первого рода} $S_1(n;k)$ или $\left[\begin{matrix}n\\k\end{matrix}\right]$ --- это количество способов разбить $n$ человек на $k$ непустых циклов.
        \thm $\left[\begin{matrix}n\\k\end{matrix}\right]=\left[\begin{matrix}n-1\\k-1\end{matrix}\right]+(n-1)\left[\begin{matrix}n-1\\k\end{matrix}\right]$
        \begin{Proof}
            тут всё то же самое. Если человек стоит в цикле один, он один. А если нет, то его можно поставить после любого человека.
        \end{Proof}
        \begin{Comment}
            \begin{center}
                \begin{tabular}{|c|ccccc|}
                    \hline
                    n\textbackslash k & 0 & 1 & 2 & 3 & 4\\
                    \hline
                    0 & 1 & 0 & 0 & 0 & 0\\
                    1 & 0 & 1 & 0 & 0 & 0\\
                    2 & 0 & 1 & 1 & 0 & 0\\
                    3 & 0 & 2 & 3 & 1 & 0\\
                    4 & 0 & 6 & 11 & 6 & 1\\
                    \hline
                \end{tabular}
            \end{center}
            Последняя строка опять является признаком того, что тут откуда-то числа Стирлинга 1 рода.
            Давайте рассмотрим, например, $t^{\overline4}=t(t+1)(t+2)(t+3)=t^4+6t^3+11t^2+6t$. Что-то знакомое. И действительно, $t^{\overline n}=\sum\limits_{k=0}^n\left[\begin{matrix}n\\k\end{matrix}\right]t^k$. Это можно доказать по индукции.\\
            А если наоборот, выражать $t^k$ через $t^{\underline l}$. Тогда $t^4=t^{\underline4}+6t^{\underline3}+7t^{\underline2}+t^{\underline1}$. И опять же, $t^n=\sum\limits_{k=0}^n\left\{\begin{matrix}n\\k\end{matrix}\right\}t^{\underline k}$.\\
            Передаём привет линейной алгебре. У нас есть многочлены степени не выше $n$. У них можно выбрать базис (размерности $n+1$). Мы бы выбрали базис $\{1,t,t^2,\ldots,t^n\}$. А можно выбрать базис $\{1,t^{\underline 1},t^{\underline 2},\ldots,t^{\underline n}\}$ или $\{1,t^{\overline 1},t^{\overline 2},\ldots,t^{\overline n}\}$. Тогда наши треугольники Стирлинга --- это матрицы перехода от стандартного базиса к одному из этих двух. Поэтому они всоего рода взаимно обратные.
        \end{Comment}
    \end{itemize}
    \pagebreak
    \paragraph{\undercolorblack{orange}{Перестановки}}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Мем в том, что массиву из $n$ чисел можно придать некоторый дополнительный смысл, и она будет обретать новые свойства.\\
            Что такое перестановка? Ну, мы как-то расположили $n$ уникальных объектов, и получаете перестановку. Вот есть у вас перестановка $[3,1,4,2,5]$. Вы можете сказать, что тогда мы переставляем первый элемент на третье место, второй --- на первое, третий --- на четвёртое, четвёртый --- на второе, а пятый --- на пятое. Мы получили ориентированный граф --- граф перестановки.
        \end{Comment}
        \dfn Если $P$ --- перестановка, то \undercolor{red}{граф перестановки} --- это граф на вершинах $[1:n]$, где рёбра есть только между $i$ и $P_i$.
        \thm Несложно заметить, что этот граф --- это просто набор циклов, потому что из каждой вершины выходит одно ребро и входит одно.
        \begin{Comment}
            Иногда удобно использовать циклическую запись перестановки. Нашу перестановку можно записать так: $(1342)(5)$. Или так: $(3421)(5)$. Или вообще так: $(5)(2134)$. Хочется как-то канонизировать это. Обычно это делают так: циклы упорядочивают по возрастанию максимальных элементов, а эти самые максимальные элементы пишут первым элементом каждого цикла. Это удобно тем, что тогда в записи можно не ставить скобки. У нас это --- $(4213)(5)$. Мы просто смотрим на первый элемент --- 4. Понятно, что всё до $5$ лежит в том же цикле, что и 4, иначе этот цикл был бы раньше, чем цикл с 4.
        \end{Comment}
        \begin{Comment}
            Чем ещё интересен граф перестановок? Тем, что он совершает некоторые <<действия>>, у нас есть элементы, которые куда-то переходят. Для <<действий>> определена композиция, то есть сначала выполнение первого <<действия>>, а потом --- второго. Пока не очень понятно, чем <<действие>> отличается от отображения, но мы ещё это обсудим. К чему тут вообще отображения? А к тому, чтобы понять, как записывать композицию <<действий>>. Есть два варианта, но сейчас побеждает функциональная нотация: $\sin\cos x$ --- это сначала применение косинуса, а потом --- синуса. Так же пишут и с <<действиями>>, $ba\text{ массив}$ --- это сначала применение к массиву $a$, а потом --- $b$. Так что же такое $ba\text{ массив}$? Это элемент $i$ сначала переходит в $a[i]$, а потом --- $b[a[i]]$.
        \end{Comment}
        \dfn \undercolor{red}{Произведением перестановок} называется их композиция как <<действий>>.
        \thm Композиция перестановок --- перестановка.
        \begin{Proof}
            Пусть $c=ab$. Понятно, что в применении $c$ к массиву будут все элементы от 1 до $n$. Почему? Ну, после применения $b$ будут все, и после $a$ --- тоже. Значит они будут все. А значит, раз их $n$, то все и по одному разу.
        \end{Proof}
        \begin{Comment}
            То есть композиция действий --- это какая-то бинарная операция на множестве перестановок. Пока не объясняя почему, обозначим это множество как $S_n$ (но вообще от слова <<symmetric>>). А композицию будет обозначать точкой, когда нам это зачем-то понадобится. Когда не понадобится, оставим $ba$.
        \end{Comment}
        \dfn Множество с бинарной операцией на нём --- \undercolor{red}{группоид}.
        \thm Как мы уже поняли, $\cdot\colon S_n^2\to S_n$. Докажем, что эта операция ассоциативна.
        \begin{Proof}
            Ну, блин, у вас $abc$ --- это в любом случае применение сначала $c$, потом $b$, а затем --- $c$. Вне зависимости от того, $(ab)c$ у вас или $a(bc)$.
        \end{Proof}
        \dfn Если операция группоида ассоциативна --- это \undercolor{red}{полугруппа}.
        \thm Полугруппа $(S_n;\cdot)$ не коммутативна.
        \begin{Proof}
            Возьмём две какие-то перестановки. Например, $a=[4,3,1,2]$ и $b=[3,1,4,2]$. Тогда $ba=[2,3,4,1]$, а $ab=[1,4,2,3]$. Что-то не одно и то же.
        \end{Proof}
        \dfn Если в полугруппе есть двусторонний нейтральный элемент --- она \undercolor{red}{моноид}.
        \thm $(S_n;\cdots)$ --- моноид.
        \begin{Proof}
            Такой элемент правда существует, это $[1,2,\ldots,n]$. Проверить то, что это двусторонний нейтральных элемент, может даже трёхлетний ребёнок.
        \end{Proof}
        \dfn Моноид, в котором у каждого элемента есть двусторонний обратный, --- \undercolor{red}{группа}.
        \thm $(S_n;\cdots)$ --- группа.
        \begin{Proof}
            Мы хотим для каждой перестановки $a$ найти такой элемент $b$, что $ab=ba=[1,2,\ldots,n]$. Это можно сделать очень легко, зная граф перестановки. Мы берём наш граф, обращаем все рёбра и получаем, что нам нужно.
        \end{Proof}
        \begin{Example}
            Рассмотрим перестановку $[3,1,2,5,4]$. Там есть цикл $1\to2\to3\to1$ и цикл $4\to5\to4$. Так развернём же их и получим перестановку $[2,3,1,5,4]$.
        \end{Example}
        \begin{Comment}
            У нас наш цикл $4\to5\to4$ перешёл сам в себя. С чего бы это? А с того, что у него длина 2. И была бы длина 1, тоже сработало бы. То есть вполне существуют перестановки, обратные сами себе.
        \end{Comment}
        \dfn Перестановка, обратная сама себе, называется \undercolor{red}{инволюцией}.
        \dfn Наша $(S_n;\cdot)$ называется \undercolor{red}{группой перестановок} или \undercolor{red}{симметрической группой}.
        \begin{Example}
            Помните, у нас были целые числа и отношение эквивалентности равенства по модулю $n$: $\mathbb Z_{\equiv_n}$. Довольно очевидно, что это множество по сложению --- группа. А также несложно заметить, что в этой группе $a_1\equiv_na_2$ и $b_1\equiv_nb_2$, то и $a_1+b_1\equiv_na_2+b_2$.
        \end{Example}
        \dfn Отношение эквивалентности, согласованное с некоторой алгебраической операцией, называется \undercolor{red}{отношением конгруэнтности}.
        \begin{Example}
            Рассмотрим, $\mathbb Z_{\equiv_2}$. Тогда у нас получается носитель группы $\mathbb B$ и групповая операция $\oplus$. Более того, это обобщается на побитовый xor: $(\mathbb Z_+;\oplus)$.
        \end{Example}
        \dfn Группа $(\mathbb Z_+;\oplus)$ называется \undercolor{red}{ним-группой}.
        \begin{Comment}
            А что такое группа с точки зрения программиста? Ну, у нас есть какие-то свойства. Чтобы их проверить, нам нужно уметь вычислять групповую операцию. Давайте посмотрим табличку, где по вертикали и горизонтали будут отложены элементы, а в табличке были произведения элементов.
        \end{Comment}
        \dfn Таблица, о которой сказано выше --- \undercolor{red}{таблица Кэли}.
        \begin{Example}
            Вот так выглядит таблица для $\mathbb Z_{\equiv_4}$ и сложения.
            \begin{center}
                \begin{tabular}{|c|cccc|}
                    \hline
                    &0&1&2&3\\
                    \hline
                    0&0&1&2&3\\
                    1&1&2&3&0\\
                    2&2&3&0&1\\
                    3&3&0&1&2\\
                    \hline
                \end{tabular}
            \end{center}
            А можно рассмотреть тот же носитель группы, но другую операцию, например, $\oplus$. И тогда мы получим
            \begin{center}
                \begin{tabular}{|c|cccc|}
                    \hline
                    &0&1&2&3\\
                    \hline
                    0&0&1&2&3\\
                    1&1&0&3&2\\
                    2&2&3&0&1\\
                    3&3&2&1&0\\
                    \hline
                \end{tabular}
            \end{center}
            Понятно, что это разные группы, например, во второй каждый элемент обратен сам себе, а в первой --- нет.
        \end{Example}
        \begin{Comment}
            Или таблица умножения.\\
            Понятно, что сложно построить таблицу для бесконечной группы, но для конечной всё хорошо. Кроме того, что это таблица для группоида, а нам надо бы свойства проверять. Посмотрим на наши примеры. Хочется сказать, что в каждой строке и каждом столбце стоит перестановка элементов группы. Почему? Ну, если там не может быть дубликатов, иначе $ab=ac$, а это плохо, к этому равенству можно применить $a^{-1}$ и получить $b=c$.
        \end{Comment}
        \begin{Comment}
            Давайте рассмотрим множество ${0,1,2,3,4,5,6,7}$ и сложение по модулю 8. Если мы возьмём подмножество ${0,2,4,6}$ с той же операцией, то у нас, как ни странно, снова получится группа. Ассоциативность мы унаследовали, нейтральный элемент с собой взяли, обратный у нас есть, это можно проверить, вот и хорошо. 
        \end{Comment}
        \dfn Если $(G;\star)$ --- группа, а $H\subset G$ и $(H;\star)$ --- также группа, что $(H;\star)$ --- \undercolor{red}{подгруппа} $(G;\star)$.
        \thm \undercolor{darkgreen}{Теорема Кэли}. Любая конечная группа изоморфна некоторой подгруппе группы перестановок.
        \begin{Proof}
            Рассмотрим нашу конечную группу $G$ на $n$ элементов. Рассмотрим $S_n$. Это избыточно, но сойдёт. Возьмём наши элементы $G$ и перенумеруем их: $g_1,g_2,\ldots,g_n$. Рассмотрим таблицу Кэли. Что стоит на $i$-той строке? $g_ig_1,g_ig_2,\ldots,g_ig_n$. Это (как мы уже объясняли) --- некоторая перестановка элементов $G$. Именно эту перестановку мы сопоставим элементу $g_i$.\\
            Проверим, что это изоморфизм. Ещё надо бы проверить, что выбранные нами $n$ перестановок --- подгруппа $S_n$, но когда мы докажем изоморфизм, у нас это само вылезет. Начнём с единичного элемента $g_k$. Несложно заметить, что тогда этому элементу сопоставлена перестановка $g_1,g_2,\ldots,g_n$, то есть тождественная перестановка, как и должно быть. Теперь посмотрим на ассоциативность. У нас была строка $g_ig_1,g_ig_2,\ldots,g_ig_n$ и строка $g_jg_1,g_jg_2,\ldots,g_jg_n$. Строка для перемножения этих элементов $g_k=g_ig_j$ --- $g_kg_1,g_kg_2,\ldots,g_kg_n$. Пусть $g_ig_1,g_ig_2,\ldots,g_ig_n$ сопоставлена перестановка $\pi$, $g_jg_1,g_jg_2,\ldots,g_jg_n$ --- $\sigma$. Но тогда $g_kg_1,g_kg_2,\ldots,g_kg_n$ --- это $g_ig_{\sigma_1},g_ig_{\sigma_2},\ldots,g_ig_{\sigma_n}$ или $g_{\pi_{\sigma_1}},g_{\pi_{\sigma_2}},\ldots,g_{\pi_{\sigma_n}}$. Так $\pi_{\sigma_t}$ --- это и есть произведение $\pi$ и $\sigma$ (мы сначала применяем $\sigma$, а потом --- $\pi$), то есть то, что нам и нужно.
        \end{Proof}
        \dfn То, что мы только что построили --- \undercolor{red}{изоморфизм Кэли}.
        \begin{Comment}
            Вообще говоря, а $S_n$ же тоже конечная группа. А значит мы можем провести изоморфизм между $S_n$ и некоторым подмножеством $S_{n!}$. То есть мы вообще не нашли что-то оптимальное. Это красивый математический факт, но с вычислительной точки зрения это \mars ASS\mars, а не что-то полезное. То есть вы можете заставить людей вкладывать их группу в группу перестановок, но они будут очень недовольны.
        \end{Comment}
        \begin{Comment}
            А ещё на перестановки можно как на матрицы смотреть. Страшно? Мне да.\\
            Мы сделали очень неэффективный шаг, мы вложили группу размера $n$ в группу размера $n!$. Но мы оперировали простыми штуками --- перестановками. А можно оперировать более громоздкими штуками, но оперировать проще.
        \end{Comment}
        \dfn Матричным представлением перестановки $\pi$ называется матрица $A_\pi$, у которой единицы стоят в $a_{\pi_ii}$, а в остальных местах --- нули.
        \begin{Example}
            Так, для перестановки $[3,1,2,5,4]$ получим
            \[
            \left(\begin{matrix}
                0 & 1 & 0 & 0 & 0\\
                0 & 0 & 0 & 0 & 0\\
                1 & 0 & 0 & 0 & 0\\
                0 & 0 & 0 & 0 & 1\\
                0 & 0 & 0 & 1 & 0
            \end{matrix}\right)
            \]
        \end{Example}
        \begin{Comment}
            Заметим, что наша матрица и является <<действием>> перестановки. Потому что умножив её на $\matr{1\\2\\3\\4\\5}$, получим $\matr{2\\3\\1\\5\\4}$. А что тут, собственно, хорошего? Ну, единица переехала на третье место, двойка --- на первое и так далее, как и надо.
        \end{Comment}
        \thm $A_{\pi\sigma}=A_\pi A_\sigma$.
        \dfn \undercolor{red}{Циклический класс перестановки} --- это количество и длины её циклов. (См. пример.)
        \begin{Example}
            Например, если в перестановке есть 3 цикла длины 1 и один цикл длины 4, то класс обозначается как $C_1^3C_4$.
        \end{Example}
        \begin{Comment}
            Что будет, если мы будем перестановку умножать саму на себя? Ну, у нас циклы будут сдвигаться. Причём так, как было с композицией отношений в начале курса. У нас появляются рёбра между теми вершинами, которые были соединены последовательностью $k$ рёбер. Кстати, если у нас был цикл длины $n$, а мы возводим его в $m$-тую степень, он разделится на $\text{НОД}(n;m)$ циклов одинаковой длины. А значит, чтобы возвести перестановку в степень $m$ и получить тождественную, нужно, чтобы $k$ было равно наименьшему общему кратному длин всех циклов данной перестановки.
        \end{Comment}
        \begin{Comment}
            Итак, мы интерпретируем перестановку как способ изменить порядок предметов. То есть если у нас есть массив $[x_1;x_2;\ldots;x_n]$ и перестановка $\pi[\pi_1;\pi_2;\ldots;\pi_n]$. Тогда применив нашу перестановку к массиву мы получим массив $[y_1;y_2;\ldots;y_n]$, где $y_{\pi_i}=x_i$. С математической точки зрения это неудобно, а удобно $y_i=x_{\pi^{-1}_i}$.\\
            Давайте попробуем обобщить перестановки. Пусть у нас есть множество $X$. На него мы можем \textit{действовать}. Так, перестановка \textit{действует} на массивы от 1 до $n$.
        \end{Comment}
        \dfn Говорят, что $A$ \undercolor{red}{действует на} $X$, если задано отображение $\cdot\colon A\times X\to X$.
        \begin{Comment}
            Это самое отображение мы тут обозначили точкой, но вообще мы будем вместо него ничего не писать. В записи $\sin x$ между $\sin$ и $x$ мы же ничего не пишем.
        \end{Comment}
        \begin{Example}
            Так, в каком-нибудь группоиде мы можем считать, что один элемент действует на другой (при помощи групповой операции).
        \end{Example}
        \begin{Example}
            Можно считать множеством $X$ $n$-мерные вектора (т.е. $X=\mathbb R^n$), а действиями --- матрицы $n\times n$. И действительно, у нас есть способ из матрицы $n\times n$ и вектора размерности $n$ получить новый вектор размерности $n$. Какой? Умножить матрицу на вектор.\\
            Заметим, что тут матрицы должны быть квадратными, чтобы при умножении вектора из множества $\mathbb R^n$ мы получили вектор из того же множества.
        \end{Example}
        \begin{Example}
            В перестановках мы же можем наш массив считать любым. Например, таким: $[a;a;b]$. И на него мы можем действовать перестановкой, скажем, $[2;3;1]$. Тогда применив данную перестановку к массиву, получим $[b;a;a]$. И вообще указанная перестановка циклически сдвигает массив. То есть тут мы в качестве $A$ берём перестановки длины $n$ (т.е. $S_n$), а в качестве $X$ --- множество векторов из каких-то элементов. В данном случае $\{a,b\}^n$.
        \end{Example}
        \begin{Comment}
            Заметим, что \textit{действие} --- это что-то странное. У нас есть такая наука, как алгебра (не линейная, а просто алгебра). Это когда мы имеем несколько классов объектов, и нам хочется вычленить какие-то общие штуки. Например, все группы --- они похожи тем, что в них операция ассоциативна и обратима. И мы можем говорить, что для любой операции с такими свойствами и чем угодно остальным, выполнено, например, то, что обратный элемент существует ровно один. Другой пример --- линейные пространства. Мы наложили несколько ограничений на множество и две операции, чтобы полученную структуру изучать независимо от её внутренней природы. Вот давайте тут сделаем также. Что можно делать с действиями? Ну, композицию, логично.
        \end{Comment}
        \dfn Пусть $a_1$ и $a_2$ --- элементы множества $A$, которое действует на $X$. Тогда \undercolor{red}{композиция} $a_1$ и $a_2$ --- это такое действие $a_1\circ a_2$, которое действует на элемент $x\in X$ как $a_1\cdot(a_2\cdot x)$.
        \begin{Comment}
            Является ли композиция действий действием множества $A$. Ну, не факт.
        \end{Comment}
        \begin{Example}
            Так, антипример для предыдущего комментария выглядит, например, так. Пусть $A$ --- это элементарные транспозиции, а $X$ --- это $\{a,b,c\}^3$ (массивы длины $3$ из букв $\{a,b,c\}$). Тогда композиция транспозиций $[2,1,3]$ и $[1,3,2]$ --- это кто? Давайте посмотрим, как эта композиция действует на массив $[a,b,c]$. Ну, она превращает его в $[b,c,a]$. Не не существует такой элементарной транспозиции (т.е. такого элемента $A$), что он бы превращал $[a,b,c]$ в $[b,c,a]$.
        \end{Example}
        \begin{Comment}
            То есть это интуитивно, к результату первого действия применить второе, но не всегда это окажется единым действием. Иногда можно, когда $A$ замкнуто относительно композиции. То есть является группоидом.
        \end{Comment}
        \thm Если действия --- группоид, то это точно полугруппа.
        \begin{Proof}
            Ну, взяв $a_1$, $a_2$, $a_3$ и рассмотрев $(a_1\circ a_2)\circ a_3$ и $a_1\circ(a_2\circ a_3)$ узнаем, что они оба в применении к $x$ дадут $a_1\cdot(a_2\cdot(a_3\cdot x))$, а значит они --- одно и то же.
        \end{Proof}
        \begin{Comment}
            Хорошо, мы не можем получить группоид действий, который не полугруппа. А вот получить не моноид мы очень легко можем, если у нас просто нет нейтрального действия в $A$. Но нам это слабо интересно, такое мы изучать не будем.
        \end{Comment}
        \dfn \undercolor{red}{Нейтральное действие} --- это действие $\operatorname{id}\colon x\mapsto x$.
        \thm Несложно заметить, что это действительно нейтральное действие, и полугруппа действий с такой штукой действительно будет моноидом.
        \begin{Example}
            В качестве примера моноида действий, не являющегося группы, можно рассмотреть патологический пример $X=\mathbb Z$, $A=\mathbb N$, а действие --- это умножение. Собственно, получим мы умножение натурального числа на целое, у которого всё плохо с обратным элементом.
        \end{Example}
        \begin{Example}
            Другой пример уже был рассмотрен нами, когда мы говорили о действии матриц на векторы. Не всегда же есть нормальная матрица.
        \end{Example}
        \begin{Comment}
            Поскольку у на дискретная математика, моноиды сами по себе нам тоже не очень интересны. Мы хотим группу. Поэтому мы будем требовать ещё и обратных элемент.\\
            А теперь можно сделать следующее. Можно сначала сказать, что у нас $A$ --- это группа, а потом делать из неё действия. То есть до этого мы сначала сказали, что у нас $A$ --- это какое-то множество, завели на нём операцию, и потом сказали, что относительно этой операции множество должно быть группой. А сейчас нам от нашего прадедушки досталась уже готовая группа, и нам лишь нужно сказать, как взаимодействуют её элементы с множеством $X$. И здесь всё, чего мы хотим от жизни, чтобы наша групповая операция (из группы нашего прадедушки) была той же самой штукой, что и композиция.
        \end{Comment}
        \dfn Говорят, что \undercolor{red}{группа} $G$ \undercolor{red}{действует на} множество $X$, если задано отображение $\colon G\times X\to X$, для которого композиция действий --- это действие произведения, а действие нейтрального элемента группы --- это тождественное действие ($\operatorname{id}$).
        \begin{Comment}
            Существует мнение, что условие на то, что нейтральный элемент --- тождественное действие, не необходимо, а его нужно вывести. Мы не придерживаемся этого мнения.
        \end{Comment}
        \begin{Comment}
            Сейчас сделаем смену кадра и поговорим о немножко другом.
        \end{Comment}
        \begin{Comment}
            У нас было $X=\mathbb B^n$. У него была мощность, как несложно понять, $2^n$.\\
            А что будет, если у нас будет не важен порядок в $\mathbb B^n$? Мы получим множество $Y=\widehat{\mathbb B^n}$. Каковой будет мощность этого множества? Ну, $n+1$.\\
            А что если мы хотим, чтобы порядок чуть-чуть не имел значения? То есть записываем мы элементы не в ряд, а по кругу. Другими словами, массивы, получаемые друг из друга циклическим сдвигом, считаются одинаковыми. Обозначим это $Z=\widetilde{\mathbb B^n}$. И тут уже сложно посчитать количество элементов. Первая (неверная) гипотеза состоит в том, что это $\frac{2^n}n$, но это как минимум неверно (у массива из всех нулей любой циклический сдвиг --- это он сам), а как максимум не целое число. Ну и фиг с ним, не будем считать. Только в частном случае посчитаем, для $n=4$. Тогда мы получим ровно 6 элементов:
            \begin{itemize}
                \item $[0,0,0,0]$.
                \item $[0,0,0,1]$.
                \item $[0,0,1,1]$.
                \item $[0,1,0,1]$.
                \item $[0,1,1,1]$.
                \item $[1,1,1,1]$.
            \end{itemize}
            Что мы сейчас глобально делали? Мы считали элементы \textit{с точностью до чего-то}. В первом случае --- с точностью до ничего, во втором --- до перестановки, в третьем --- с точностью до циклического сдвига. То есть, если вспомнить первую лекцию по дискретке, станет понятно, что мы берём наше множество $X$ и какое-то отношение эквивалентности на нём, после чего считаем классы эквивалентности. То есть наше $Y$ --- это $X/\sim_{S_n}$ (фактормножество $X$ по отношению <<являться перестановками друг друга>>), $Z$ --- это фактормножество $X$ по отношению <<быть циклическими перестановками друг друга>>, а само $X$, кстати --- это $X/=$.\\
            И мы это считаем. Понятно, что количество классов эквивалентности произвольного отношения мы один хер посчитаем. Но на самом деле есть теория о некоторых \textit{хороших} отношениях эквивалентности, которую сформулировали Пойа и Бернсайд. И это как раз связано с действием группы на множество.
        \end{Comment}
        \thm Пусть у нас группа $G$ действует на $X$. Ведём отношение <<равно с точностью до действия $G$>>: $x\sim_Gy\Leftrightarrow \exists g\in G~y=gx$. Утверждается, что $\sim_G$ --- отношение эквивалентности.
        \begin{Proof}
            Рефлексивность? Почему $x$ эквивалентен сам себе? Потому что в группе $G$ есть нейтральный элемент $e$, который в применении к $x$ и даст $x$.\\
            Симметричность? Если $y=gx$, то $x=g^{-1}y$. Вообще говоря, это нужно доказать. $g^{-1}y=g^{-1}(gx)=(g^{-1}g)x=ex=x$. Тут в первом равенстве мы применили посылку условия, во втором --- аксиому действия группы на множество, в третьем --- определение обратного элемента группы, в четвёртом --- другую аксиому действия группы на множество.\\
            Транзитивность? Пусть $z=hy$, а $y=gx$. Ну, тогда $z=h(gx)$. По аксиоме действия группы на множество это равно $(hg)x$, а $hg$ --- это какой-то элемент $G$.
        \end{Proof}
        \begin{Comment}
            Чтобы не писать много значков, вместо $X/\sim_G$ пишут $X/G$.
        \end{Comment}
        \dfn Классы эквивалентности по нашему отношению (то есть элементы $X/G$) называются \undercolor{red}{орбитами}.
        \begin{Proof}
            Почему они так называются? Ну, если в качестве действия рассмотреть повороты плоскости, то орбита каждого элемента --- это окружность.
        \end{Proof}
        \begin{Example}
            Нам хочется в жизни заниматься подсчётом орбит.\\
            Пусть у нас есть $\mathbb B^4$ и группа $S_4$. Тогда чему равна орбита $\Orbit([0;1;0;1])$? Ну, мы можем любым образом переставить элементы, раз уж у нас $S_4$. А значит мы получим все наборы из нулей и единиц, в которых два нуля и две единицы.\\
            А что, если мы возьмём другую группу действий --- циклические сдвиги? Ну, тогда $\Orbit([0;1;0;1])=\{[0;1;0;1];[1;0;1;0]\}$.
        \end{Example}
        \dfn \undercolor{red}{Неподвижной точкой} элемента $g\in G$ называется элемент $x\in X$, для которого $gx=x$. Множество всех неподвижных точек $g$ обозначается $I(g)$ или $I_g$. То есть $I_g=\{x\in X\mid gx=x\}$.
        \begin{Example}
            Вспомним наш пример с поворотами плоскости. У поворота на $0^\circ$ есть много неподвижных точек --- все точки плоскости. А у других поворотов? А у них одна неподвижная точка --- начало координат.\\
            А если рассмотреть, например плоскость и сдвиги, то неподвижных точек не будет ни у кого.
        \end{Example}
        \begin{Example}
            Или для перестановки, которая меняет местами первый и второй элементы массивы $[0;0;1;0]$, $[1;1;0;0]$ и куча других --- неподвижные точки.
        \end{Example}
        \dfn \undercolor{red}{Стабилизатором} элемента $x\in X$ называется множество $\St x=\{g\in G\mid gx=x\}$. То есть множество тех $g\in G$, для которых $x$ --- неподвижная точка.
        \begin{Example}
            Вот чему равен стабилизатор $[0;1;0;1]$ в группе $S_n$? Ну, вот: $\{(1)(2)(3)(4);(13)(2)(4);(1)(3)(24);(13)(24)\}$. Напоминаю, что так, как написано в множестве, пишутся перестановки по циклам.
        \end{Example}
        \thm В отличие от множества неподвижных точек, стабилизатор не может быть пустым.
        \begin{Proof}
            Ведь в стабилизатор любого элемента входит нейтральный элемент группы.
        \end{Proof}
        \thm $\sum\limits_{x\in X}|\St x|=\sum\limits_{g\in G}|I_g|$.
        \begin{Proof}
            И слева, и справа написано количество пар $(g;x)$, таких, что $gx=x$.
        \end{Proof}
        \begin{Comment}
            Несмотря на очевидность леммы, она не бесполезна. Например, повороты плоскости считать проще, чем все точки плоскости. И обычно действий меньше, чем объектов, а значит их и считать проще. То есть в нашей сумме проще суммировать что-то по элементам группы, чем что-то по элементам множества.
        \end{Comment}
        \thm \undercolor{darkgreen}{Лемма Бернсайда}. $|X/G|=\frac{\sum_{g\in G}|I_g|}{|G|}$. То есть количество орбит равно среднему количеству неподвижных точек.
        \begin{Proof}
            Нарисуем таблицу, строки которой пронумерованы элементами $G$, а столбцы --- $X$. А в ячейках написано действие соответствующего группового элемента на элемент $X$. Например, так выглядит кусок этой таблицы на $X=\mathbb B^4$ и $G$ --- циклические сдвиги.
            \begin{center}
                \begin{tabular}{|c|c|c|c|c|c|c}
                    \hline
                    & 0000 & 0001 & 0010 & $\cdots$ & 0101 & $\cdots$\\
                    \hline
                    0 & 0000 & 0001 & 0010 & $\cdots$ & 0101 & $\cdots$\\
                    \hline
                    1 & 0000 & 1000 & 0001 & $\cdots$ & 1010 & $\cdots$\\
                    \hline
                    2 & 0000 & 0100 & 1000 & $\cdots$ & 0101 & $\cdots$\\
                    \hline
                    3 & 0000 & 0010 & 0100 & $\cdots$ & 1010 & $\cdots$\\
                    \hline
                \end{tabular}
            \end{center}
            Что можно сказать про столбцы векторов из одной орбиты (как тут, про 0001 и 0010)? Их столбцы равны с точностью до перестановки. И это логично для любого действия. Давайте для каждой орбиты вырежем все столбцы, кроме одного. В этой таблице суммарно $|G|\cdot|X/G|$ клеток (ну, разумно, мы оставили только $|X/G|$ столбцов). Теперь посмотрим на то, что каждый элемент $x$ встречается только в одном столбце (потому что орбиты --- классы эквивалентности, они либо совпадают, либо не пересекаются). А сколько раз $x$ встречается в этом столбце? Ну, $|\St x|$ раз. А значит $|G|\cdot|X/G|=\sum\limits_{x\in X}|\St x|=\sum\limits_{g\in G}|I_g|$. Ну, так и всё.
        \end{Proof}
        \begin{Example}
            Давайте применим это на практике. У нас было $Z=\widetilde{\mathbb B^4}$. Это фактормножество $\mathbb B^4$ по циклическим сдвигам. То ест в нашей группе есть ровно 4 элемента --- сдвиг на 0, на 1, на 2 и на 3. Так давайте же посчитаем $\frac{|I_0|+|I_1|+|I_2|+|I_3|}4$. $I_0$ --- это всё $\mathbb B^4$, потому что тождественная перестановка не меняет элементы ($|I_0|=16$). Чему равно $I_1$? Ну, какие элементы остаются на месте при сдвиге на единицу? Ну, только $[0,0,0,0]$ и $[1,1,1,1]$ ($|I_1|=2$). Чему равно $I_2$? Ну, тут есть 4 элемента: $[0,0,0,0]$, $[1,1,1,1]$, $[0,1,0,1]$ и $[1,0,1,0]$. А в $I_3$ находится то же самое, что и в $I_1$. То есть наш ответ --- $\frac{16+2+4+2}4=6$, как раз столько мы и посчитали ранее.
        \end{Example}
        \begin{Example}
            А давайте это в общем случае посчитаем. Для этого нужно понять, чему равно $I_k$ для произвольного $k$. Ну, у нас все массивы длины $n$ бьются на циклы. И понятно, что внутри каждого цикла все элементы должны быть одинаковы (чтобы это было неподвижной точкой сдвига на $k$), то есть $|I_k|$ --- это $2^{\text{количество циклов}}$. Осталось эти циклы посчитать. Можно подумать, порисовать картинки и убедится, что это количество --- это наибольший общий делитель $n$ и $k$. То есть ответ в нашей задаче --- $\frac{\sum\limits_{k=0}^{n-1}2^{\gcd(n;k)}}n$.
        \end{Example}
        \begin{Comment}
            Почему лемма Бернсайда это лемма, а не теорема? Ну, исторически, потому что ценность этой леммы самой по себе такая себе, её только используют в других (более важных утверждениях) для непосредственного подсчёта на конкретных примерах (как то, что мы доказали выше про $\widetilde{\mathbb B^n}$).
        \end{Comment}
        \begin{Comment}
            Вообще то, что мы сейчас рассмотрели, это комбинаторный объект, который называется ожерельем. То есть мы сейчас пытались найти количество способов составить ожерелье из $n$ бусин двух различных цветов. Поскольку ожерелье круглое, у нас и получается, что циклические сдвиги ожерелья --- то же самое ожерелье. Но ведь можно составлять его не из бусин двух типов, а из бусин $|T|$ типов. То есть у нас есть множество бусин $T$, и из него мы бусины и берём.
        \end{Comment}
        \thm \undercolor{darkgreen}{Формула Пойи}. Пусть у нас $X$ --- $T^n$, а $G$ --- подгруппа $S_n$. Тогда $$|X/G|=\frac{\sum\limits_{g\in G}{|T|}^{\text{число циклов в перестановке }g}}{|G|}$$.
        \begin{Proof}
            Это просто частный случай леммы Бернсайда, про который мы уже всё сказали в одном из примеров выше.
        \end{Proof}
        \begin{Example}
            Давайте сделаем небольшое замечание. Рассмотрим $\{0,1,2\}^n$. А в качестве действия будем прибавлять константу по модулю 3 ко всем компонентам вектора. То есть $G=\{+0,+1,+2\}$, и, например $[1,0,0,2]+1=[2,1,1,0]$. Как тогда выглядит лемма Бернсайда? Ну, очень легко: $\frac{3^n+0+0}3$. И зачем нам этот пример? А затем, что раньше мы неправильно считали $\widetilde{\mathbb B^2}$ как $\frac{2^n}n$. Так вот тут (когда ни у какого элемента, кроме нейтрального, нет неподвижных точек) аналогичная формула будет верной. 
        \end{Example}
        \thm Если $\forall g\neq e~I_g=\varnothing$, то $|X/G|=\frac{|X|}{|G|}$.
    \end{itemize}
    \paragraph{\undercolorblack{orange}{Конструируемые комбинаторные объекты}.}
    \begin{itemize}
        \item[]
        \begin{Comment}
            Раньше мы смотрели на комбинаторные объекты как зоологи на животных. А с математической точки зрения вот есть все эти объектики, и чего?\\
            Наша тема --- это то, на что мы посмотрим сейчас, а потом вернёмся (когда будем говорить о производящих функциях.)\\
            Кто такой <<конструируемые комбинаторные объекты>>. Вот есть такой базовый комбинаторный объект --- \mintinline{c}{void}. Он один, единый и неделимый. Его называют <<атом>> (да, все определения будут настолько мутными). И из него мы будем составлять какие-то штуки по каким-то правилам. И мы хотим, чтобы зная распределение объектов по \textit{весу}, конструируя из них новый, мы тоже могли узнать его вес, где вес --- количество атомов.
        \end{Comment}
        \dfn Атом --- это множество из одного элементарного элемента. Будем называть его $U$, а множество атомов --- $U=\{u\}$.
        \dfn \undercolor{red}{Вес} некоторого объекта --- количество атомов в нём.
        \begin{Comment}
            Вот есть у нас множество каких-то объектов $A$. И в их состав входят объекты из какого-то количества атомов. Это количество --- <<вес>> объекта. И в множестве $A$ есть $a_0$ объектов веса 0, $a_1$ --- веса 1 и так далее.
        \end{Comment}
        \begin{Comment}
            Что мы можем делать с объектами? Ну, мы можем взять два множества объектов $A$ и $B$ и сделать $C=A\times B$ (декартово произведение). Если у нас веса в $A$ были $a_0$, $a_1$, $a_2$, ..., а в $B$ --- $b_0$, $b_1$, $b_2$, ..., то что же будет в $C$? Вот чему равно $c_n$? Ну, мы можем поставить на первое место пары вес $i$, а на второе --- $n-i$. Сколько способов выбрать на первое место элемент с весом $i$? Ну, $a_i$. А на второе место --- $n-i$? Очевидно, $b_{n-i}$. Так мы получим $c_n=\sum_{i=0}^na_ib_{n-i}$. Это выглядит как какой-то многочлен (или, точнее, ряд), но это уже углубление в производящие функции, что мы делать не будем. Пока что.\\
            Вот мы получили $c_n=\sum_{i=0}^na_ib_{n-i}$. Может можно сделать что-то более простое, чем сумму? Да, можно.
        \end{Comment}
        \dfn Если множества $A$ и $B$ не пересекаются, то их дизъюнктным объединением $A\sqcup B$ называется $A\cup B$. (Как прямая сумма для линейных подпространств.)
        \begin{Comment}
            Тут всё банально, если $C=A\sqcup B$, то $c_n=a_n+b_n$.
        \end{Comment}
        \begin{Comment}
            А что мы можем делать с $U$? Объединить само с собой не можем. Значит придётся делать $V=U\times U$, получив $v_0=v_1=0$, $v_2=1$, $v_3=v_4=\cdots=0$. То есть опять множество одного элемента, это скучно. Но всё же, точно ли мы не можем делать $U\sqcup U$. Что бы мы могли иметь ввиду, когда пишем $U\sqcup U$? То, что у нас то самое $u\in U$ оно слева и справа разное. Так вот, можем, но тут нам придётся сделать ещё одну абстракцию --- пометки. Это то, что не имеет веса, но что мы можем приписать к атому, чтобы получить новый атом того же веса (не обязательно к атому, на самом деле, но не суть). То есть если у нас $A=\{a\}$ и $B=\{b\}$ --- пометки, то можно взять $U\times A=\{(u;a)\}$ и $U\times B=\{(u;b)\}$ и получить, вроде как разные множества (которые имеют элементы с тем же весом, что и $U$), которые можно дизъюнктно объединить.
        \end{Comment}
        \begin{Comment}
            Давайте посмотрим на такую штуку $T=E\cup U\times(T\times T)$, где $E=\{e\}$, а $e$ --- что-то нулевого веса. Это выглядит как что-то очень странное, какое-то совсем ужасное рекурсивное определение, бр-р-р. Но на самом деле тут можно жить как в динамическом программировании. То есть мы должны как-то узнать, чему равно, например, $t_0$, а потом ссылаться только на элементы меньшего веса. Начинаем мы с того, что $t_0=1$, ведь количество элементов веса ноль только один, $e$, а дальше смотрим: $t_n$ --- это атом и элемент пары $T\times T$. Тогда $t_n=\sum\limits_{i=0}^nu_ip_{n-i}=\sum\limits_{i=0}^nu_i\sum\limits_{j=0}^{n-i}t_jt_{n-i-j}=\sum\limits_{j=0}^{n-1}t_jt_{n-1-j}$. Так мы же получили числа Каталана, и неспроста. Ведь двоичное дерево --- это либо пустое нечто, либо корень ($U$) и два его ребёнка ($T\times T$).
        \end{Comment}
        \begin{Comment}
            Помните $U\times U=V$? У нас тогда получилось, что $v_2=1$, а все остальные --- 0. То есть по логике если мы сделаем $\bigcup\limits_{i=0}^\infty U^i$, мы получим множество, у которого есть по одному элементу каждого веса, так? Ну, да, если бы у нас было легально делать бесконечные объединения.
        \end{Comment}
        \dfn $\Seq A$ --- такое $B$, что $B=E\cup A\times B$. $\Seq$ называют <<\undercolor{red}{sequence}>>.
        \begin{Comment}
            Ну, давайте посмотрим на $B=E\cup A\times B$. Давайте попробуем посчитать $b_0$. Оно, вроде как, равно единице, но тут нужно дополнительное условие --- $a_0=0$, ведь иначе $b_0$ ссылается на само себя (если $b_0$ равно чему-то, то $(a\times b)_0$ равно тому же самому, умноженному на $a_0$, а значит $b_0=a_0b_0+1$, что полная дичь). А если мы накладываем условие на $a_0=0$, то получаем $b_n=\sum\limits_{i=1}^na_ib_{n-i}$.\\
            Какой у этого смысл? Мы делаем последовательность из элементов $A$ суммарного веса $n$. Если $a_0\neq0$, то мы можем запихать в любое место любой последовательности произвольное количество элементов нулевого веса, а значит последовательностей (при $a_0\neq0$) любого веса бесконечное количество.
        \end{Comment}
        \begin{Example}
            Давайте попробуем $Z=\Seq U$. $z_0=1$, как мы знаем, а $z_n=\sum\limits_{i=1}^nu_iz_{n-i}=u_1z_{n-1}=z_{n-1}$. То есть все $z_n$ равны единице, и мы получили натуральные числа с нулём.
        \end{Example}
        \begin{Example}
            А мы можем взять $\Seq Z$? К сожалению, нет, у него $z_0=1\neq0$. Так давайте возьмём $Z^{>0}=Z\setminus\{\varepsilon\}$. Тогда $\Seq Z^{>0}$ обозначим за $P$. Тогда $p_0=1$, $p_1$ посчитаем руками, тоже получим $1$, а $p_n=\sum\limits_{i=1}^nz_ip_{n-i}=\sum\limits_{i=1}^np_{n-i}=p_{n-1}+\sum\limits_{i=2}^np_{n-i}=p_{n-1}+\sum\limits_{i=1}^{n-1}p_{n-i-1}=2p_{n-1}$.\\
            Какой смысл у этого? Мы берём последовательности, где на каждом месте стоит натуральное число, и хочется узнать количество с суммой $n$.
        \end{Example}
        \begin{Comment}
            У нас были упорядоченные пары, а мы хотим чего-то ещё, чего-то неупорядоченного.
        \end{Comment}
        \begin{Comment}
            Как вводить powerset ($\PSet A=B$)? Это довольно сложно, проще, если мы введём $B_k=\{C\subset A\mid \forall c\in C~w(c)\leqslant k\}$ ($w(c)$ --- вес $c$).\\
            Тогда $b_{n,k}=\sum\limits_{i=0}^{\lfloor n/k\rfloor}\matr{a_k\\i}b_{n-ki,k-1}$. (Мы перебираем, сколько элементов веса ровно $k$ мы можем взять.) Тогда $b_n=b_{n,n}$, лол, а $b_0=2^{a_0}$, потому что любой из элементов веса ноль можно взять либо нет.
        \end{Comment}
        \dfn $\PSet$ (\undercolor{red}{powerset}) --- это то, что описано выше.
        \begin{Example}
            $Q=\PSet Z^{>0}$. Утверждается, что $q_n$ --- количество разбиений $n$ на различные слагаемые. Ну, докажем это. Рассмотрим $q_{n,k}$ --- количество разбиений $n$ на слагаемые веса не больше $k$. Тогда по формуле $q_{n,k}=\sum\limits_{i=0}^{\lfloor n/k\rfloor}\matr{z_k\\i}b_{n-ki,k-1}=\sum\limits_{i=0}^{\lfloor n/k\rfloor}\matr{1\\i}b_{n-ki,k-1}=q_{n,k-1}+q_{n-k,k-1}$. Эта формула и есть количество разбиений $n$ на различные слагаемые, потому что мы либо берём $k$, либо нет.
        \end{Example}
        \begin{Comment}
            Теперь обсудим $\MSet$ --- мультимножество. То есть мы хотим из $A$ выбрать какие-то объекты с суммарным весом $n$, но с повторениями. Тут мы уж точно запрещаем $a_0\neq0$, иначе мы их можем повторять сколько угодно.\\
            Тут нам понадобятся сочетания с повторениями.
        \end{Comment}
        \dfn \undercolor{red}{Количество сочетаний с повторениями} из $n$ по $k$ --- это количества способов из $n$ объектов выбрать $k$ штук, если каждый объект можно взять несколько раз.
        \thm Количество сочетаний с повторениями из $n$ по $k$ равно $\matr{n-k-1\\k}$.
        \begin{Proof}
            Давайте представим сочетание с повторениями в канонической (возрастающей) форме. Получим неубывающую последовательность $a_i$ длины $n$, в которых каждый элемент от 1 до $n$ включительно. Давайте к каждому элементу прибавим его позицию. Получим возрастающую последовательность $b_i=a_1+i-1$ со свойством $b_i\in[i;n+i-1]$. Утверждается, что полученное нами соответствие между $a_i$ и $b_i$ --- биекция. Это несложно проверить, ведь из строго возрастающей последовательности $b_i$ с указанным свойством можно беспрепятственно вычесть номер, получив корректную последовательность, к тому же убывающую, потому что $b_{i+1}>b_i\Leftrightarrow b_{i+1}\geqslant b_i-1\Leftrightarrow a_{i+1}-i-1\geqslant a_i-i-1$.\\
            То есть наших сочетаний с повторениями столько же, сколько возрастающих последовательностей длины $n$ с числами $b_i$ от $i$ до $n+i-1$. Ну, а сколько их? Нижняя граница не влияет, из-за строгой возрастаемости. Ну, а так мы можем выбрать какие-то $k$ различных чисел от 1 до $n+k-1$, отсортировать их и получить последовательность $b_i$.
        \end{Proof}
        \begin{Comment}
            А как жить после этого с мультимножеством? Ну, мы точно обязаны вырезать все элементы веса 0, это мы уже говорили, и тогда по в формуле получим простое изменение $b_{n,k}=\sum\limits_{i=0}^{\lfloor n/k\rfloor}\matr{a_k+i-1\\i}b_{n-ki,k-1}$, где $B_k=\{C\mid C\in\MSet A\land\forall c\in C~w(c)\leqslant k\}$. При этом, несложно заметить, $b_0=0$.
        \end{Comment}
        \begin{Example}
            Рассчитаем $P=\MSet Z^{>0}$. $p_{n,k}=\sum\limits_{i=0}^{\lfloor n/k\rfloor}\matr{z_k+i-1\\i}p_{n-ki,k-1}=\sum\limits_{i=0}^{\lfloor n/k\rfloor}\matr{i\\i}p_{n-ki,k-1}=\sum\limits_{i=0}^{\lfloor n/k\rfloor}p_{n-ki,k-1}=p_{n,k-1}+\sum\limits_{i=1}^{\lfloor n/k\rfloor}p_{n-ki,k-1}=p_{n,k-1}+\sum\limits_{i=0}^{\left\lfloor\frac{n-k}k\right\rfloor}p_{n-ki-k,k-1}=p_{n,k-1}+p_{n-k,k}$. Вот мы и получили на самом деле, разбиение на неупорядоченные произвольные слагаемые.
        \end{Example}
        \begin{Comment}
            Есть ещё одна структура --- $\Cyc$ (цикл). Чтобы тут всё посчитать, придётся пользоваться предыдущей лекцией, но там всё получится ужасно, надо будет вводить дополнительную структура (последовательности длины $n/\text{количество циклов}$ веса $i$), и всё это будет выглядеть страшно, поэтому не будем.
        \end{Comment}
        \begin{Comment}
            Если мы попытаемся сделать нашими операциями перестановку, например, то у нас не получится, потому что мы не можем разложить ограничение на внутреннюю структуру элементов. Единственное, что мы можем --- фиксировать суммарный вес. Поэтому всё, что мы можем --- это внести коррективы в модель, получив новую, <<помеченные комбинаторные объекты>>. В них каждый атом имеет номер, они от 1 до $n$, а все объекты различны. Тогда что такое пара ($C=A\times B$)? Если мы возьмём $a_i$ и $b_{n-i}$, то там будут пометки в первом и пометки во втором объекте, они будут повторяться. Это надо фиксить. Давайте временно забудем о пометках, а потом посмотрим все варианты как-то поставить пометки в элементе-паре. Утверждается, что есть соответствие между расстановкой пометок в нашей паре и расстановкой пометок в отдельных элементах. Рассмотрим элемент из $A$. На нём есть пометки от 1 до $n$, а мы хотим от 1 до $i$. Ну так давайте перенумеруем их в том порядке, который был возрастающий в объекте-паре. Но это же не биекция. Например, если у нас элемент-пара состоит из объекта веса 2 из $A$ и объекта веса 3 из $B$, то мы можем перенумеровать всё как $[1,4],[5,2,3]$, и это перейдёт в $[1,2],[3,1,2]$ и пометки на паре $[1,5],[4,2,3]$ перейдёт туда же. Это плохо. Поэтому из пометок, выделенных на первый элемент пары, нас интересует только относительный порядок. То есть нам нужно выбрать только то, какие пометки идут в первый элемент пары, потому что порядок, в котором они идут, задаётся разметкой исходного элемента. То есть количество разметок пары ровно в $\matr{n\\i}$ больше, чем разметок отдельных компонентов. А это значит, что $c_n=\sum\limits_{i=1}^n\matr{n\\i}a_ib_{n-i}$.
        \end{Comment}
        \begin{Comment}
            После этого мы можем снова определить $\Seq A=B=E\cup A\times B$, ведь мы умеем пользоваться $\times$: $b_n=\sum\limits_{i=1}^n\matr{n\\i}a_ib_{n-1}$. А ещё в случае помеченных объектов очень легко работает цикл. $c_{n,k}$ --- количество циклов веса $n$ длины $k$, а $b_{n,k}$ --- количество последовательностей веса $n$ длины $k$. А поскольку у нас всё помечено (то есть все элементы, которые мы переставляем, различны), они как раз и отличаются в $k$ раз: $c_{n,k}=\frac{b_{n,k}}k$, а $c_n=\sum\limits_{k=1}^n\frac{b_{n,k}}k$. Мы ещё, правда, не знаем, что такое $b_{n,k}$, но на самом деле знаем, $b_{n,k}=\sum\limits_{i=1}^n\matr{n\\i}a_ib_{n-1,k-1}$ Для $\PSet$ и $\MSet$ там всё сложно, не будем это расписывать.\\
            А будем расписывать $U=\{u\}$. Что такое $\Seq U=P$? $p_n=\sum\limits_{i=0}^n\matr{n\\i}u_ip_{n-i}=\matr{n\\1}u_1p_{n-1}=np_{n-1}$. То есть $p_n=n!$, и это логично, ведь если раньше у нас был один способ взять последовательность длины $n$ из одного элемента, то теперь у нас все элементы различные, а значит количество способов взять последовательность из $n$ разных элементов --- $n!$.
        \end{Comment}
    \end{itemize}
\end{document}
