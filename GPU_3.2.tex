\documentclass{article}
\input{Headers/header}
\input{Headers/formal}

\geometry{margin=1in}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyfoot[L]{}
\fancyfoot[C]{Иванов Тимофей}
\fancyfoot[R]{\pagename\ \thepage}
\fancyhead[L]{}
\fancyhead[R]{\leftmark}
\renewcommand{\sectionmark}[1]{\markboth{#1}{}}

\usepackage[outputdir=Output, cachedir=mintedcache]{minted2}

\begin{document}
    \section{Введение.}
    О чём курс? Есть теоретическая база распараллеливания, и это было на курсе Елизарова. Но есть другой класс задач, в который параллельность тривиальная, и нам надо лишь быстро параллельно посчитать, без бед с теорией. Особенность в том, что для такого массового многопоточного исполнения CPU не очень подходит, потому что он создавался и оптимизировался для быстрого исполнения небольшого числа потоков. Типа 16 штук. Это очень мало. А если хочется параллельно обрабатывать десятки тысяч одинаковых данных, эти 16 потоков скорее будут выполнять ваши вычисления последовательно, а не параллельно. Поэтому тут нужны другие оптимизации и другой подход.\\
    Что такое видеокарта? Это куча убогих процессоров, которых прям очень-очень много. И оптимизированы они под то, чтобы очень сильно параллелить. Каждый один из них медленный. Но если ваша задача бьётся на тысячи и десятки тысяч потоков, будет именно то, что хочется.\\
    Мы будем рассматривать классический учебный пример: умножение матриц. Оно и полезное практически, и много полезных идей и техник содержит. И рассматривать мы будем его начиная с процессора, чтобы был какой-то бейзлайн. Его же мы будем использовать для проверки правильности работы, потому что в процессе оптимизации можно очень много набажить.\\
    Как программировать видеокарточки? Изначально появились библиотеки в духе OpenGL, где были шейдеры: специальные программки в видеокартах, которые были нужны для трёхмерной отрисовки. Можно их и использовать: сгенерировать прямоугольник, загрузить входные данные в текстуры, выходные данные тоже выводить в текстуру, и потом из текстуры выгружаем. А потом стало понятно, что хочется не только рисовать, а именно что считать, и для этого хочется иметь более удобное API.\\
    Первым популярным из таковых стала CUDA. Получила она популярность не потому, что она была очень хороша, а потому, что NVIDIA влила туда кучу денег: куча учебных курсов, документации, бесплатно делали разным людям реализацию на CUDA. Зачем? Чтобы продавать лицензии. Права на исполнения CUDA-кода имеют только NVIDIA. Ну и как бы оно в целом норм, жизнеспособно. И до сих пор оно поддерживается, чтобы уже имеющийся код до сих пор работал на всех их видеокартах.\\
    Из ещё заслуживающего интереса упомянем OpenCL. У него тоже есть интересная история в том же духе. Таким же образом, как NVIDIA, поступает Apple. А ещё такие компании очень не любят, когда такое делают с ними, поэтому Apple и NVIDIA ненавидят друг друга. Поэтому Apple проспонсировали создание открытого стандарта, являющегося конкурентом CUDA: OpenCL. И владеет им не Apple, а Khronos Group (те же люди, которые владеют OpenGL). И OpenCL в целом вообще более общий, чем CUDA. Он реализован не только под видеокарты, но и под процессоры и FPGA, например. FPGA~--- нечто среднее между железом и софтом: микросхемы, конфигурацию которых можно задать программно. Оно обычно программируется на специальных языках (например, verilog). И OpenCL позволяет программировать на FPGA на почти чистом C. Обратная сторона такой широкой поддержки: управление неповоротливым концерном. Пока NVIDIA щедро льют в CUDA новые фичи (из нового железа), в OpenCL всё происходит гораздо более медлительно. Из интересного NVIDIA не поддержала вторую версию OpenCL, и Khronos в третьей версии пошли им на встречу, сделав почти все фичи из версии 2 опциональными. Но нам, в целом, не важно, нам хватит OpenCL 1.2.\\
    Ещё существует такая штука как HIP. Это убогая попытка AMD сделать совместимость с CUDA. Бинарная совместимость жёстко охраняется патентами, на программная совместимость запатентовать нельзя. Поэтому вы немного переименовываете функции CUDA, и получаете HIP. Меняете \Verb|cudaMalloc| на \Verb|hipMalloc|, и больше ничего. Получаете код под HIP. Но вот беда: поддержка у HIP'а в говне. Он работает только на некоторых карточках, притом необязательно на новых. И никакой логики в том, где оно поддерживается, нет. Совершенно не programmer-friendly.\\
    Ну и не будем забывать, что уже миллион лет Intel пытаются выйти на рынок видеокарт. Они поддерживают OpenCL, но и пытаются продвигать своё API, но это никем не используется, очевидно.\\
    Ещё есть Metal от Apple, но яблочники сосут, это не будем трогать.\\
    Ещё в новых версиях OpenGL (и в других графических библиотеках) добавили вычислительные шейдеры, и их можно использовать примерно так, как OpenCL (Vulkan Compute, например, вообще использует практически то же промежуточное представление, что и OpenCL). Но оно всё равно предназначено для графики (т.е. игрушек). Просто говоря~--- точность говно. Особенно точность деления. Ну и наконец Vulkan~--- низкоуровневое графическое API. Инициализация Vulkan~--- тысяча строк. Vulkan Compute~--- в десяток раз меньше, но это всё равно дохера много. А ещё шейдеры Vulkan Compute консервативны. Из самого грустного~--- там нет указателей. Объекты передаются через сложные странных хэндлы. Передать указатель на матрицу в функцию нельзя. Удачи выразить свою мысль так, чтобы ничего не было скопировано. Это всё можно починить кучей расширений, которые для начала надо перечислить, включить и убедиться, что эти расширения у вас есть. Количество усилий для этого огромно. Ну и наконец, по сравнению с OpenCL, Vulkan нельзя запустить ни на чём, кроме видеокарт.\\
    Кстати про OpenGL. Он говно. Его проектировали сто лет назад, и сейчас видеокарты работают совсем не так, как сейчас устроены видеокарты. Поэтому дайвер для него~--- огромный кусок дерьма. Vulkan появился не просто так, как бы. Игры написаны идиотами и если их запускать по стандарту, они сдохнут. Поэтому, например, NVIDIA пишет драйвера с кучей костылей по тому, как заставить игрушки работать, да ещё и быстро. Поэтому, например, NVIDIA имела более слабое железо, но FPS'ов было больше. И тогда AMD выпустили низкоуровневое API, которое соответствовало бы устройству видеокарт. Оно называлось Mantle. Это чудо тоже отдали Khronos'у, который сделал из этого Vulkan. Собственно, по той же причине, что OpenGL не соответствует устройству карт, Intel сильно получили в рожу, когда выпустили свои первые карты (они состояли из кучи первых Pentium'ов): они не умели в эту чёрную магию, поэтому им пришлось использовать DXVK~--- транслятор DirectX в Vulkan (который изначально был создан для запуска игр под Linux).\\
    План курса такой: сначала потрогаем CUDA (владельцы красных карт возьмут народный конвертер из CUDA в HIP), а потом OpenCL.\\
    Как это всё использовать? CUDA~--- расширение C++, которое добавляет магические заклинания. Его надо скомпилировать так: файл .cu подаётся компилятору CUDA, который разделяет его на device-код и host-код. host~--- это просто C++, который кормится обычному компилятору. И device-код компилируется специальным компилятором от NVIDIA, и получается специальный бинарник, который вставляется прямо внутрь вашего бинарника. Всё это линкуется с библиотекой, которая занимается как раз тем, что вычленяет device-бинарник и отправляет его на видеокарту, попутно разбираясь со временами жизни и т.п. Эти действия можно и руками делать, если очень хочется.\\
    OpenCL делает примерно то же самое, что вы получите от CUDA, если будете делать все шаги руками. Вам придётся самим указывать, где брать бинарник, с чем линковаться и т.п.
    \section{CUDA.}
    \paragraph{Установка.}
    Просто потому, что визуально оно простое. Потому, что NVIDIA все страшные вещи замела под ковёр. А точнее, в библиотеку от которой у нас будет зависимость. Что надо для CUDA? Нужны заголовки и либы, то есть SKD. Оно называется Cuda toolkit.\\
    После этого у нас будет вариация на тему C++. Мы уже помним, что мы пишем файлики с расширением \textit{.cu} на C++ с расширениями, а компилятор пилит это на C++ и device-код для CUDA. Первый компилируется обычным компилятором, а второй специфичным компилятором.\\
    Чтобы вызвать функцию CUDA надо сделать \mintinline{c++}|#include <cuda.h>|.\\
    Первое, что надо сделать:
    \begin{minted}{c++}
        int dev_num;
        cudaError_t err = cudaGetDeviceCount(&dev_num);
    \end{minted}
    Если всё хорошо, вам вернут ноль. Иначе~--- код ошибки. Какие там коды, можно посмотреть в \mintinline{c++}|<cuda.h>|. Из интересного, clang-18 умеет компилировать Cuda. Версия 19 имеет с этим проблемы (как минимум под виндой). Также компилироваться можно из сред разработки, например, Visual Studio. Чтобы компилить из командной строки, делаем следующее (и вправляем пути, если надо):
    \begin{minted}{console}
        clang++ kernel.cu -o kernel.exe -O3 -lcudart_static --cuda-gpu-arch=sm_61 \
            -Wno-unknown-cuda-version -fms-runtime-lib=dll -Wl,-nodefaultlib:libcmt \
            --cuda-path="C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6" \
            -L"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6\lib\x64"
    \end{minted}
    Если вы получите нулевой код возврата и один девайс, это кайф. Если вам вернули ноль или вернули ошибку, которая показывает, что нет девайсов, то грустно. Что делать в таком случае? Если видеокарточка зелёная, значит криво на неё встали дрова, и их надо переставить. Если она красная и не очень древняя (архитектура RDNA и новее) и AMD о ней не забыли, можно поставить ROCm~--- народный транслятор CUDA в HIP, плюс подходящий рантайм. Вам надо скачать ZLUDA (\url{https://github.com/lshqqytiger/ZLUDA}). Для Windows из него надо взять nvcuda.dll и положить в WINDOWS\textbackslash System32. Если карточка новая, всё хорошо, ставьте себе версию 6, и всё должно работать. Если у вас что-то старее RDNA, можно попытаться взять версию 5 и надеяться, что встанет, но надежды мало. Под Linux всё сложнее но и больше шансов на успех. Если карточка новая, оно может встать из коробки и так. Иначе надо гуглить, наверняка кто-то умный вправил так, чтобы всё работало. Если у вас синяя видеокарта, говорят, что если она новая, старая версия ZLUDA умеет транслировать в Level zero runtime, если у вас он, можете попробовать, но никто ничего не гарантирует. Если ничего из вышеперечисленного не помогает или у Вас мак, есть только сервер.
    \paragraph{Устройства.}
    Что дальше? Дальше для каждого девайса вызываем \mintinline{c++}|cudaGetDeviceProperties|. Оно вернём кучу всего. По-хорошему ваша программа должна давать возможность выбрать девайс пользователю, если их несколько. Работать на всех девайсах сразу сложно (с точки зрения написания кода). Особенно это актуально для ZLUDA, потому что если у вас есть несколько девайсов AMD, ROCm сначала перечисляет интеграшку, а потом нормальные карты (пока все остальные делают наоборот). А ещё хорошо бы иметь приличные значения по умолчанию. Как понять, что за карты у вас? Собственно \mintinline{c++}|cudaGetDeviceProperties|. Из интересного пользователю есть поле \Verb|.name|, из интересного нам~--- \Verb|.major| и \Verb|.minor|. Это compute capability. Это про вычислительный уровень вашего железа. Разные поколения карточек имеют разные возможности. В каких-то картах добавляют новые фичи, в каких-то даже убирают. Как правило больше~--- лучше, но необязательно. Для старших версий это правда, а для младших версий~--- вообще необязательно, потому что \Verb|.0| обычно~--- карты для data center'ов. Девайсы с одинаковым CC как правило отличаются тем, сколько раз вычислительные блоки скопипастили. Чем больше~--- тем быстрее. А вообще про CC можно открыть статью Википедии, где есть огромная таблица о том, что карточка умеет.\\
    После этого нужно сделать \mintinline{c++}|cudaSetDevice|, где вы говорите, какое устройство хочется. Он устанавливает устройство для текущего \textbf{потока}. В процессе этого всего нужно проверять возвращаемые значения.
    \paragraph{Память.}
    Чтобы что-то вычислить нам надо сначала иметь данные. А где у нас данные? Правильно, в оперативке. А мы хотим скопировать данные на девайс, и дальше уже запускать вычисления на этих скопированных данных. В этом основная проблема вычислений на видеокарте: время передачи данных на устройство может превышать время вычисления на процессоре. Исключением из этого являются (точнее, могут быть) интеграшки, потому что они способны работать без копирования (если это правильно накодить).\\
    Чтобы выделить память надо вызвать \mintinline{c++}|cudaMalloc|. Как скопировать данные с хоста на девайс? Во-первых, есть \mintinline{c++}|cudaMemcpy|, которая помимо стандартных аргументов (куда, откуда, сколько) принимает направление. Направление~--- это либо <<с хоста на девайс>> (\mintinline{c++}|cudaMemcpyHostToDevice|), либо <<с девайса на хост>> (\mintinline{c++}|cudaMemcpyDeviceToHost|), либо <<с девайса на девайс>> (\mintinline{c++}|cudaMemcpyDeviceToDevice|). Но ещё там есть \mintinline{c++}|cudaMallocAsync|. Эта штука не останавливает процессорный тред пока не скопируется. Она ставит задачу копирования в очередь, и продолжает исполнение хост-кода. Очередей, вообще говоря, несколько, и по умолчанию вам ставят в дефолтную очередь текущего потока. Нам ничего кроме дефолтной не понадобятся, но вообще можно создавать другие очереди и указывать их.
    \paragraph{Вычисление.}
    Пусть мы хотим сделать $a+b$. Как сказать, что мы хотим нашу штуку на девайсе? Тут используется магическое слово \mintinline{cu}|__global__|:
    \begin{minted}{cu}
        __global__ void add(const int* a, const int* b, int* c)
        {
            *c = *a + *b;
        }
    \end{minted}
    Это ключевое слово значит, что это точка входа на девайсе, которую можно вызвать с хоста. Если использовать магическое слово \mintinline{cu}|__device__|, то это функция, которую можно вызывать только из девайса.\\
    Как это запустить? Вот так: \mintinline{cu}|add<<<1, 1>>>(a, b, c)|. Оно не возвращает код ошибки, если вы хотите узнать, удачно ли оно поставилось в очередь, можете сделать \mintinline{c++}|cudaGetLastError|. Тут единички~--- то, сколько вычислителей на видеокарте мы хотим. Об этом ещё потом поговорим. После этого лучше делать синхронное копирование с девайса на хост. Почему синхронное? Потому, что нам нужно подождать, что всё предыдущее посчиталось, и можно, конечно, явно ждать, но зачем, если можно подождать, пока синхронное копирование кончится. Ведь начнётся оно только тогда, когда всю очередь до него разгребут.\\
    После этого нужно сделать \mintinline{c++}|cudaFree|, чтобы освободить выделенную память.\\
    А ещё в самом конце рекомендуют писать \mintinline{c++}|cudaDeviceReset|. Во-первых, после этой команды если вы под профилировщиком, он начнёт выводить результаты. Во-вторых, есть ситуация, что если не вызвать эту функцию, видеокарта может не перейти в режим низкого энергопотребления (а остаться в режиме <<мы активно вычисляем>>).\\
    Как теперь превратить это из суммы двух чисел в сумму двух массивов? Для начала надо изучить понятие SIMT~--- single instruction multiple threads. Одно и то же будет выполняться кучей тредов. Как эти треды создать? Для начала надо понять числа в тройных угловых скобках. Первое~--- сетка, второе~--- блок. Блок~--- тесно связанная группа потоков. Размер блока ограничен вашей видеокартой, и его можно получить из \mintinline{c++}|cudaGetDeviceProperties| и он следует из compute capability. А первый параметр~--- сколько блоков запускать. Вообще индексация может быть и двухмерной, и трёхмерной, но это нам пока не надо. Пригодится это для матриц.\\
    Так как, собственно, складывать массивы? А вот так. В нашей \mintinline{cu}|__global__|-функции можно получить \mintinline{cu}|threadIdx.x|. Это номер треда внутри блока. Ещё есть \mintinline{cu}|blockDim| и \mintinline{cu}|gridDim|. Они тоже трёхмерные, это количество тредов внутри блока и количество блоков в сетке. И ещё есть, конечно же, \mintinline{cu}|blockIdx|~--- индекс блока внутри общей сетки блоков. Из интересного ещё есть \mintinline{cu}|warpSize|, но про варпы мо поговорим потом. На NVIDIA это уже очень давно (и не собирается меняться) 32. Итого пишем следующее:
    \begin{minted}{cu}
        __global__ void add(const int* a, const int* b, int* c)
        {
            int i = threadIdx.x + blockIdx.x * blockDim.x;
            c[i] = a[i] + b[i];
        }
    \end{minted}
\end{document}