\documentclass{article}
\input{Headers/header}
\input{Headers/formal}
\input{Headers/graphs}

\usepackage[outputdir=Output, cache=false]{minted}

\DeclareMathOperator{\Expected}{\mathsf{E}}
\undef\Variance
\DeclareMathOperator{\Variance}{\mathsf{D}}
\DeclareMathOperator{\Bias}{Bias}

\begin{document}
    \tableofcontents
    \section{Основы.}
    \begin{itemize}
        \item Exploratory DA~--- анализ (чаще всего глазками) данных, чтобы найти в нём пропуски, выбросы и прочий мусор, который вы можете глазами найти. Тут же вы думаете о том, чем эти данные являются, и как их хранить. А ещё тут вы можете формировать базовые гипотезы.
        \item Confirmatory DA~--- не рассказали, что такое.
        \item Показательный анализ данных~--- машинное обучение с целью что-нибудь предсказывать.
        \item Визуализация данных.
    \end{itemize}
    Знание~--- закономерность в некоторой области, которые позволяют решать проблемы.\\
    Data Mining~--- один из этапов в извлечении знаний из данных. Тут три этапа:
    \begin{enumerate}
        \item Сбор данных.
        \item Выделение признаков.
        \item Применение ML.
    \end{enumerate}
    Фактически это как DA, но с акцентом на немного другое.\\
    Data Science:
    \begin{itemize}
        \item Сбор данных.
        \item Интеграция данных (data integration).
        \item Хранение данных (data warehousing).
        \item Анализ данных.
        \item Высокопроизводительные вычисления (high-performance computing).
    \end{itemize}
    Как это с точки зрения бизнеса выглядит? Есть CRISP-DM:
    \begin{enumerate}
        \item Понимание бизнеса (перевод слов заказчика на что-то понятное).
        \item Понимание данных (требование правильных данных от заказчика).
        \item Подготовка данных (понять, как хранить данные).
        \item Моделирование (собственно, решать задачу).
        \item Оценка (понять, что мы получили).
        \item Если всё хорошо, внедряем, заворачиваем в красивую обёртку и т.п.
    \end{enumerate}
    \paragraph{Данные.}
    Данные бывают структурированные, неструктурированные и частично структурированные. Чаще всего мы встречаем неструктурированные. Это видео, посты в соцсетях и любая другая хрень, которую мы встречаем. Частично структурированные~--- XML, JSON и прочий мусор, который до полностью структурированных не дотягивает. А полностью структурированные данные~--- это матрицы (из $n$ объектов по $m$ характеристик у каждого).\\
    Имея неструктурированные или частично структурированные данные, мы почти всегда хотим его структурировать, чтобы было удобнее.\\
    В DA строку матрицы называют object, instance, sample, example и как угодно ещё, а столбец матрицы~--- feature, attribute, characteristic или factor.\\
    Что важно про наши матрицы? Чтобы порядок столбцов и строк был не важен. Если ваш алгоритм не такой, это странно. Исключение: временной ряд.
    \paragraph{Признаки.}
    Глобально их два: категория и число (качество и количество). Первое дискретно, второе непрерывно, категории мы можем только на равенство проверять, а с числами можем арифметику делать. Категориям обычно целое число сопоставляют, чтобы удобно было. Но есть и другие типы, например, порядковый: дискретный, но есть порядок. Впрочем, очень часто он сводится к либо к категории, либо к числу.\\
    С числами работать гораздо удобнее, но если надо, легко можно дискретизировать наше число любом образом (например, баллы в оценку~--- дискретизация). Как преобразовать порядковый тип во что-то? Можно преобразовать в число через порядковый номер. А можно в категорию, создав $k$ бинарных значений вида ${}<x$.\\
    \subparagraph{One-hot encoding.}
    Берём категорию из $k$ значений, преобразуем в $k$ булевых категорий вида ${}=c_i$, а потом из истины сделать 1, а из лжи~--- 0. Отличный план, как из категории сделать число.\\
    Ещё можно преобразовывать иначе, например категории сопоставлять случайный вектор или число, бинарное представление которого используется как новые признаки.\\
    Важное примечание: числа~--- не всегда числа. Например, если мы цифры распознаём, то плохо считать числа числами, а не категориями. Потому что изображение тройки не находится между изображением двойки и четвёрки. Поэтому аккуратно с преобразованием категорий в числа.\\
    Ещё интересность: время. Если у нас есть периоды, то хреначим синус и косинус, чтобы периодично было:
    \[
    f_{2e-1}=\sin\frac{2\pi t}{T_e}\qquad\qquad f_{2e}=\cos\frac{2\pi t}{T_e}
    \]
    Или ещё интересность, цвет. Цвет кодируем в RGB, а не в HSV/HSB, потому что там тон цвета неоднозначный. Красный~--- это и 0, и 360.\\
    Как хранить данные? Самый простой формат: CSV. Он плохо стандартизирован, правда. А ещё его недостаток в том, что там шапки нет. Нет ни информации о типах, ничего. Если хочется более человеческого~--- ARFF: примерно как CSV, но более стандартизирован и имеет шапку с типами данных. Типы: Numeric (число), Nominal (категория), String и Date.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.9\linewidth]{Images/ARFF-example}
        \label{fig:arff-example}
    \end{figure}\noindent
    Прочие типы данных.
    \begin{itemize}
        \item Картинка. Самое простое~--- двух- (или трёх-)мерная матрица. И есть 1000 алгоритмов, как привести её в один вектор (притом довольно маленький).
        \item Текст. Тут сложнее. Самое тупое~--- посчитать частоту слов. Но не надо, тут сразу двигайте в сторону LLM.
        \item Звук. Это сигнал во времени, и самый простой вариант его представить~--- вектор значений. Для него тоже есть всякие штуки типа спектрограммы Мэла, чтобы преобразовывать звук во что-то более адекватное.
        \item Видео. Это мультимодальный объект, то есть объект, в котором разные типы внутри. Их анализировать тоже своими методами.
    \end{itemize}
    \paragraph{Нормализация.} Зачем? Чтобы признаки с большей дисперсией не влияли на результат сильнее остальных.\\
    Как? Сначала отбросим единицы измерения. Чтобы это было корректно, алгоритм должен быть инвариантен относительно линейных преобразований над признаками. А дальше надо как-то нормализовать дисперсию. Можно, например, вычесть из каждого признака минимум и поделить на разность между максимумом и минимумом. Это $[0;1]$-масштабирование. А ещё можно вычесть (выборное) мат. ожидание из признака и поделить на (выборное) стандартное отклонение это Z-масштабирование. Способов ещё куча, но это два простых.\\
    Как жить с ситуацией, когда у нас признаки зависимы? Никак не жить, если так, вы плохо собрали данные.
    \paragraph{Веса и сэмплирование.}
    Отлично, мы всё нормализовали, а теперь у нас из специфики задачи какие-то признаки более важны. Или могут быть объекты более важны.\\
    Говорят, что объект/признак обладает весом $w$, если он в $w$ раз больше влияет на результат. Если $w$ целое, то это эквивалентно тому, что объект/признак встречается $w$ раз.\\
    Примеры: учёт веса признака
    \[
    \operatorname{dist}(a;b)=\sqrt{\sum\limits_{j=1}^mw_j(a_j-b_j)^2}
    \]
    учёт веса объекта:
    \[
    \mathfrak L_D(\theta)=\sum\limits_{x\in D}w(x)\mathfrak L(x;\theta)
    \]
    А есть сэмплирование: вместо вхождения кучу раз в датасет мы можем сэмплировать объекты с вероятностью $\frac{w_j}{\sum w_j}$.\\
    Ещё это может использоваться для балансировки. Если вы знаете, что реальное распределение не очень сочетается с распределением данных, то мы можем пореже выбирать объекты мажоритарного класса или почаще объекты миноритарного класса (например, в данных одна группа большая, а другая~--- маленькая, а в жизни они равны, и мы даём большие веса малой группе). Да и в принципе, если нас не интересует распределение признаков по классам, распределение будет чаще подсовывать вам штуки из мажоритарного класса, что вам не всегда хочется.\\
    Как сэмплировать? Можно рандомно, можно каждый $x$-тый объект (систематически), можно стратифицированно (ровно в соотношение с какой-то категорией), можно кластерное (если кто-то уже за нас разбил объекты на множества, их множеств можно выбрать несколько).
    \paragraph{Шум.}
    Аномалии~--- плохие объекты для модели. Ошибки~--- плохие объекты для реальности. Пример: пусть у нас есть расход топлива автомобилями. И у нас у одного указан 30л / 100км. Если это грузовой автомобиль, а остальные~--- легковые, то это аномалия. А если объект всем хорош, и данные такие, то это ошибка (а возникнуть такое могло из-за того, что кто-то подразумевал мили на галлон).\\
    Пропуски в наборе данных. Откуда? Из склейки разных наборов данных, из изначально херовых данных или откуда угодно ещё. В CSV нет стандартного способа это обозначить, в ARFF это вопросик, а ещё можно специальное значения: None/Null для строк, категория с несуществующим номером для категорий или NaN для числа. Что делать с этим? Вырезать (признак или объект), заменить (заполнить пропуск средним арифметическим/модой/алгоритмом предсказания, который умеет в пропуски) или добавить (добавить новый признак, который говорит, были ли пропуск или нет; если там была категория, можно отдельное значение категории туда добавить, а не новое свойство делать).
    \\\\\\
    \section{Терминология.}
    \paragraph{Виды искусственного интеллекта.}
    \begin{itemize}
        \item \textbf{Слабый (узкий) ИИ}. Нацелен на решение специфичной задачи, причем способ ее решения не подходит для решения других задач.
        \item \textbf{Сильный (общий) ИИ}. Способен решать множество задач, достигает или превосходит человеческие способности.
        \item \textbf{AI-полная задача}~--- задача, решение которой предполагает создание сильного AI.
        \item \textbf{Проблема ускользающей цели}~--- по мере решения задач ИИ их выписывают из списка задач сильного ИИ и относят к задачам слабого ИИ.
        \item\textbf{Интеллектуальная система}~--- система, решающая одну или несколько задач ИИ.
        \item Помимо машинного обучения выделяют \textbf{экспертную систему}, построенную на основе фактов и правил, извлекаемых из экспертов.
    \end{itemize}
    Отличия экспертной системы от ML:
    \begin{itemize}
        \item Экспертная система строится не от частного к общему, как ML, а от общего к частному.
        \item Экспертная система получает решение не с помощью обучения на большом количестве данных, а с помощью привлечения экспертов (лингвистов) для формализации правил и выстраивания паттернов.
    \end{itemize}
    \paragraph{Связь ML и других дисциплин..}
    \underline{Статистика.} Цель статистики~--- на основе большого числа данных выявить паттерны, по которым строятся эти данные (т.е. распределения и другое). ML не всегда основано на статистике, но часто является <<\textit{более сильной версией}>> статистики.\\
    \underline{Анализ данных.} Пока ML~--- разработка алгоритмов, DA~--- это их применение.
    \paragraph{Устройство машинного обучения глобально.}
    Определение 1: машинное обучение~--- процесс, дающий компьютерам способность обучаться новому, не будучи непосредственно запрограммированными делать это.\\
    Определение 2: программа обучается с опытом $E$ решению некоторой задачи $T$, по метрике качества $P$, если качество ее решения задачи $T$, измеренное согласно $P$, растет вместе с увеличением $E$.\\
    Задача машинного обучения состоит из:
    \begin{itemize}
        \item Набора данных $\mathcal{D}$.
        \item Целевой функции~--- либо функция качества (выигрыша, правдоподобия) $\mathcal{Q}$, либо функция потерь (ошибок, риска) $\mathcal{L}$.
    \end{itemize}
    Алгоритм учится решать задачу, если он максимизирует функцию качества (или минимизирует функцию потерь) для заданного набора данных ($\mathcal Q_{\mathcal D}$ или $\mathcal L_{\mathcal D}$ соотвественно).\\
    В задачах оптимизации у нас есть некая функция потерь $\mathcal{L}$, для которой мы хотим найти минимум, то есть такой вектор параметров $\theta$, что $\mathcal{L}(\theta)\rightarrow\min$. В задачах машинного обучения помимо параметров есть еще данные. Если мы поменяем датасет, минимум функции тоже поменяется. Чтобы их учесть, давайте на каждом объекте набора данных измерим функцию потерь для этого объекта с конкретными параметрами и будем минимизировать сумму этих значений по всем объектам:
    \[\mathcal{L}_{\mathcal{D}}(\theta)=\sum_{x\in\mathcal{D}}\mathcal{L}(x, \theta)\rightarrow\min\]
    Эту сумму можно было бы поделить на размер набора данных, но поскольку он всегда константный вне зависимости от $\theta$, можно минимизировать без него.
    \paragraph{Аппроксимация функции ошибки.}
    Иногда функция качества или функция потерь, которую сообщает заказчик, не формализуется или слишком сложно вычислима. В таком случае выбирается другая функция, которая определяется конкретной задачей машинного обучения. Почему так норм? Ну, например, потому, что настоящая функция потерь уже наверняка была упрощена при формализации потери. Или потому, что при подсчёте истинной функции потерь мы будем работать непростительно долго.\\
    Аппроксимация применяется не только по отношению к функциям: например, в реальной жизни данных может быть потенциально бесконечное количество, но в задаче мы ограничиваемся конечным набором (выборкой).
    \paragraph{Алгоритм машинного обучения в деталях.}
    Обычно у алгоритмов в жизни есть параметры и ввод. И исходя из них алгоритм выдаёт какой-то вывод. Например, утилите \Verb|ffmpeg| вы скармливаете видеофайл(ы) и опции вида \Verb|-b:v|, \Verb|-b:a| и любые другие. А она вам выдаёт какие-то другие видеофайл(ы). Тут ситуация несколько другая:
    \begin{figure}[H]
        \includegraphics[width=0.8\linewidth]{Images/ml_scheme.png}
        \label{img:ml_scheme}
    \end{figure}\noindent
    Мы хотим сделать алгоритм предсказания, который по объекту делает предсказания. Но тут у нас параметры нам даёт не пользователь, а мы делаем их сами. А точнее, пишем специальный алгоритм (алгоритм обучения), который их нам и сделает. И это уже обычный алгоритм, с обычным входом и выходом. Его вход~--- набор данных, его выход~--- параметры алгоритма предсказания, а его параметры называются специальным словом <<\textbf{гиперпараметры}>>. То есть параметры~--- это параметры модели, которые меняются в процессе обучения, а гиперпараметры~--- фиксированные параметры алгоритма обучения.\\
    Пример: аппроксимация полиномом. Мы решили, что наш алгоритм предсказания будет выглядеть как
    \[
    y=a+bx+cx^2+dx^3+\cdots
    \]
    Где $x$~--- вход, $y$~--- выход, а $a,b,c,d,\ldots$~--- параметры. И для обучения мы себе выбрали, что будем искать только полиномы размерности 2. Тогда вот это вот <<2>>~--- гиперпараметр, точки~--- набор данных, а $a,b,c,0,0,\ldots$ будут нашими параметрами.
    \subparagraph{Уровни модельности алгоритмов.}
    \begin{enumerate}
        \item \textbf{Безмодельные алгоритмы} (<<in-place>>) решают задачу без явного построения модели. Самый тупой пример~--- нахождение матожидания и использование его как результат всех предсказаний. То есть in-place алгоритм не имеет обучения и параметров, он просто запоминает данные и гиперпараметры, и сразу на основе них делает предсказание.
        \item \textbf{Моделирующие алгоритмы} обучаются один раз и строят модель, которую потом можно переиспользовать без повторного обучения, но нельзя обновить при поступлении новых данных. Пример~--- алгоритмы кластеризации.
        \item \textbf{Алгоритмы онлайн-обучения} имеют возможность переобучаться при добавлении новых данных.
        \item Возможность объединить две модели.
    \end{enumerate}
    \paragraph{Определение понятий машинного обучения.}
    Непонятно, что называть решением задачи машинного обучения: алгоритм обучения модели, обученную модель или результат ее применения? Также непонятно, от чего вычисляется функция оценки качества: от алгоритма, от модели или от ее применения. Ответ: зависит от ситуации. Обозначим за $A$ алгоритм обучения, за $\theta$~--- полученные в результате обучения параметры, за $a_{\theta}$~--- полученную в результате обучения модель с параметрами $\theta$. Введем следующие функции оценки качества:
    \begin{enumerate}
        \item $\mathcal{L}(A,\mathcal{D})$~--- функция от алгоритма обучения
        \item $\mathcal{L}_{\mathcal{D}}(a_{\theta})$~--- функция от обученной модели
        \item $\mathcal{L}(\widehat{y},y)$, $\widehat{y}=a_{\theta}(x)$~--- функция для результата применения, то есть отклонение истинного результата от предсказанного обученной моделью
    \end{enumerate}
    Очевидно, нам хочется, имея одну из этих функций, вычислять остальные. Поэтому введем три понятия:
    \begin{enumerate}
        \item \textbf{Валидация}~--- вычисление $\mathcal{L}(A,\mathcal{D})$ из $\mathcal{L}_{\mathcal{D}}(a_{\theta})$.
        \item \textbf{Тестирование модели} $a_{\theta}$ на наборе данных $\mathcal{D}$~--- вычисление $\mathcal{L}_{\mathcal{D}}(a_{\theta})$ из $\mathcal{L}(\widehat{y},y)$.
        \item \textbf{Применение}~--- вычисление $a_{\theta}(x)$.
    \end{enumerate}
    \subparagraph{Валидация на отложенных данных.}
    Пусть мы хотим понять обобщающую способность нашей модели, но у нас есть ограниченный набор данных. Мы хотим убедиться, что наша модель будет выдавать хорошие результаты за пределами датасета, на котором ее обучали (для этого мы ее и обучаем). Обычно в таких случаях исходный датасет разбивают на training и test часть случайным образом. (Вообще, необязательно случайным; например, если объекты могут составить временной ряд, то разбивать объекты лучше по времени.) Тогда:
    \[\mathcal{L}(A,\mathcal{D})=\mathcal{L}(A(\mathcal{D}_{\mathrm{train}}),\mathcal{D}_{\mathrm{test}})\]
    \paragraph{Сравнение алгоритмов.}
    Чтобы сравнить в машинном обучении два алгоритма, один из которых не является частным случаем другого (тогда очевидно, что общий алгоритм лучше), нужно, во-первых, ввести метрику их сравнения, а во-вторых сравнивать на одинаковых наборах данных. Под метрикой подразумевается уже знакомая нам функция качества от алгоритма $\mathcal{L}(A,\mathcal{D})$, а также способ ее вычисления. Часто при решении задачи берут какой-то наивный алгоритм (baseline) и сравнивают остальные алгоритмы с ним. Теорема <<No free lunch theorem>> гласит, что алгоритм может работать хорошо только на определенном наборе данных, а на остальных данных проседает, т.е. нельзя покрыть одним алгоритмом все, нужно выбирать исходя набора данных в данной задаче. Лучший алгоритм для конкретной задачи с конкретным набором данных и конкретной метрикой называется State-of-the-art (SOTA).
    \paragraph{Стандартные задачи машинного обучения.}
    \begin{figure}[H]
        \begin{tabularx}{0.8\textwidth}{|X|XXX|}
            \hline
            $a\colon X\to Y$ & $Y=\{y_1,\ldots,y_k\}$ & $Y=\operatorname{Pr}^k$ & $Y=\mathbb R^k$\\
            \hline
            Обучение с учителем & Классификация & Мягкая классификация & Восстановление регрессии\\
            \hline
            One-class classification & Поиск аномалий & Восстановление плотности & {\color{red}Генерация объектов}\\
            \hline
            Обучение без учителя & Кластеризация & Мягкая кластеризация & Выделение признаков\\
            \hline
        \end{tabularx}
    \end{figure}\noindent
    Что здесь происходит? Задачу машинного обучения можно представить как отображение из $X$ в $Y$.
    В качестве столбцов у нас то, что может быть результатом:
    \begin{itemize}
        \item $Y=\{c_1,c_2,...,c_n\}$: результатом является сопоставление класса каждому объекту из набора. 
        \item $Y=\operatorname{Pr}^k$: результатом является сопоставление вектора вероятностей размера $k$ каждому объекту из набора.
        \item $Y=\mathbb{R}^k$: результатом является одно число или вектор чисел размера $k$.
    \end{itemize}
    В качестве строк выступает то, что служит источником для построения модели:
    \subparagraph{Обучение с учителем}
    В качестве источника выступают размеченные данные, то есть выборка, содержащая верные ответы. Цель алгоритма~--- научится эти правильные ответы предсказывать.
    \begin{itemize}
        \item \textbf{Задача классификации}~--- задача сопоставления каждому объекту категории на основе распределения по категориям в обучающем датасете. Наивное решение~--- всегда брать моду (самый популярный класс). Примеры: определить, является и письмо спамом или определить, какая цифра на картинке.
        \item \textbf{Задача мягкой классификации}~--- задача сопоставления объекту вектора, где каждое число обозначает уверенность в попадании объекта в конкретный класс. Выделяют вероятностную классификацию, где вектор должен быть корректным вероятностным вектором.
        \item \textbf{Задача восстановления регрессии}~--- задача предсказания ответа для произвольного объекта на основе сопоставления ответов объектам в обучающем датасете. Наивное решение~--- брать мат. ожидание.
    \end{itemize}
    Ещё в случае обучения с учителем $Y$ может быть перестановкой или рангом. В таком случае мы пытается установить порядок на наших объектах. Используется это крайне редко.\\
    Особняком стоят задачи прогнозирования временных рядов, где при имеющихся $y_{t-m},\ldots,y_{t-2},y_{t-1}$ надо предсказать $y_t$. Самый яркий пример~--- цены акций. Особняком они потому, что у них нет признаков (кроме, собственно, $y_t$). Где такие взять? Ну, например, считать, что
    \[
    x_t=(y_{t-m},\ldots,y_{t-2},y_{t-1})
    \]
    Это называется авторегрессия. Или можно брать какую-то функцию от $t$, это конструирование признаков. Наивное решение~--- арифметическое скользящее среднее или экспоненциально скользящее среднее.
    \begin{figure}[H]
        \includegraphics[width=0.8\linewidth]{Images/Supervised-learning-problem-convertions}
    \end{figure}\noindent
    \subparagraph{One-class classification}
    В качестве источника выступают данные, почти все из которых принадлежит одному классу (известно для каждого объекта, принадлежит он или нет).
    \begin{itemize}
        \item \textbf{Задача поиска аномалий}~--- среди существующих объектов нужно найти объекты другого класса.
        \item \textbf{Задача новизны}~--- среди каких-то новых объектов нужно найти объекты другого класса.
        \item \textbf{Задача восстановления плотности}~--- требуется восстановить плотность распределения и для новых объектов определить, кто из них выбивается из распределения. Обычно используется метод максимального правдоподобия, где мы оцениваем параметры распределения на основе объектов из выборки. Подробнее~--- на матстате.
        \item \textbf{Задача генерации новых объектов}~--- по данным объектам нужно сгенерировать новые.
    \end{itemize}
    Не путайте генерацию с сэмплированием (выбрать объект из существующих) и аугментацией (примитивными аналитическими операциями сгенерировать новые объекты, например, повернуть картинку котика, чтобы получить новую картинку котика). Это другие вещи, но их можно использовать как наивное решение.\\
    Из интересного, чтобы оценить качество генерации используются алгоритмы классификации (на два класса: реальный объект или сгенерированный). А ещё можно сохранять некоторую статистику между реальными объектами, тогда классификация разделяется на два вида моделей:
    \begin{itemize}
        \item Генеративные~--- обучают совместное распределение вероятностей, и задача классификации сводится к задаче восстановления плотности.
        \item Дискриминативные~--- обучают условное распределение, и модель пытается найти разделяющее правило.
    \end{itemize}
    Обычно используются в паре, но об этом подробнее во втором семестре.
    \subparagraph{Обучение без учителя.}
    В качестве источника выступают неразмеченные данные, для которых требуется самостоятельно придумать целевой признак $\widehat Y$. И дальше уже распределение на задачи ведётся в зависимости от того, каким является наш $\widehat Y$ (а не $Y$, как было в таблице).
    \begin{itemize}
        \item \textbf{Задача кластеризации}~--- требуется разделить объекты на подмножества (кластеры), чтобы в каждом кластере объекты были максимально похожи друг на друга. Пример: вы пишете Твиттер, и хотите разделить пользователей по информационным пузырям.
        \item \textbf{Задача мягкой кластеризации}~--- аналогично мягкой классификации.
        \item \textbf{Задача выделения признаков}~--- алгоритм должен отображать объект из $X$ в пространство (чаще всего меньшей размерности), которое он сам и придумает. Самое наивное~--- умножим на рандомную матрицу. Используется для уменьшения размерности, чтобы, например, нарисовать наши данные.
        \item \textbf{Задача конструирования признаков}~--- по сути, более общая задача извлечения признаков: дана какая-то абстрактная штука (картинка, текст, etc), хочется сделать себе вектор признаков. Решается явно, а не алгоритмами ML. Пример: пытаемся наложить шаблон на изображения, суммируем по всем возможным наложениям. Получаем новый признак для каждого шаблона. А вот шаблоны уже можно искать машинным обучением.
    \end{itemize}
    \subparagraph{Другие задачи машинного обучения.}
    \begin{itemize}
        \item Можно не только принимать на вход пропуски, но и возвращать. Трактуется как отказ от классификации/кластеризации. Первое используется в ансамблях, второе~--- в поиске аномалий.
        \item \textbf{Предсказание и заполнение пропусков}. Выберем признак с пропусками целевым, остальные заполним как-нибудь, натренируем модель на данных без пропусков, заполним пропуски результатом тестового прогона.
        \item \textbf{Коллаборативная фильтрация}. Дано множество оценок пользователями. И оценок, блин, мало (существенно меньше произведения числа пользователей и вещей). Требуется предсказать оценку данной вещи данным пользователем. Решается трудно, этим занимается специальный раздел (рекомендательные системы), очевидно, тем же алгоритмом, что заполнение пропусков, оно не решается, зато наоборот (заполнить пропуски совместной фильтрацией)~--- норм план.
        \item \textbf{Обучение на привилегированных данных}. Тренировочные данные содержат дополнительную информацию ($X'$), недоступную при тестировании. Базовое решение: либо не использовать $X'$, либо взять другую модель, которая предсказывает $X'$, и пользоваться ей, либо честно самому предсказывать и $X'$, и $Y$. Пример: предсказываем результат футбольного матча, и наши привилегированные данные~--- число красных и жёлтых карточек, например. На тестовых данных такое неизвестно.
        \item \textbf{Обучение на частично размеченных данных} (оно же \textbf{обучение с частичным привлечением учителя}). Лишь малая часть тренировочных данных (и никакая часть тестовые данные) содержит правильный ответ. Наивное решение: не использовать разметку (т.е. обучаться полностью без учителя) или не использовать неразмеченные объекты.
        \item \textbf{Активное обучение}~--- вариация на тему предыдущего. Можно задавать Оракулу вопросы о значении меток, и надо за минимум обращений к Оракулу восстановив $f\colon X\to Y$. То есть по сути надо не функцию найти даже, а стратегию выработать, как обращаться к Оракулу.
        \item Обучение с подкреплением (Reinforcement Learning). Есть агент и есть среда. Агент взаимодействует со средой, среда меняет состояние и возвращает награду. Задача: максимизировать награду. Пример: задача об одноруком бандите, он что-то знает об одноруком бандите, и должен выработать стратегию, как максимизировать выигрыш.
    \end{itemize}
    \section{Обучение с учителем.}
    Если алгоритм работает хуже или сравнимо с наивным (наивный, напомню, берёт моду / мат. ожидание меток), значит или алгоритм плох (и не выявляет зависимость $Y$ от $X$), либо данные плохи (и в них $Y$ не зависит от $X$).\\
    Сведение к бинарной классификации: пусть у нас есть алгоритм бинарной вероятностной классификации. Как из него многоклассовую сделать?
    \begin{itemize}
        \item <<Один против всех>>. Заданный класс помечаем как единицу, остальные~--- как минус единицу. Возьмём $k$ таких классификаторов, у кого вероятность больше, того и берём.
        \item <<Один против одного>>: обучим $\frac{k(k-1)}2$ классификаторов $a^b_{u,v}$: для каждой пары классов $u,v$ выберем множество объектов $X_{u,v}$, принадлежащих одному из этих классов. Обучим на этом множестве классификатор $a^b_{u,v}$ предсказывать вероятность того, что объект принадлежит $u$, после чего сделаем себе
        \[
        a(x)=\operatorname*{argmax}_c\prod\limits_va^b_{c,v}(x)\cdot\prod\limits_u(1-a^b_{u,c}(x))
        \]
        Тут обучаем много классификаторов, но для каждого выборки поменьше.
    \end{itemize}
    На мягкой классификации тоже работает.
    \paragraph{Настройка гиперпараметров как часть обучения.}
    Помните \hyperref[img:ml_scheme]{красивую идеалистичную картинку} про гиперпараметры, параметры и данные? Так вот на самом деле ситуация иная:
    \begin{figure}[H]
        \includegraphics[width=0.8\textwidth]{Images/ml_scheme_advanced}
    \end{figure}\noindent
    Естественно, гиперпараметры тоже надо настраивать, и не руками. Делим нашу выборку на две части: $X_{\mathrm{train}}$ и $X_{\mathrm{valid}}$. И гиперпараметры у нас не одни, а несколько комбинаций. Мы кидаем этот набор в некоторый алгоритм перебора гиперпараметров, он нам даёт какие-то гиперпараметры, кидаем его в обучение, из них и $X_{\mathrm{train}}$ получаем параметры. Дальше из алгоритма предсказания и $X_{\mathrm{valid}}$ получаем функцию ошибки. Из неё меняем гиперпараметры. Повторим до тех пор, пока всё не будет хорошо. Когда будет~--- засунем уже весь набор в обучение и получим финальные параметры модели.\\
    $X_{\mathrm{valid}}$ называется \textbf{валидационным множеством}. Иногда финальные параметры модели не берут заново, а берут ровно последний получившийся из обучения на $X_{\mathrm{train}}$ алгоритм. Ещё из интересного алгоритм обучения обычно неявно настраивает гиперпараметры (если может это сделать). Например, градиационный спуск имеет гиперпараметром количество шагов, но он итеративный алгоритм, и он сам примерно знает, когда его останавливать.\\
    \textbf{Начальное состояние генератора псевдослучайных чисел нельзя настраивать!}\\
    Как настраивать гиперпараметры? Поиск по сетке, случайный поиск эволюционные алгоритмы, или, из современного, байесовская оптимизация.
    \paragraph{Переобучение и регуляризация.}
    Начиная с определённого уровня сложности модели, чем лучше он работает на тренировочных данных, тем хуже на реальных. Пример: у нас есть $y(x)=\frac1{1+25x^2}$. Мы не знаем $y$. И мы аппроксимируем это чудо полиномом. Притом всё делаем на $[-2;2]$. Тогда где-то до степени 15 ошибка на контрольной выборке падает, а потом резко начинает расти. Выглядит это примерно так:
    \begin{figure}[H]
        \includegraphics[width=0.8\textwidth]{Images/ml_overfitting_example}
    \end{figure}\noindent
    Тут взята степень где-то 32, белые точки~--- обучающая выборка, красные~--- контрольная. Видно, что ошибка модели на контрольной модели огромна, а на обучающей она примерно ноль.\\
    Аналогично есть недообучение, когда нам просто не хватает сложности модели:
    \begin{figure}[H]
        \includegraphics[width=0.8\textwidth]{Images/ml_overfitting_underfitting}
    \end{figure}\noindent
    Тут в качестве ошибки берётся MSE, и показаны значения ошибки для недообученной и переобученной модели.\\
    В реальной жизни такие картиночки мы не построим, придётся бороться как-то иначе. Для этого есть регуляризация. Это добавление ограничений с целью предотвратить переобучение. ПО сути это штраф за сложность модели:
    \[
    \scriptL_{\mathrm{reg}}(\theta,\scriptD)=\scriptL(\theta,\scriptD)+\lambda\cdot\operatorname{complexity}(\theta)
    \]
    Тут $\lambda$~--- константа, и мы минимизируем $\scriptL_{\mathrm{reg}}(\theta,\scriptD)$.\\
    Это мягкая регуляризация, а есть ещё жёсткая: минимизировать $\scriptL(\theta,\scriptD)$ при условии, что $\operatorname{complexity}(\theta)<C$ ($C$~--- новый гиперпараметр, являющийся ограничением на сложность).
    \subparagraph{Статистическое обоснование переобучения.}
    \begin{itemize}
        \item \textbf{Смещение} (bias)~--- погрешность оценки, возникающая в результате ошибочного предположения в алгоритме. Высокое смещене~--- недообучение.
        \item \textbf{Дисперсия} (variance)~--- ошибка чувствительности к малым отклонениям в тренировочным наборе. Высокая дисперсия~--- переобучение.
    \end{itemize}
    Откуда это всё? Пусть $f$~--- реальная зависимость, $\widehat y$~--- предсказание модели, $y=f(x)+\varepsilon$~--- измерение с шумом $\varepsilon$, где $\Expected[\varepsilon]=0$, $\Variance[\varepsilon]=\sigma^2$. Тогда
    \[
    \Expected[y]=f\qquad\Variance[y]=\sigma^2
    \]
    А отсюда ожидание квадрата ошибки:
    \[\begin{split}
        \Expected\left[(y-\widehat y)^2\right]&=\Expected\left[y^2+{\widehat y}^2-2y\widehat y\right]=\\
        &=\Expected\left[y^2\right]+\Expected\left[{\widehat y}^2\right]-\Expected\left[2y\widehat y\right]=\\
        &=\Variance[y]+f^2+\Variance[\widehat y]+\Expected[\widehat y]^2-2f\Expected\left[\widehat y\right]=\\
        &=\Variance[y]+\Variance[\widehat y]+\left(f-\Expected[\widehat y]\right)^2=\\
        &=\sigma^2+\Variance[\widehat y]+\Bias[\widehat y]^2
    \end{split}\]
    То есть тут неустранимая погрешность измерений, дисперсия модели и квадрат смещения модели.
    \subparagraph{Двойной спуск.}
    Не так давно умные люди показали, что с определённой точки ошибка на тестовых данных снова начнёт падать. Нам это полезно, чтобы нейронные сети не переобучались (потому что в них параметров огромное количество, и возникает вопрос, почему они не переобучаются). Кому очень интересно, почитайте Belkin M., Shu, D.J., Ma, S., \& Mandal, S. (2018). Reconciling modern machine learning and the bias-variance trade-off.\\
    Но работает это только для супер-больших моделей, пока мы не пишем нейронки, у нас всё ещё есть классический bias-variance trade-off.
    \paragraph{Перекрёстная проверка.}
    Пусть $|\scriptD_{\mathrm{train}}|=r$, $|\scriptD_{\mathrm{test}}|=e$. Разобьём наш набор данных всеми возможными способами на $\scriptD_{\mathrm{train}}$ и $\scriptD_{\mathrm{test}}$. Тогда хотелось бы в качестве валидации использовать
    \[
    \scriptL(A,\scriptD)=\frac1{\binom{e+r}e}\sum\limits_{\scriptD=\scriptD_{\mathrm{train}}\cup\scriptD_{\mathrm{test}}}\scriptL(A(\scriptD_{\mathrm{train}}),\scriptD_{\mathrm{test}})
    \]
    Тут мы точно не потеряем обобщающую способность. Но тут нам надо построить и обучить огромное количество моделей, поэтому такой подход (полная кросс-валидация) не используется.\\
    Вместо этого используют кросс-валидацию по $k$ блокам: распилим наш набор данных на $k$ частей, каждая часть будет тестовой ровно один раз. Модель мы будем строить всего $k$ раз таким образом. $k$ бывает 5 либо 10.\\
    Крайний случай~--- валидация по одному объекту Это полная кросс-валидация, где $e=1$.\\
    Ещё бывает стратифицированная кросс-валидация. Если объекты каких-то классов недостаточно часто встречаются, лучше разбивать на блоки так, чтобы в каждом блоке эта статистика была пропорциональна статистике выборки.\\
    Если у нас задача временного ряда (т.е. ранние блоки дают более плохой результат), то тут среднее по всем разбиениям заменяется на среднее взвешенное среднее с экспоненциально растущими весами блоков.\\
    Важно:
    \begin{itemize}
        \item Усреднять суммой можно только аддитивные функции ошибки/качества.
        \item Для усреднения можно использовать $t$ кросс-валидацию по $k$ блокам (повторить кросс-валидацию по $k$ блокам $t$ раз).
        \item Если кросс-валидация используется для настройки гиперпараметров, модель требуется заново обучить на всём наборе данных.
    \end{itemize}
    \subparagraph{Другие причины переобучения.}
    Перекрёстная проверка не поможет вам в следующих случаях:
    \begin{itemize}
        \item Нерепрезентативные (смещённые) данные.
        \item Плохо подобранная метрика качества измерения алгоритма.
        \item Систематические смещения в методах валидации.
        \item Непонимание скрытых гиперпараметров.
    \end{itemize}
    Короче, если вы продолбались сами или данные отстойные, то вам ничего не поможет.
    \subsection{Оценка задачи классификации.}
    Поскольку с категориями нельзя делать ничего, кроме проверки из на равенство, заведём себе вот такую точность:
    \[
    \operatorname{Accuracy}(y;\widehat y)=\frac1n\sum\limits_{i=1}^n[y_i=\widehat y_i]
    \]
    Где $n$~--- количество объектов в выборке, $y$~--- вектор реальных ответов, а $\widehat y$~--- вектор предсказаний. Но тут есть проблема. Представьте, что у вас 90\% объектов одного класса. И вы получили точность 70\%. Это хорошо? Да ну хз, но, вероятно, нет, и вы переобучаетесь на мажоритарный класс.\\
    \textbf{Коэффициент Каппа Коэна}:
    \[
    \kappa(po,pe)=\frac{po-pe}{1-pe}
    \]
    Где $po$~--- полученная точность, а $pe$~--- точность случайного/наивного алгоритма.\\
    Что использовать функцией ошибки? Например, вот это
    \[
    \operatorname{ErrorRate}(y;\widehat y)=\frac1n\sum\limits_{i=1}^n[y_i\neq\widehat y_i]=1-\operatorname{Accuracy}
    \]
    Но это используют редко, чаще используют что-нибудь, что можно производной ковырять.\\
    Из более интересного: Confusion matrix: в ячейке $CM_{t,c}$ хранится количество объектов класса $t$, которые были приняты нашим алгоритмом за объекты класса $c$.\\
    Простой пример: бинарная классификация. Тогда у нас наша матрица состоит из четырёх ячеек, которые, как несложно догадаться, называются <<True Positive>>, <<True Negative>>, <<False Positive>> (мы посчитали, что positive, а оно нет, type 1 error) и <<False Negative>> (type 2 error). Тут следующие величины:
    \begin{itemize}
        \item Точность (не accuracy, а \textbf{precision}): $\displaystyle\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}$.
        \item Полнота (\textbf{recall}): $\displaystyle\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}$.
        \item Индекс Жаккара (\textbf{JaccardIndex}): $\displaystyle\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}+\mathrm{FP}}$.
    \end{itemize}
    Так вот, используют такую штуку, как F-мера:
    \[
    F_\beta=(1+\beta^2)\frac{\mathrm{Precision}\cdot\mathrm{Recall}}{\beta^2\cdot\mathrm{Precision}+\mathrm{Recall}}
    \]
    Например, могут использовать $F_1$.\\
    Как делать F-меру для нескольких классов? Ну, можно делать <<один против всех>>, мы уже учились. То есть у нс для каждого класса $j$ сформировались свои $\mathrm{TP}_j$, $\mathrm{FP}_j$ и $\mathrm{FN}_j$. Тогда
    \begin{itemize}
        \item Можно усреднить $\mathrm{TP}$, $\mathrm{FP}$ и $\mathrm{FN}$, из них посчитать $\mathrm{Precision}$ и $\mathrm{Recall}$. Полученная мера~--- micro-average F-score.
        \item Можно для каждого $j$ посчитать $\mathrm{Precision}_j$ и $\mathrm{Recall}_j$, после чего усреднить уже их. Результат будет другим, и называться будет macro-average F-score.
        \item А можно уже посчитать F-score для каждого класса и её усреднить. Это будет average F-score.
    \end{itemize}
    Лучшей среди них нет. Хороший заказчик сам знает, что ему надо из этого.\\
    Ещё есть вот такая штука:
    \begin{itemize}
        \item \textbf{Чувствительность} (true positive rate, $\mathrm{TPR}$)~--- тупо Recall (т.е. $\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}$).
        \item \textbf{Специфичность} (true negative rate, $\mathrm{TNR}$)~--- отношение $\frac{\mathrm{TN}}{\mathrm{TN}+\mathrm{FP}}$.
        \item ROC-кривая~--- зависимость $\mathrm{TPR}$ от $\mathrm{TNR}$.
        \item Площадь под ROC-кривой (AUC ROC)~--- функция качества бинарной классификации.
    \end{itemize}
    Рандомный классификатор имеет тупо прямую линию из $(0;0)$ в $(1;1)$. Идеальный~--- точка $(0;1)$. В многоклассовой классификации используется редко, поэтому говорить об этом не будем.\\
    Как ROC строить? Берём мягкую классификацию. Она для каждого объекта сообщает нам вероятность, что объект принадлежит положительному классу. Так вот что мы делаем? А делаем следующее. Установим карандаш в левый нижний угол, будем перебирать $(p_i;y_i)$ по возрастанию $p_i$. Если $y_i$ положителен, сдвинем карандаш право, если отрицателен~--- то вверх. Получим ломаную, которую если аппроксимировать, получится нормальная ROC-кривая.
    \subsection{Оценка задачи регрессии.}
    Как сделать себе функцию ошибки? Есть сумма квадратов и его друзья:
    \begin{itemize}
        \item Простая и популярная функция:
        \[
        \operatorname{SS}(y;\widehat y)=\|y-\widehat y\|^2_2=\sum\limits_{i=1}^n(y_i-\widehat y_i)^2
        \]
        Легко вычисляется, имеет приятную производную, но некорректно сравнивать результаты для массивов разного размера.
        \item
        \[
        \operatorname{MSE}(y;\widehat y)=\frac1n\operatorname{SS}(y;\widehat y)
        \]
        Жаль, размерность не совпадает с размерностью целевого признака.
        \item Чтобы это починить, берём корень $\operatorname{MSE}$ (получаем $\operatorname{RMSE}$). Но одна проблема всё ещё остаётся: нужен baseline.
        \item Чтобы по чинить и это, вводим:
        \[
        \operatorname{NRMSE}(y;\widehat y)=\frac{\operatorname{RMSE}(y;\widehat y)}{\sigma[Y]}=\sqrt{\frac{\operatorname{SS(y;\widehat y)}}{\operatorname{SS}(y;\overline y)}}
        \]
        Где $\overline y$~--- среднее.\\
        Тут уже много всего хорошего: функция безразмерна, не требует baseline, минимум совпадает с $\operatorname{MSE}$, на нормализованных данных вообще равна $\operatorname{MSE}$. Разве что надо уточнять, какую мы берём нормализацию, в некоторых случаях делят на $\max[Y]-\min[Y]$ вместо $\sigma[Y]$.
        \item Ещё есть коэффициент детерминации, это уже функция качества, а не ошибки:
        \[
        R^2=1-(\operatorname{NRMSE}(y;\widehat y))^2
        \]
    \end{itemize}
    А ещё есть функции ошибки на основе модуля:
    \begin{itemize}
        \item \[\operatorname{MAE}(y;\widehat y)=\frac1n\sum\limits_{i=1}^n|y_i-\widehat y_i|\]
        Размерность совпадает с размерностью целевого признака.
        \item Средняя абсолютная процентная ошибка:
        \[
        \operatorname{MAPE}(y;\widehat y)=\frac{100\%}n\sum\limits_{i=1}^n\left|\frac{y_i-\widehat y_i}{y_i}\right|
        \]
        Хорошо, что безразмерна, плохо, что придётся доопределять при $y_i=0$.
        \item Можно брать симметричную версию:
        \[
        \operatorname{SMAPE}(y;\widehat y)=\frac{100\%}n\sum\limits_{i=1}^n\left|\frac{y_i-\widehat y_i}{|y_i|+|\widehat y_i|}\right|
        \]
        Тоже безразмерна. Говорят что она симметрична относительно аргументов, но тут как посмотреть: если $y_i=0$, то у $\widehat y_i=110$ $\operatorname{SMAPE}$ будет меньше, чем у $\widehat y_i=90$. Тут тоже иногда надо доопределять, а ещё больше разночтений: иногда $100\%$ заменяют на $200\%$, и/или $|y_i|+|\widehat y_i|$~--- на $y_i+\widehat y_i$.
    \end{itemize}
    Вообще для регрессий говорят чаще о функциях ошибки, а для классификаций~--- о функциях качестве. Но по сути это, конечно же, не важно.
    \section{Непараметрические и метрические методы.}
    \paragraph{Классификация на основе похожести.}
    Концепция проста: если выглядит как утка и крякает, значит утка. Более математически, объекты одного класса похожи по признакам. Как это формализовать? Взять какое-то метрику на $X$: $\rho\colon X\times X\to[0;+\infty)$ с симметрией, неравенством треугольника и всем остальным. Например, такое расстояние:
    \[
    \rho(a;b)=\left(\sum\limits_i|a_i-b_i|^p\right)^{1/p}
    \]
    В случае $p=\infty$ получается максимум разности модулей компонент.\\
    Ещё вариация:
    \[
    \begin{aligned}
        \operatorname{CosineSimilarity}(a;b)&=\dfrac{\sum\limits_ia_ib_i}{\sqrt{\sum\limits_ia_i}\sqrt{\sum\limits_ib_i}}\in[-1;1]\\
        \operatorname{CosineDistance}(a;b)&=1-\operatorname{CosineSimilarity}\in[0;2]
    \end{aligned}
    \]
    Или ещё (расстояние Махаланобиса):
    \[
    \rho(a;b)=\sqrt{(a-b)^{\mathsf T}S^{-1}(a-b)}
    \]
    Где $S$~--- матрица ковариации $X$ (оценивается на тренировочном наборе данных). Напоминание: матрица ковариаций~--- матрица, составленная из попарных ковариаций двух случайных векторов. Ковариация вычисляется так:
    \[
    \operatorname{cov}(x;y)=\Expected[(x-\Expected x)(y-\Expected y)]
    \]
    У расстояния Махаланобиса встроенная нормализация (потому что так ковариация определяется).
    \paragraph{Метод ближайших соседей.}
    Пусть $x_{(u,1)}$~--- ближайший сосед $u$:
    \[
    x_{(u,1)}=\operatorname*{argmin}_{x\in\scriptD_{\mathrm{train}}}\rho(u;x)
    \]
    Возьмём его класс, profit.\\
    Это простая штука, легко реализовать, нетрудно понять, но эта хрень очень чувствительна к шуму, не очень качественно работает, и не имеет явных гиперпараметров (что неправильно). И ещё недостаток~--- диаграмма Воронова как структура данных. Искать в ней быстро, но обновление этой диаграммы очень долгое, ровно как и построение.\\
    Всё это было алгоритмом одного ближайшего соседа. Вместо этого будем искать $k$ ближайших соседей, из которых будем брать мажоритарный класс. Пока забьём на то, как именно это делать, сфокусируемся на другом. Что, если получилось поровну объектов нескольких разных классов? Можно захардкодить, конечно, но это костыль.\\
    Поэтому план такой: обобщённый метрический классификатор. Введём себе апостериорные веса (не путать с априорными весами, если требуются и те, и другие, например, перемножаем)~--- функцию значимости $i$-того соседа $u$ (обозначим $w_{(i,u)}$). Алгоритм такой:
    \[
    a_{\mathrm{GDC}}(u;\scriptD_{\mathrm{train}})=\operatorname*{argmax}_{y\in Y}\sum\limits_{i=1}^{|\scriptD_{\mathrm{train}}|}\left[y(x_{u,i})-y\right]w_{i,u}
    \]
    То есть мы берём сумму весов для тех объектов, которые лежат в заданном классе (и выбираем класс с максимальным значением). По сути это оценка близости оценка к классу.\\
    Но как задавать веса (они же функции значимости)? Ну, как-то, чтобы он убывал по расстоянию. И умные люди придумали следующее:
    \paragraph{Ядерная функция и оценка Парзена~--- Розенблатта.}
    Функция называется ядерной, если она симметрична, неотрицательная и имеет единичный интеграл на $(-\infty;+\infty)$. Обычно это функция, которая равна нулю при значениях меньше $-1$ и больше $1$. Примеры:
    \begin{itemize}
        \begin{multicols}{2}
            \item $\displaystyle \operatorname{Uniform}(u)=\frac12[u<1]$
            \item $\displaystyle \operatorname{Triangular}(u)=(1-|u|)[u<1]$
            \columnbreak
            \item $\displaystyle \operatorname{Epanechnikov}(u)=\frac34(1-u^2)[u<1]$
            \item $\displaystyle \operatorname{Gaussian}(u)=\frac1{\sqrt{2\pi}}\exp\left(\frac{-u^2}2\right)$
        \end{multicols}
    \end{itemize}
    \begin{figure}[H]
        \includegraphics[width=0.8\textwidth]{Images/ml_kernel_functions}
    \end{figure}\noindent
    (Какие-то они неубывающие. А нифига, убывающие, потому что нас интересует значение функции только на $u\geqslant0$.)\\
    Так вот, если $Pr([a;b])$~--- мера вероятности на $[a;b]$, то
    \[
    p(x)=\lim\limits_{h\to\infty}\frac1{2h}Pr([x-h;x+h])
    \]
    Хотелось бы восстанавливать плотность распределения так. Тогда эмпирическая оценка (с окном шириной $h$):
    \[
    \widehat{p_h(x)}=\frac1{2nh}\sum\limits_{i=1}^n[|x-x_i|<h]
    \]
    Так вот, сюда можно всунуть ядерную функцию $K$ и получить оценку Парзена~--- Розенблатта (с окном шириной $h$):
    \[
    \widehat{p_h(x)}=\frac1{2nh}\sum\limits_{i=1}^nK\left(\frac{|x-x_i|}h\right)
    \]
    И суть в то, что $\widehat{p_h(x)}$ сходится к $p(x)$.\\
    Как обобщить это до многомерного случая? Если объекты описываются $m$ вещественными признаками $f_j\colon X\to\mathbb R$, то так:
    \[
    \widehat{p_h(x)}=\frac1n\sum\limits_{i=1}^n\prod\limits_{j=1}^m\frac1{h_j}K\left(\frac{f_j(x)-f_j(x_i)}{h_j}\right)
    \]
    Если же $X$~--- метрическое пространство с мерой расстояния $\rho$, то
    \[
    \widehat{p_h(x)}=\frac1{nV(h)}\sum\limits_{i=1}^n\prod\limits_{j=1}^m\frac1{h_j}K\left(\frac{\rho(x;x_i)}h\right)
    \]
    А $V(h)=\int_XK\left(\frac{\rho(x;x_i)}h\right)~\mathrm dx$~--- множитель нормализации. Но у нас, простите, функции ядерные, поэтому это тупо $\frac12$.\\
    Но то в непрерывном случае. А в дискретном
    \[
    \begin{aligned}
        a(u;\scriptD_{\mathrm{train}};{\color{red}h};K)&=\operatorname*{argmax}_{y\in Y}\sum\limits_{i=1}^{|\scriptD_{\mathrm{train}}|}\left[y(x_{u,i})-y\right]K\left(\frac{\rho(u;x_{(u,i)})}{\color{red}h}\right)\\
        a(u;\scriptD_{\mathrm{train}};{\color{red}k};K)&=\operatorname*{argmax}_{y\in Y}\sum\limits_{i=1}^{|\scriptD_{\mathrm{train}}|}\left[y(x_{u,i})-y\right]K\left(\frac{\rho(u;x_{(u,i)})}{\color{red}\rho(u;x_{(u,k+1)})}\right)
    \end{aligned}
    \]
    Верхнее~--- с фиксированной шириной окна, нижнее~--- с нефиксированной.\\
    Сверху мы фактически мы строим шар радиуса $h$ вокруг нашего объекта. Все объекты вне его нас не интересуют,, а внутри мы присваиваем объектам веса в зависимости от $K$.
    \begin{figure}[H]
        \includegraphics[width=0.8\textwidth]{Images/ml_knn-1}
    \end{figure}
    Но вопрос: как определить $h$? Для этого и есть нефиксированная ширина окна: берём до $k+1$-го соседа. То есть чтобы влияли $k$ ближайших соседей.
    \paragraph{Непараметрическая регрессия.}
    Всё выше было про классификацию, а что про регрессию? Идея схожая: будем рассматривать такие решения, что они в некоторой области $u$ константны. Тогда
    \[
    \scriptL(\theta)=\sum\limits_{i=1}^{|\scriptD_{\mathrm{train}}|}w_{(i,u)}(\theta-y(x_i))^2
    \]
    Если аналитически решать минимизацию $\scriptL$ относительно $\theta$, получится средневзвешенное.
    \begin{figure}[H]
        \includegraphics[width=0.8\textwidth]{Images/ml_knn-2}
    \end{figure}\noindent
    Тогда ядерное сглаживание Надарая~--- Ватсона:
    \[
    a_{\mathrm{NPR}}(u;\scriptD_{\mathrm{train}})=\dfrac{\sum\limits_{x_i\in\scriptD_{\mathrm{train}}}y_iK\left(\frac{\rho(x_i;u)}h\right)}{\sum\limits_{x_i\in\scriptD_{\mathrm{train}}}K\left(\frac{\rho(x_i;u)}h\right)}
    \]
    Тут с фиксированным окном, точно так же можно сделать и с нефиксированным.\\
    Теорема (кажется, Новикова). Пусть
    \begin{itemize}
        \item $\scriptD_{\mathrm{train}}$~--- выборка, распределённая по $p(x,y)$.
        \item $K$~--- такая ядерная функция, что $\int\limits_0^{+\infty}K(r)~\mathrm dr<\infty$ и $\lim\limits_{r\to\infty}rK(r)=0$.
        \item $\forall x\in X.\;E(y^2\mid x)<\infty$.
        \item $\lim\limits_{i\to\infty}h_i=0$, $\lim\limits_{i\to\infty}ih_i=\infty$
    \end{itemize}
    Тогда алгоритм непараметрической регрессии (по схеме Надарая~--- Ватсона) стремятся к $E(y\mid x)$ по вероятности в любой точке $x\in X$, где $E(y\mid x)$, $p(x)$, $D(y\mid x)$ непрерывны и $p(x)>0$. 
    \paragraph{Анализ всего вышеперечисленного.}
    \begin{itemize}
        \item Теоретически неравенство треугольника не нужно.
        \item Модельность метода зависит от структуры данных для хранения ближайших соседей.
        \item k-NN~--- регрессия с one-hot-преобразованием.
        \item Выбор функции ядра влияет на гладкость функции потерь, а на итоговое качество обычно влияет не так сильно.
        \item Выбор $h$ либо $k$ влияет на качество приближения.
        \item Чем больше $h$ либо $k$, тем меньше сложность модели (в крайнем случае $k=n$ алгоритм получается тождественен наивному). Вообще есть эвристика, что количество соседей выше $\sqrt n$ использовать не не надо.
    \end{itemize}
    \paragraph{Реализация.}
    Чтобы это всё быстро (хоть и приближённо) работало используется такая штука, как k-d-дерево или его аналоги.
    \paragraph{Связанные задачи.}
    \begin{itemize}
        \item Few-shot (one-shot) learning. Мало примеров, много классов, мало примеров каждого класса, причём множество классов может меняться. Основная задача (классификация)~--- 1-NN, подзадача~--- выделение признаков (строим пространство, в котором будет вычисляться расстояние). Впрочем, сейчас есть более нормальные методы.
        \item Непараметрическая генерация объектов. Берём точку $a$, смотрим $k$ ближайших точек её класса, выбираем одну, случайно выбираем точку на $(a;b)$, кидаем точку между ними с тем же классом.
        \item Поиск связей Томека. У нас могут быть близкие пары объектов разного класса (называются такие связями Томека). Если мы такое нашли, можно удалить оба, можно удалить объект мажоритарного класса, или можно объекту мажоритарного класса поменять метку. Утверждается, что мы немного восстановим компактность.
        \item LOWESS. Вместо удаления объектов их можно учитывать с меньшим локальным весом:
        \[
        w_i=K(y(x_i)-a(x_i,\scriptD_{\mathrm{train}}\setminus\{x_i\}))
        \]
        Функция ядра может отличаться от таковой в $a$, если что. Сам же $a$ может теоретически быть любым алгоритмом, но обычно используют kNN.
    \end{itemize}
\end{document}